<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Olivier Binette">
<meta name="dcterms.date" content="2019-05-24">
<meta name="description" content="Some notes regarding various ‘optimalities’ of posterior distributions.">

<title>Bayesian Optimalities – Olivier Binette</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "Search",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-059GDHV0GP"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-059GDHV0GP', { 'anonymize_ip': false});
</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #c00000;
      }

      .quarto-title-block .quarto-title-banner {
        color: #c00000;
background: #f8f8f8;
      }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Bayesian Optimalities – Olivier Binette">
<meta property="og:description" content="Some notes regarding various ‘optimalities’ of posterior distributions.">
<meta property="og:site_name" content="Olivier Binette">
<meta name="twitter:title" content="Bayesian Optimalities – Olivier Binette">
<meta name="twitter:description" content="Some notes regarding various ‘optimalities’ of posterior distributions.">
<meta name="twitter:card" content="summary">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Olivier Binette</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../pages/blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-creations" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Creations</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-creations">    
        <li>
    <a class="dropdown-item" href="../../../pages/research.html">
 <span class="dropdown-text">Publications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../pages/software.html">
 <span class="dropdown-text">Software</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../pages/teaching.html">
 <span class="dropdown-text">Teaching</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.github.com/OlivierBinette"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../pages/blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Bayesian Optimalities</h1>
                  <div>
        <div class="description">
          Some notes regarding various ‘optimalities’ of posterior distributions.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">technical</div>
                <div class="quarto-category">math</div>
                <div class="quarto-category">statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Olivier Binette </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 24, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#point-estimation-and-minimal-expected-risk" id="toc-point-estimation-and-minimal-expected-risk" class="nav-link active" data-scroll-target="#point-estimation-and-minimal-expected-risk">1. Point estimation and minimal expected risk</a>
  <ul class="collapse">
  <li><a href="#squared-error-loss" id="toc-squared-error-loss" class="nav-link" data-scroll-target="#squared-error-loss">Squared error loss</a></li>
  </ul></li>
  <li><a href="#randomized-estimation-and-information-risk-minimization" id="toc-randomized-estimation-and-information-risk-minimization" class="nav-link" data-scroll-target="#randomized-estimation-and-information-risk-minimization">2. Randomized estimation and information risk minimization</a></li>
  <li><a href="#online-learning-regret-and-kullback-leibler-divergence" id="toc-online-learning-regret-and-kullback-leibler-divergence" class="nav-link" data-scroll-target="#online-learning-regret-and-kullback-leibler-divergence">3. Online learning, regret and Kullback-Leibler divergence</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="point-estimation-and-minimal-expected-risk" class="level2">
<h2 class="anchored" data-anchor-id="point-estimation-and-minimal-expected-risk">1. Point estimation and minimal expected risk</h2>
<p>This first section is not directly about properties of the posterior distribution, but it is rather concerned with some summaries of the posterior which have nice statistical properties in different contexts.</p>
<section id="squared-error-loss" class="level3">
<h3 class="anchored" data-anchor-id="squared-error-loss">Squared error loss</h3>
<p>Suppose <span class="math inline">\(\pi\)</span> is a prior on an <strong>euclidean</strong> parameter space <span class="math inline">\(\Theta \subset \mathbb{R}^d\)</span> with norm <span class="math inline">\(\|\theta\|^2 = \theta^T \theta\)</span> defined through the dot product. Given a likelihood <span class="math inline">\(p _ \theta(X)\)</span> for data <span class="math inline">\(X\)</span>, the posterior distribution is defined as</p>
<p><span class="math display">\[
\pi(A \mid X) \propto \int _ A p _ \theta(X) \pi(d\theta)
\]</span></p>
<p>and the mean of the posterior distribution is</p>
<p><span class="math display">\[
\hat \theta _ {\pi} = \int \theta \,\pi(d\theta \mid X) = \mathbb{E} _ {\theta \sim \pi}[\theta \mid X].
\]</span></p>
<p>If we define the <em>risk</em> of an estimator <span class="math inline">\(\hat \theta\)</span> for the estimation of a parameter <span class="math inline">\(\theta _ 0\)</span> as</p>
<p><span class="math display">\[
R(\hat \theta; \theta _ 0) = \mathbb{E} _ {X \sim p _ \theta}[\|\theta _ 0 - \hat \theta(X)\|^2],
\]</span></p>
<p>and if</p>
<p><span class="math display">\[
B _ \pi(\hat \theta) = \mathbb{E} _ {\theta _ 0 \sim \pi}[R(\hat \theta; \theta _ 0)]
\]</span></p>
<p>is the expected risk of <span class="math inline">\(\hat \theta\)</span> with respect to the prior <span class="math inline">\(\pi\)</span> (also called the <strong>Bayes risk</strong>), then we have that the posterior mean estimate <span class="math inline">\(\hat \theta _ {\pi}\)</span> satisfies</p>
<p><span class="math display">\[
B _ \pi(\hat \theta) \geq B _ \pi(\hat \theta _ \pi)
\]</span></p>
<p>for any estimator <span class="math inline">\(\hat \theta\)</span>. That is, the posterior mean estimate minimizes the expected risk.</p>
<p>The proof follows from the fact that</p>
<p><span class="math display">\[
\| \theta _ 0 - \hat \theta(X) \|^2 \geq \|\theta _ 0 - \hat \theta _ \pi(X) \|^2 + \langle \theta _ 0 - \hat \theta _ \pi(X), \hat \theta _ \pi(X) - \hat \theta(X)\rangle.
\]</span></p>
<p>Writing the expected risk as an expected posterior loss, i.e.&nbsp;using the fact that</p>
<p><span class="math display">\[
\mathbb{E} _ {\theta _ 0 \sim \pi}\left[\mathbb{E} _ {X \sim p _ {\theta _ 0}}[\,\cdot\,]\right] = \mathbb{E} _ {X \sim m}\left[\mathbb{E} _ {\theta _ 0 \sim \pi(\cdot \mid X)}[\,\cdot\,]\right]
\]</span></p>
<p>where <span class="math inline">\(m\)</span> has density <span class="math inline">\(m(x) = \int p _ \theta(x) \pi(\theta)\,d\theta\)</span>, and since</p>
<p><span class="math display">\[
\mathbb{E} _ {\theta _ 0 \sim \pi(\cdot \mid X)}\left[\langle \theta _ 0 - \hat \theta _ \pi(X), \hat \theta _ \pi(X) - \hat \theta(X)\rangle\right] = 0,
\]</span></p>
<p>we obtain the result.</p>
<p>A few remarks:</p>
<ol type="1">
<li>The expected risk has stability properties. If <span class="math inline">\(\tilde \pi\)</span> and <span class="math inline">\(\pi\)</span> are two priors that are absolutely continuous with respect to each other, and if <span class="math inline">\(\|\log \frac{d\tilde \pi}{d\pi}\| _ \infty \leq C\)</span>, then</li>
</ol>
<p><span class="math display">\[
   e^{-C}B _ \pi(\hat\theta) \leq B _ {\tilde \pi}(\hat \theta) \leq e^C B _ {\pi}(\hat \theta).
\]</span></p>
<p>If the risk <span class="math inline">\(R(\hat \theta; \theta _ 0)\)</span> is uniformly bounded by some constant <span class="math inline">\(M\)</span> over <span class="math inline">\(\theta _ 0\in \Theta\)</span>, then</p>
<p><span class="math display">\[
   B _ {\tilde \pi}(\hat \theta) \leq \sqrt{M B _ {\pi}(\hat \theta)} \left\|d\tilde\pi/d\pi\right\| _ {L^2(\pi)}.
\]</span></p>
<p>This shows how small chances in the prior does not result in a dramatic change in the expected loss of an estimator, as long as the priors have “compatible tails” (i.e.&nbsp;a manageable likelihood ratio).</p>
<ol start="2" type="1">
<li><p>It is sometimes advocated to choose the prior <span class="math inline">\(\pi\)</span> so that the risk <span class="math inline">\(R(\hat \theta _ \pi; \theta _ 0)\)</span> is constant over <span class="math inline">\(\theta _ 0\)</span>: the resulting estimator <span class="math inline">\(\hat \theta _ \pi\)</span> is then agnostic, from a risk point of view, to <span class="math inline">\(\theta _ 0\)</span>. This may result in a sample-size dependent prior (which is arguably not in the Bayesian spirit), but the fun thing is that it makes the expected risk <em>maximal</em> and the Bayes estimator <span class="math inline">\(\hat \theta _ \pi\)</span> minimax: <span class="math inline">\(\hat \theta  _  \pi \in \arg\min  _   {\hat\theta} \sup  _   {\theta  _  0}R(\hat \theta;\theta  _  0)\)</span>. Indeed, in that case we have for any estimator <span class="math inline">\(\hat \theta\)</span> that <span class="math inline">\(\sup   _   {\theta  _  0} R(\hat \theta; \theta  _  0) \geq B  _  \pi(\hat \theta) \geq B  _   \pi(\theta  _  \pi) = \sup  _   {\theta  _   0}R(\hat \theta  _  \pi;\theta  _  0)\)</span>, from which it follows that <span class="math inline">\(\hat \theta  _  \pi\)</span> is minimax.</p></li>
<li><p>The idea of minimizing expected risk is not quite Bayesian, since it required us to first average over all data possibilities when computing the risk. One of the main advantage of the Bayesian framework is that it allows us to <em>condition</em> over the observed data, rather than pre-emptively considering all possibilities, and we can try to make use of that. Define the <strong>posterior expected loss</strong> (or <strong>posterior risk</strong>) or an estimator <span class="math inline">\(\hat \theta\)</span>, conditionally on <span class="math inline">\(X\)</span>, as</p></li>
</ol>
<p><span class="math display">\[
   R _ \pi(\hat \theta\mid X) = \mathbb{E} _ {\theta _ 0 \sim \pi(\cdot \mid X)}\left[(\hat \theta(X) - \theta _ 0)^2\right].
\]</span></p>
<ul>
<li style="list-style-type: none;">
It is clear from the previous computations that the posterior mean estimate minimizes the posterior risk, and hence the two approaches are equivalent. It turns out that, whatever the loss function we consider (under some regularity condition ensuring that stuff is finite and minimizers exist), minimizing the posterior risk is equivalent to minimizing the Bayes risk. In other words, we have that for any loss function (again under some regularity conditions ensuring finiteness and existence of stuff), we have
</li>
</ul>
<p><span class="math display">\[
   \arg\min _ {\hat \theta}\mathbb{E} _ {X \sim m}\left[\mathbb{E} _ {\theta _ 0 \sim \pi(\cdot \mid X)}[\ell(\hat \theta(X), \theta _ 0)]\right] = \arg\min _ {\hat\theta}\mathbb{E} _ {\theta _ 0 \sim \pi(\cdot \mid X)}[\ell(\hat \theta(X), \theta _ 0)].
\]</span></p>
<ul>
<li style="list-style-type: none;">
This is roughly self-evident if we think about it. An interesting consequence is that any estimator minimizing a Bayes risk is a function of the posterior distribution.
</li>
</ul>
</section>
</section>
<section id="randomized-estimation-and-information-risk-minimization" class="level2">
<h2 class="anchored" data-anchor-id="randomized-estimation-and-information-risk-minimization">2. Randomized estimation and information risk minimization</h2>
<p>Let <span class="math inline">\(\Theta\)</span> be a model, let <span class="math inline">\(X \sim Q\)</span> be some data and let <span class="math inline">\(\ell _ \theta(X)\)</span> be a loss associated with using <span class="math inline">\(\theta\)</span> to fitting the data <span class="math inline">\(X\)</span>. For instance, we could have <span class="math inline">\(\Theta = \{f:\mathcal{X} \rightarrow \mathbb{R}\}\)</span> a set of functions, <span class="math inline">\(X =\{(U _ i, Y _ i)\} _ {i=1}^n \subset \mathcal{X}\times \mathbb{R}\)</span> a set of features with associated responses, and <span class="math inline">\(\ell _ \theta(X) = \sum _ {i}(Y _ i -\theta(U _ i))^2\)</span> the sum of squared loss.</p>
<p>There may be a parameter <span class="math inline">\(\theta _ 0\in\Theta\)</span> minimizing the risk <span class="math inline">\(R(\theta) = \mathbb{E} _ {X\sim Q}[\ell _ \theta(X)]\)</span>, which will then be our learning target. Now we consider <em>randomized</em> estimators taking the form <span class="math inline">\(\theta\sim \hat \pi _ X\)</span>, where <span class="math inline">\(\hat\pi _ X\)</span> is a data-dependent distribution, and the performance of this estimation method can then be evaluated by the empirical risk <span class="math inline">\(R _ X (\hat\pi _ X) = \mathbb{E} _ {\theta \sim \hat \pi _ X}[\ell _ \theta(X)]\)</span>.</p>
<p>Here we should be raising an eyebrow. There is typically no point in having the estimator <span class="math inline">\(\theta\)</span> being random, i.e.&nbsp;we typically will prefer to take <span class="math inline">\(\hat \pi _ X\)</span> a point mass rather than anything else. But bear with me for a sec.&nbsp;The cool thing is that if we choose</p>
<p><span class="math display">\[
\hat \pi _ X = \arg\min _ {\hat \pi _ X} \left\{R(\hat \pi _ X) + D(\hat \pi _ X \| \pi)\right\}, \tag{$*$}
\]</span></p>
<p>where <span class="math inline">\(D(\hat \pi _ X\| \pi) = \int \log \frac{d\hat \pi _ X}{d\pi} \,d\hat \pi _ X\)</span> is the Kullback-Leibler divergence, then this distribution will satisfy</p>
<p><span class="math display">\[
d\hat \pi _ X(\theta) \propto e^{-\ell _ \theta(X)}d\pi(\theta).
\]</span></p>
<p>That is, Bayesian-type posteriors arise by minimizing the empirical risk of a randomized estimation scheme penalized by the Kullback-Leibler divergence form prior to posterior <a href="https://ieeexplore.ieee.org/document/1614067/">(Zhang, 2006)</a>.</p>
<p>For the proof, write</p>
<p><span class="math display">\[
R _ X(\hat \pi _ X) + D(\hat \pi _ X \| \pi) = \int \left(\ell _ \theta(X) + \log\frac{d\hat \pi _ X(\theta)}{d\pi(\theta)}\right) d\hat \pi _ X (\theta)=\int\left(\log\frac{d\hat\pi _ X(\theta)}{e^{-\ell _ \theta(X)}d\pi(\theta)}\right)d\hat \pi _ X(\theta)
\]</span></p>
<p>which is also equal to <span class="math inline">\(D(d\hat \pi _ X \| e^{-\ell _ \theta(X)} d\pi)\)</span> and, by properties of the Kullback-Leibler divergence, obviously minimized at <span class="math inline">\(d\hat \pi _ X \propto e^{\ell _ \theta(X)}d\pi(\theta)\)</span>.</p>
<p>Is this practically useful and insightful? Possibly. But at least this approach is suited to a general theory, as shown in Zhang (2006) and as I reproduce below.</p>
<p>Let us introduce a Rényi-type generalization error defined, for <span class="math inline">\(\alpha \in (0,1)\)</span>, by</p>
<p><span class="math display">\[
d _ \alpha(\theta; Q) = -\alpha^{-1}\log\mathbb{E} _ {X' \sim Q}[e^{-\alpha \ell _ \theta(X')}].
\]</span></p>
<p>This is a measure of loss associated with the use of a parameter <span class="math inline">\(\theta\)</span> to fit new data <span class="math inline">\(X' \sim Q\)</span>. We also write</p>
<p><span class="math display">\[
d _ \alpha(\hat \pi _ X; Q) = -\mathbb{E} _ {\theta \sim \hat \pi _ X}\left[ \alpha^{-1}\log\mathbb{E} _ {X' \sim Q}[e^{-\alpha \ell _ \theta(X')}] \right]
\]</span></p>
<p>for the expected Rényi generalization error when using the randomization scheme <span class="math inline">\(\theta \sim \hat \pi _ X\)</span>.</p>
<p>In order to get interesting bounds on this generalization error, we can follow the approach of Zhang (2006).</p>
<section id="change-of-measure-inequality" class="level4">
<h4 class="anchored" data-anchor-id="change-of-measure-inequality">Change of measure inequality</h4>
<p>We’ll need the change of measure inequality, which states that for any function <span class="math inline">\(f\)</span> and distributions <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\hat \pi\)</span> on <span class="math inline">\(\Theta,\)</span></p>
<p><span class="math display">\[
\mathbb{E} _ {\theta \sim \hat\pi}[f(\theta)] \leq D(\hat \pi \| \pi) + \log \mathbb{E} _ {\theta \sim \pi}\left[e^{f(\theta)}\right].
\]</span></p>
<p>Indeed, with some sloppyness and Jensen’s inequality we can compute</p>
<p><span class="math display">\[
\log \int e^{f(\theta)}\pi(d\theta)\geq \int f(\theta)\log(d\pi/d\hat\pi(\theta))d\hat \pi = \mathbb{E} _ {\theta \sim \hat \pi}[f(\theta)] - D(\hat \pi\|\pi).
\]</span></p>
</section>
<section id="generalization-error-bound" class="level4">
<h4 class="anchored" data-anchor-id="generalization-error-bound">Generalization error bound</h4>
<p>We can now attempt bounding <span class="math inline">\(d _ \alpha(\hat \pi _ X;Q)\)</span>. Consider the difference <span class="math inline">\(\Delta _ X (\theta) = d _ \alpha(\theta;Q) - \ell _ \theta(X)\)</span> between the generalization error and the empirical loss corresponding to the use of a fixed parameter <span class="math inline">\(\theta\)</span>. Then by the change of measure inequality,</p>
<p><span class="math display">\[
\exp\{\mathbb{E} _ {\theta \sim \hat \pi _ X}[\Delta _ X(\theta)] - D(\hat \pi _ X\|\pi)\} \leq \mathbb{E} _ {\theta \sim \pi}\left[e^{\Delta _ X(\theta)}\right]
\]</span></p>
<p>and hence for any <span class="math inline">\(\pi\)</span>,</p>
<p><span class="math display">\[
\mathbb{E} _ {X \sim Q}\left[\exp\left\{\mathbb{E} _ {\theta \sim \hat \pi _ X}[\Delta _ X(\theta)] - D(\hat \pi _ X\|\pi)\right\}\right] \leq \mathbb{E} _ {X \sim Q}\left[\mathbb{E} _ {\theta \sim \pi}\left[e^{\Delta _ X(\theta)}\right]\right] = 1
\]</span></p>
<p>By Markov’s inequality, this implies that <span class="math inline">\(\forall t &gt; 0\)</span>,</p>
<p><span class="math display">\[
\mathbb{P}\left(\mathbb{E} _ {\theta \sim \hat \pi _ X}[\Delta _ X(\theta)] - D(\hat \pi _ X\|\pi) \geq t\right) \leq e^{-t}.
\]</span></p>
<p>Rewriting yields</p>
<p><span class="math display">\[
d _ \alpha(\hat \pi _ X;Q) \leq R _ X(\hat \pi _ X) + D(\hat \pi _ X\|\pi) + t
\]</span></p>
<p>with probability at least <span class="math inline">\(1-e^{-t}\)</span>. To recap: the term <span class="math inline">\(d _ \alpha(\hat \pi _ X;Q)\)</span> is understood as a generalization error, on the right hand side <span class="math inline">\(R _ X(\hat \pi _ X) = \mathbb{E} _ {\theta \sim \hat \pi _ X}[\ell _ \theta(X)]\)</span> is the empirical risk, the Kullback-Leibler divergence <span class="math inline">\(D(\hat \pi _ X\|\pi)\)</span> penalizes the complexity of <span class="math inline">\(\hat\pi _ X\)</span> seen as a divergence from a “prior” <span class="math inline">\(\pi\)</span>, and <span class="math inline">\(t\)</span> is a tuning parameter.</p>
</section>
</section>
<section id="online-learning-regret-and-kullback-leibler-divergence" class="level2">
<h2 class="anchored" data-anchor-id="online-learning-regret-and-kullback-leibler-divergence">3. Online learning, regret and Kullback-Leibler divergence</h2>
<p>Following <a href="http://www.stat.yale.edu/~arb4/publications_files/information%20theoric%20characterization%20of%20bayes%20performance.pdf">Barron (1998)</a>, suppose we sequentially observe data points <span class="math inline">\(X _ 1, X _ 2, X _ 3, \dots\)</span> which are say i.i.d. with common distribution <span class="math inline">\(Q\)</span> with density <span class="math inline">\(q\)</span>. At each time step <span class="math inline">\(n\)</span>, the goal is to predict <span class="math inline">\(X _ {n+1}\)</span> using the data <span class="math inline">\(X^n = (X _ 1, \dots, X _ n)\)</span>. Our prediction is not a point estimate of <span class="math inline">\(X _ {n+1}\)</span>, but somewhat similarly as in the randomized estimation scenario we output a density estimate <span class="math inline">\(\hat p _ n = p(\cdot \mid X^n)\)</span>, the goal being that <span class="math inline">\(p(X _ {n+1}\mid X^n)\)</span> be as large as possible. A bit more precisely, we individually score a density estimate <span class="math inline">\(\hat p _ n\)</span> through the risk <span class="math inline">\(\ell _ q(\hat p _ n) = \mathbb{E} _ {X _ {n+1}\sim q}[\log(q(X _ {n+1})/\hat p _ n(X _ {n+1} ))] = D(q\| \hat p _ n)\)</span> which is the Kullback-Leibler divergence between <span class="math inline">\(\hat p _ n\)</span> and <span class="math inline">\(q\)</span>. The <em>regret</em> over times <span class="math inline">\(n=1, 2,\dots, N\)</span> is the sum of the risk over the whole process, i.e.</p>
<p><span class="math display">\[
\text{regret} = \sum _ {n=1}^N D(q\| \hat p _ n).
\]</span></p>
<p>Formally, this process is equivalent to estimating the distribution of <span class="math inline">\(X^N\)</span> all at once: our density estimate <span class="math inline">\(\hat p^N\)</span> of <span class="math inline">\(X^N\)</span> would simply be</p>
<p><span class="math display">\[
\hat p^N(X^N) = \prod _ {n=1}^N \hat p _ n(X _ n)
\]</span></p>
<p>and the regret is, by the chain rule, simply <span class="math inline">\(D(q^N \| \hat p^N)\)</span>, where <span class="math inline">\(q^N\)</span> is the <span class="math inline">\(N\)</span>th independent product of <span class="math inline">\(q\)</span>.</p>
<p>Given a prior <span class="math inline">\(\pi\)</span> over a space of distributions for <span class="math inline">\(q\)</span>, our problem then to minimize the Bayes risk</p>
<p><span class="math display">\[
B _ \pi(\hat p^N) = \mathbb{E} _ {q\sim \pi} D(q^N\|\hat p^N).
\]</span></p>
<p>This is achieved by choosing <span class="math inline">\(\hat p^N(x) = \hat p _ \pi^N(x) = \int q^N(x) \pi(dq)\)</span> the <em>prior predictive</em> density. This is equivalent to using, at each time step <span class="math inline">\(n\)</span>, the poterior predictive density <span class="math inline">\(\hat p _ {n, \pi}(x) = \int q(x) \,\pi(dq\mid \{X  _  i\}  _ {i=1}^n)\)</span>.</p>
<p>To see this minimizing property of the Bayes average, it suffices to write</p>
<p><span class="math display">\[
B _ \pi(\hat p^N) = \mathbb{E} _ {q \sim \pi} \left[D(q^N\| \hat p _ \pi^N)\right] + D(\hat p _ \pi^N \| \hat p^N).
\]</span></p>
<p>Note that an consequence of this analysis is also that the posterior predictive distribution <span class="math inline">\(\hat p _ {n, \pi}\)</span> will minimize the expected posterior risk:</p>
<p><span class="math display">\[
\hat p _ {n, \pi} \in \arg\min _ {\hat p _ {n}} \mathbb{E} _ {q \sim \pi(\cdot\mid X^n)}\left[D(q\|\hat p _ n)\right].
\]</span></p>
<p>Following section 1, this furthermore means that the posterior predictive distribution minimizes the Bayes risk associated with the Kullback-Leibler loss.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-copyright"><h2 class="anchored quarto-appendix-heading">Copyright</h2><div class="quarto-appendix-contents"><div>Olivier Binette</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/olivierbinette\.ca");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="olivierbinette/olivierbinette.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkxNTk0Mzc2MjQ=" data-category="General" data-category-id="DIC_kwDOCYDTOM4CA7V-" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright © 2024 Olivier Binette
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>