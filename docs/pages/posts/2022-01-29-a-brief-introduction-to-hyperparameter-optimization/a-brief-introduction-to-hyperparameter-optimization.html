<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Olivier Binette">
<meta name="dcterms.date" content="2022-01-29">
<meta name="description" content="Overview of a few black-box hyperparameter optimization techniques: grid search, randomized search and sequential model-based optimization.">

<title>Intro to Hyperparameter Optimization for Machine Learning – Olivier Binette</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "Search",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-059GDHV0GP"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-059GDHV0GP', { 'anonymize_ip': false});
</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #c00000;
      }

      .quarto-title-block .quarto-title-banner {
        color: #c00000;
background: #f8f8f8;
      }
</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Intro to Hyperparameter Optimization for Machine Learning – Olivier Binette">
<meta property="og:description" content="Overview of a few black-box hyperparameter optimization techniques: grid search, randomized search and sequential model-based optimization.">
<meta property="og:site_name" content="Olivier Binette">
<meta name="twitter:title" content="Intro to Hyperparameter Optimization for Machine Learning – Olivier Binette">
<meta name="twitter:description" content="Overview of a few black-box hyperparameter optimization techniques: grid search, randomized search and sequential model-based optimization.">
<meta name="twitter:card" content="summary">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Olivier Binette</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../pages/blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-creations" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Creations</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-creations">    
        <li>
    <a class="dropdown-item" href="../../../pages/research.html">
 <span class="dropdown-text">Publications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../pages/software.html">
 <span class="dropdown-text">Software</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../pages/teaching.html">
 <span class="dropdown-text">Teaching</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.github.com/OlivierBinette"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../pages/blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Intro to Hyperparameter Optimization for Machine Learning</h1>
                  <div>
        <div class="description">
          <p>Overview of a few black-box hyperparameter optimization techniques: grid search, randomized search and sequential model-based optimization.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">technical</div>
                <div class="quarto-category">statistics</div>
                <div class="quarto-category">machine learning</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="https://olivierbinette.github.io">Olivier Binette</a> </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              Duke University
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 29, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">1 Introduction</a></li>
  <li><a href="#background-and-terminology" id="toc-background-and-terminology" class="nav-link" data-scroll-target="#background-and-terminology">1.1 Background and Terminology</a>
  <ul class="collapse">
  <li><a href="#models-parameters-and-performance-evaluation" id="toc-models-parameters-and-performance-evaluation" class="nav-link" data-scroll-target="#models-parameters-and-performance-evaluation">Models, Parameters and Performance Evaluation</a></li>
  <li><a href="#hyperparameters" id="toc-hyperparameters" class="nav-link" data-scroll-target="#hyperparameters">Hyperparameters</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  </ul></li>
  <li><a href="#black-box-optimization-methods" id="toc-black-box-optimization-methods" class="nav-link" data-scroll-target="#black-box-optimization-methods">2 Black-Box Optimization Methods</a>
  <ul class="collapse">
  <li><a href="#grid-search" id="toc-grid-search" class="nav-link" data-scroll-target="#grid-search">2.1 Grid Search</a></li>
  <li><a href="#random-search" id="toc-random-search" class="nav-link" data-scroll-target="#random-search">2.2 Random Search</a></li>
  <li><a href="#sequential-model-based-optimization" id="toc-sequential-model-based-optimization" class="nav-link" data-scroll-target="#sequential-model-based-optimization">2.3 Sequential Model-Based Optimization</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">3 Summary</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1 Introduction</h2>
<p>Machine learning is easy, right? You pick a model, fit it to your data, and out come predictions.</p>
<p><a href="ml.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="ml.svg" class="img-fluid"></a></p>
<p>Not quite. Sometimes we talk about the fancy math and algorithms under the hood to make it look serious, but we rarely talk about how difficult it is to transform whatever data can gather into useful, actionable predictions that have business value.</p>
<p>There are many challenges. First, there’s the transformation of a business problem into something that’s remotely approachable by machine learning and statistics. Second, there’s the development of a data collection plan or, more often than not, the identification of observational data which is already available. With the collection of this data comes the third step, modeling, which bridges between numbers and useful answers. Modeling may have to account for all kinds of issue with your data, such as class imbalance, missingness, and non-representativeness. You also want to obtain <em>good</em> answers, so throughout this step <strong>you loop between model specification, evaluation, and refinement</strong>. It is a lengthy process of research and investigation into the performance of your model, insights into the <em>why</em> of what you observe, and various fixes and improvements to your model. Finally, in a fourth stage, you must account for how your model will be used and the management of its lifecycle.</p>
<p><a href="workflow.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="workflow.svg" class="img-fluid"></a></p>
<p>Moral of the story: there is a lot work involved. We need all hands on deck. And even more than that, <strong>we need robust automatization tools</strong> to support this machine learning workflow.</p>
<p>This blog post is about a single set of tools – <strong>hyperparameter optimization techniques</strong> – used to help with the model specification, evaluation, and refinement loop. I will focus on the standard machine learning framework of supervised learning. In this context, machine learning algorithms can be seen as black boxes which take in some data, a bunch of tuning <em>hyperparameters</em> specified by the user of the algorithm, and which output predictions. The quality of the predictions can be evaluated through data splitting or cross-validation. That is, we’re always able to compare predictions to ground truth for the data we have at hand.</p>
<p>My goal is to describe key approaches to hyperparameter optimization (see Table 1) in order to provide <strong>conceptual understanding that can be helpful practice.</strong> I describe <em>black-box</em> methods which treat the machine learning algorithm as, well, a black blox. This includes <strong>grid search</strong>, <strong>randomized search</strong>, and sequential model-based optimization such as <strong>Bayesian optimization.</strong> There are additional methods to be considered, such as <strong>Hyperband</strong> and <strong>Bayesian model selection</strong>, which integrate with the learning algorithms themselves. These will be for another blog post.</p>
<table class="caption-top table">
<caption>Table 1: Different types of hyperparameter optimization methods</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Black-box methods</th>
<th style="text-align: left;">Integrated methods</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Grid Search</td>
<td style="text-align: left;">Hyperband</td>
</tr>
<tr class="even">
<td style="text-align: left;">Randomized Search</td>
<td style="text-align: left;">Bayesian Model Selection</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sequential Model-Based Optimization</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<!--**Disclaimer:** my goal in this post is **not** to say that grid search is bad, or that you should be using algorithm X instead of agorithm Y for hyperparameter optimization. The model specification, evaluation and refinement loop is an important part of the machine learning workflow which leads to useful insights into the behavior and performance of your model. It should not be entirely automated. Hyperparameter optimization techniques should be used to gain more insights into your model and to improve your productivity, not as a drop-in replacement for model building. Use whatever technique works the best for you given what you're trying to achieve.
-->
<p>Before getting into the detail of these methods though, let’s go over some basic concepts and terminology which I’ll be using.</p>
</section>
<section id="background-and-terminology" class="level2">
<h2 class="anchored" data-anchor-id="background-and-terminology">1.1 Background and Terminology</h2>
<p>First, let’s talk about models, parameters and performance evaluation. This is going to be the occasion for me to introduce some terminology and notations.</p>
<section id="models-parameters-and-performance-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="models-parameters-and-performance-evaluation">Models, Parameters and Performance Evaluation</h3>
<p>A <strong>model</strong> is a mathematical representation of something going on in the real world. For instance, suppose you want to predict whether or not a given stock <span class="math inline">\(X\)</span> is going to go up tomorrow. A model for this could be: predict it’s going to go up with probability <span class="math inline">\(\alpha\)</span> if it went up today, otherwise predict it’s not going to go up with probability <span class="math inline">\(\beta\)</span>. There’s only one <strong>variable</strong> in this model (whether or not the stock went up today), and there are two <strong>parameters</strong>, the probabilities <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Here the parameters could be learned if we had historical data.</p>
<p>You could consider more sophisticated models such as classical time series models or reccurent neural networks. In all cases, you have variables (the input to your model), parameters (what you learn from data), and you end up with predictions.</p>
<p>You can compare the performance of any model by comparing the predictions to what actually happened. For instance, you could look at how often your predictions were right. That’s a performance <strong>evaluation metric</strong>. Your goal is usually to build a model which will keep on performing well.</p>
<p>Formally, let <span class="math inline">\(R\)</span> be the (average) future performance of your model. You don’t know this quantity, but you can estimate it as <span class="math inline">\(\hat R\)</span> using techniques such as cross-validation and its variants. There might be a bias and a variance to <span class="math inline">\(\hat R\)</span>, but the best we can do in practice is to try to find the model with the best estimated performance (modulo certain adjustments).</p>
<p>This brings us to the question: <strong>how should you choose a model?</strong> The standard in machine learning is to choose a model which maximizes <span class="math inline">\(\hat R\)</span>. It’s not the only solution, and it’s not always the best solution (it can be better to do model averaging if <span class="math inline">\(\hat R\)</span> has some variance), but it’s what we’ll focus on through this blog post.</p>
<p>Furthermore, we’ll approach this problem through the lens of hyperparameter selection.</p>
</section>
<section id="hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters">Hyperparameters</h3>
<p>Hyperparameters are things that have you have to specify before you can run a model, such as:</p>
<ul>
<li>what data features to use,</li>
<li>what type of model to use (linear model? random forest? neural network?)</li>
<li>other decisions that go into the specification of a model:
<ul>
<li>the number of layers in your neural network,</li>
<li>the learning rate for the gradient descent algorithm,</li>
<li>the maximum depth for decision trees, etc.</li>
</ul></li>
</ul>
<p>There is only a practical distinction between parameters and hyperparameters. Hyperparameters are things that are usually set separately from the other model parameters, or that do not nicely fit within a model’s learning algorithm. Depending on the framework you’re using, parameters can become hyperparameters and vice versa. For example, by using ensemble methods, you could easily transform the “model type choice” hyperparameter to a simple parameter of your ensemble that is learned from data.</p>
<p>The key thing is that, in practice, there will typically be some distinction between parameters of your model and a set of hyperparameters that you have to specify.</p>
<p>Through experience, you can learn what hyperparameters work well for the kinds of problems that you work on. Other times, you might carefully tune parameters and investigate the impact of your choices on model performance.</p>
<p>The manual process of hyperparameter tuning can lead to important insights into the performance and behavior of your model. However, it can also be a menial task that would be better automated through hyperparameter optimization algorithms aiming to maximize <span class="math inline">\(\hat R\)</span>, such as those that I review below.</p>
</section>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Let’s look at an example to make things concrete. This is adapted from <a href="https://scikit-optimize.github.io/stable/auto_examples/hyperparameter-optimization.html">scikit-optimize’s tutorial for tuning scikit-learn estimators</a>.</p>
<p>We’ll consider the <a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">California housing dataset</a> from the scikit-learn library. Each row in this dataset represents a census block and contains aggregated information regarding houses in that block. Our goal will be to predict median house price at the block level given these other covariates.</p>
<div id="cf166312" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a>dataset <span class="op">=</span> fetch_california_housing(as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a>X <span class="op">=</span> dataset.data <span class="co"># Covariates</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>n_features <span class="op">=</span> X.shape[<span class="dv">1</span>] <span class="co"># Number of features</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>y <span class="op">=</span> dataset.target <span class="co"># Median house prices</span></span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">MedInc</th>
<th data-quarto-table-cell-role="th">HouseAge</th>
<th data-quarto-table-cell-role="th">AveRooms</th>
<th data-quarto-table-cell-role="th">AveBedrms</th>
<th data-quarto-table-cell-role="th">Population</th>
<th data-quarto-table-cell-role="th">AveOccup</th>
<th data-quarto-table-cell-role="th">Latitude</th>
<th data-quarto-table-cell-role="th">Longitude</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>8.3252</td>
<td>41.0</td>
<td>6.984127</td>
<td>1.023810</td>
<td>322.0</td>
<td>2.555556</td>
<td>37.88</td>
<td>-122.23</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>8.3014</td>
<td>21.0</td>
<td>6.238137</td>
<td>0.971880</td>
<td>2401.0</td>
<td>2.109842</td>
<td>37.86</td>
<td>-122.22</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>7.2574</td>
<td>52.0</td>
<td>8.288136</td>
<td>1.073446</td>
<td>496.0</td>
<td>2.802260</td>
<td>37.85</td>
<td>-122.24</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>5.6431</td>
<td>52.0</td>
<td>5.817352</td>
<td>1.073059</td>
<td>558.0</td>
<td>2.547945</td>
<td>37.85</td>
<td>-122.25</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>3.8462</td>
<td>52.0</td>
<td>6.281853</td>
<td>1.081081</td>
<td>565.0</td>
<td>2.181467</td>
<td>37.85</td>
<td>-122.25</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20635</td>
<td>1.5603</td>
<td>25.0</td>
<td>5.045455</td>
<td>1.133333</td>
<td>845.0</td>
<td>2.560606</td>
<td>39.48</td>
<td>-121.09</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">20636</td>
<td>2.5568</td>
<td>18.0</td>
<td>6.114035</td>
<td>1.315789</td>
<td>356.0</td>
<td>3.122807</td>
<td>39.49</td>
<td>-121.21</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20637</td>
<td>1.7000</td>
<td>17.0</td>
<td>5.205543</td>
<td>1.120092</td>
<td>1007.0</td>
<td>2.325635</td>
<td>39.43</td>
<td>-121.22</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">20638</td>
<td>1.8672</td>
<td>18.0</td>
<td>5.329513</td>
<td>1.171920</td>
<td>741.0</td>
<td>2.123209</td>
<td>39.43</td>
<td>-121.32</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20639</td>
<td>2.3886</td>
<td>16.0</td>
<td>5.254717</td>
<td>1.162264</td>
<td>1387.0</td>
<td>2.616981</td>
<td>39.37</td>
<td>-121.24</td>
</tr>
</tbody>
</table>

<p>20640 rows × 8 columns</p>
</div>
</div>
</div>
<p>For the regression, we’ll use scikit-learn’s gradient boosted trees estimator. This model has a number of internal parameters which don’t need to know much about, as well as hyperparameters which can be used to tune the model. This includes the <code>max_depth</code> hyperparameter for the maximum depth of decision trees, <code>learning_rate</code> for the learning rate of gradient boosting, <code>max_features</code> for the maximum number of features to use in each decision trees, and a few more. Ranges of reasonable values for these parameters are specified in the <code>space</code> variable below.</p>
<div id="57ae97b6" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">from</span> skopt.space <span class="im">import</span> Real, Integer</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>model <span class="op">=</span> GradientBoostingRegressor(n_estimators<span class="op">=</span><span class="dv">25</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a>space  <span class="op">=</span> [Integer(<span class="dv">1</span>, <span class="dv">8</span>, name<span class="op">=</span><span class="st">'max_depth'</span>),</span>
<span id="cb2-7"><a href="#cb2-7"></a>          Real(<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="st">"log-uniform"</span>, name<span class="op">=</span><span class="st">'learning_rate'</span>),</span>
<span id="cb2-8"><a href="#cb2-8"></a>          Integer(<span class="dv">1</span>, n_features, name<span class="op">=</span><span class="st">'max_features'</span>),</span>
<span id="cb2-9"><a href="#cb2-9"></a>          Integer(<span class="dv">1</span>, <span class="dv">50</span>, name<span class="op">=</span><span class="st">'min_samples_leaf'</span>)</span>
<span id="cb2-10"><a href="#cb2-10"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now, the last thing we need is an estimator <span class="math inline">\(\hat R\)</span> for the model’s performance. This is our <code>Rhat()</code> function (i.e.&nbsp;<span class="math inline">\(\hat R\)</span>) which we’ll try to maximize. Here we use a cross-validated mean absolute error score.</p>
<div id="08c578bc" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="kw">def</span> Rhat(<span class="op">**</span>params):</span>
<span id="cb3-4"><a href="#cb3-4"></a>  model.set_params(<span class="op">**</span>params)</span>
<span id="cb3-5"><a href="#cb3-5"></a>  </span>
<span id="cb3-6"><a href="#cb3-6"></a>  <span class="cf">return</span> <span class="op">-</span>np.mean(cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb3-7"><a href="#cb3-7"></a>                                  scoring<span class="op">=</span><span class="st">"neg_mean_absolute_error"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>With this, we can fit the model to the data (using default hyperparameter values to begin with), and evaluate the model’s performance.</p>
<div id="3312eb48" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>model.fit(X, y)</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a>Rhat()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>np.float64(0.5503720160635011)</code></pre>
</div>
</div>
<p>Here the unit for median house price was in hundreds of thousands of dollars and we can interpret the model performance at this scale. The value <span class="math inline">\(\hat R \approx 0.55\)</span> means that, on average, the absolute error of the model is $55,000. We’ll see if we can do better using hyperparameter optimization.</p>
</section>
</section>
<section id="black-box-optimization-methods" class="level2">
<h2 class="anchored" data-anchor-id="black-box-optimization-methods">2 Black-Box Optimization Methods</h2>
<p>Black-box hyperparameter optimization algorithms consider the underlying machine algorithm as unknown. We only assume that, given a set of hyperparameters <span class="math inline">\(\lambda\)</span>, we can compute the estimated model performance <span class="math inline">\(\hat R(\lambda)\)</span>. There is usually variance in <span class="math inline">\(\hat R(\lambda)\)</span>, but this is not something that I will talk about in this post. We will therefore consider <span class="math inline">\(\hat R\)</span> as a deterministic function to be optimized.</p>
<p>Note: in practice, <strong>you need to account for the variance in <span class="math inline">\(\hat R\)</span></strong>, as otherwise you could get bad surprises. It’s just not something I’m covering in this post, since I want to focus on a conceptual understanding of the optimization algorithms.</p>
<p>We can use almost any technique to try to optimize <span class="math inline">\(\hat R\)</span>, but there are a number of challenges with hyperparameter optimization:</p>
<ol type="1">
<li><span class="math inline">\(\hat R\)</span> is usually rather costly to evaluate.</li>
<li>We usually do not have gradient information regarding <span class="math inline">\(\hat R\)</span> (otherwise, hyperparameters for which we have gradient information could easily be incorporated as parameters of the underlying ML algorithms).</li>
<li>The hyperparameter space is usually complex. It can contain discrete variables and can even be tree-structured, where some hyperparameters are only defined conditionally on other hyperparameters.</li>
<li>The hyperparameter space is usually somewhat high-dimensional, with more than just 2-3 dimensions.</li>
</ol>
<p>These particularities of the hyperparameter optimization problem has led the machine learning community to favor some of the optimization techniques which I discuss below.</p>
<section id="grid-search" class="level3">
<h3 class="anchored" data-anchor-id="grid-search">2.1 Grid Search</h3>
<p>The first technique to consider is <strong>grid search</strong>, which is a brute force approach to hyperparameter optimization. It is the simplest of all – you simply specify values to consider for each hyperparameter, and then evaluate your model performance for each combination of hyperparameter. At the end, you keep the hyperparameter configuration which performed best.</p>
<p>There are a few advantages to this approach:</p>
<ul>
<li>It gives you precise control over what hyperparameter configurations are evaluated.</li>
<li>It is simple to implement and easily parallelizable.</li>
</ul>
<p>However, there are also a number of serious drawbacks:</p>
<ol type="1">
<li>The runtime scales exponentially in the number of hyperparameter dimensions.</li>
<li>The runtime is tied to the hyperparameter search space which you specify. To reduce runtime, you need to manually redefine this space.</li>
</ol>
<p>Let’s see an example of how this works in practice. First, we define a grid of hyperparameter values to evaluate. Given the scoring function <span class="math inline">\(\hat R\)</span>, we can then use scikit-learn’s <code>GridSearchCV()</code> function to evaluate the model performance at each hyperparameter combination. This is done below:</p>
<div id="ec077486" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co"># Budget of 54 evaluations</span></span>
<span id="cb6-4"><a href="#cb6-4"></a>grid <span class="op">=</span> {</span>
<span id="cb6-5"><a href="#cb6-5"></a>  <span class="st">'max_depth'</span>: [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb6-6"><a href="#cb6-6"></a>  <span class="st">'learning_rate'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>],</span>
<span id="cb6-7"><a href="#cb6-7"></a>  <span class="st">'max_features'</span>: [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>],</span>
<span id="cb6-8"><a href="#cb6-8"></a>  <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">10</span>]</span>
<span id="cb6-9"><a href="#cb6-9"></a>}</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="kw">def</span> scoring(estimator, X_test, y_test):</span>
<span id="cb6-12"><a href="#cb6-12"></a>  y_pred <span class="op">=</span> estimator.predict(X_test)</span>
<span id="cb6-13"><a href="#cb6-13"></a>  <span class="cf">return</span> <span class="op">-</span>np.mean(np.<span class="bu">abs</span>(y_test <span class="op">-</span> y_pred))</span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a>results <span class="op">=</span> GridSearchCV(model, grid, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span>scoring).fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can then recover the best score and best hyperparameters. The best model is slightly better than the default model we looked at earlier, with a $4,000 decrease in average absolute error.</p>
<div id="7f366a0b" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="op">-</span>results.best_score_ <span class="co"># Lowest cross-validated mean absolute error</span></span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a>{key:results.best_params_[key] <span class="cf">for</span> key <span class="kw">in</span> grid.keys()} <span class="co"># Best parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>{'max_depth': 5,
 'learning_rate': 0.1,
 'max_features': 8,
 'min_samples_leaf': 1}</code></pre>
</div>
</div>
<p>It is also informative to plot an histogram for the distribution of model scores. We can see that most model configurations performed much worst than the default.</p>
<div id="350ef480" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a>plt.clf()</span>
<span id="cb9-4"><a href="#cb9-4"></a>p <span class="op">=</span> plt.hist(<span class="op">-</span>results.cv_results_[<span class="st">"mean_test_score"</span>], bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb9-5"><a href="#cb9-5"></a>p <span class="op">=</span> plt.title(<span class="st">"Score distribution for evaluated hyperparameters"</span>, loc<span class="op">=</span><span class="st">"left"</span>)</span>
<span id="cb9-6"><a href="#cb9-6"></a>p <span class="op">=</span> plt.xlabel(<span class="st">"Cross-validated average absolute error"</span>)</span>
<span id="cb9-7"><a href="#cb9-7"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-8-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-8-output-1.png" width="558" height="449" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="random-search" class="level3">
<h3 class="anchored" data-anchor-id="random-search">2.2 Random Search</h3>
<p>The second method we’ll look at is <strong>random search.</strong> Here, the idea is to sample a number <span class="math inline">\(k\)</span> of hyperparameter configurations at random from a given space, and to evaluate those random configurations.</p>
<p>This might seem like a silly idea. Why pick hyperparameter values at random?</p>
<p>The answer is that doing so <strong>removes all computational penalties</strong> from the consideration of useless hyperparameter dimensions. That is, imagine that a number <span class="math inline">\(s\)</span> of your hyperparameters have actually no impact on model performance. With grid search, the consideration of these hyperparameters would incur you a computational penalty which is exponential in <span class="math inline">\(s\)</span>. With random search, however, there is <strong>no penalty at all</strong> for adding these <span class="math inline">\(s\)</span> additional hyperparameter dimensions. The results from random search with or without these additional dimensions are <strong>exactly the same</strong> in both cases.</p>
<p>This is the huge advantage of random search over grid search: you do not get penalized for useless dimensions. Furthermore, in practice, being able to tune the search effort through the number of samples <span class="math inline">\(k\)</span> can be quite convenient.</p>
<p>Let’s see how this can be implemented in practice. We’ll define a hyperparameter space which is similar to the grid space we specified earlier, but which is filled in with additional possible values. We can then run scikit-learn’s <code>RandomizedSearchCV()</code> function to do the randomized search:</p>
<div id="1dece01b" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="im">from</span> scipy.stats <span class="im">import</span> loguniform</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="co"># Around roughly the same values as for the grid search</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>param_distribution <span class="op">=</span> {</span>
<span id="cb10-6"><a href="#cb10-6"></a>  <span class="st">'max_depth'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">8</span>),</span>
<span id="cb10-7"><a href="#cb10-7"></a>  <span class="st">'learning_rate'</span>: loguniform(<span class="fl">0.01</span>, <span class="dv">1</span>),</span>
<span id="cb10-8"><a href="#cb10-8"></a>  <span class="st">'max_features'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb10-9"><a href="#cb10-9"></a>  <span class="st">'min_samples_leaf'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb10-10"><a href="#cb10-10"></a>}</span>
<span id="cb10-11"><a href="#cb10-11"></a></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="co"># Budget of 54 evaluations</span></span>
<span id="cb10-13"><a href="#cb10-13"></a>results <span class="op">=</span> RandomizedSearchCV(model, param_distribution, n_iter<span class="op">=</span><span class="dv">54</span>, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span>scoring, random_state<span class="op">=</span><span class="dv">0</span>).fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The results are below. By considering a richer hyperparameter space, and without being penalized by this in the same way we would with a grid search, randomized search allows us to find a better model with the same amount of effort.</p>
<div id="a0627aee" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="op">-</span>results.best_score_ <span class="co"># Lowest cross-validated mean absolute error</span></span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>{key:results.best_params_[key] <span class="cf">for</span> key <span class="kw">in</span> grid.keys()} <span class="co"># Best parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>{'max_depth': 6,
 'learning_rate': np.float64(0.1453937524243155),
 'max_features': 7,
 'min_samples_leaf': 26}</code></pre>
</div>
</div>
<p>Again, we can look at the distribution of model performance for sampled hyperparameter configurations. It’s quite similar to grid search, with only a few better-performing models being identified.</p>
<div id="c27bdb0b" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a>plt.clf()</span>
<span id="cb13-4"><a href="#cb13-4"></a>p <span class="op">=</span> plt.hist(<span class="op">-</span>results.cv_results_[<span class="st">"mean_test_score"</span>], bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb13-5"><a href="#cb13-5"></a>p <span class="op">=</span> plt.title(<span class="st">"Score distribution for evaluated hyperparameters"</span>, loc<span class="op">=</span><span class="st">"left"</span>)</span>
<span id="cb13-6"><a href="#cb13-6"></a>p <span class="op">=</span> plt.xlabel(<span class="st">"Cross-validated average absolute error"</span>)</span>
<span id="cb13-7"><a href="#cb13-7"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-11-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-11-output-1.png" width="572" height="449" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sequential-model-based-optimization" class="level3">
<h3 class="anchored" data-anchor-id="sequential-model-based-optimization">2.3 Sequential Model-Based Optimization</h3>
<p>All of the techniques considered so far made no assumption at all about the function <span class="math inline">\(\hat R\)</span> to optimize.</p>
<p>This is a problem, because we do have prior information about <span class="math inline">\(\hat R\)</span>. We can expect <span class="math inline">\(\hat R\)</span> to have some level of regularity, meaning that similar hyperparameter configurations should have similar performance. This knowledge allows us to make inference about <span class="math inline">\(\hat R(\lambda)\)</span> given the evaluation of <span class="math inline">\(\hat R\)</span> at other points <span class="math inline">\(\tilde \lambda \not = \lambda\)</span>.</p>
<p>More formally, suppose we have evaluated <span class="math inline">\(\hat R\)</span> at a sequence of hyperparameter configurations <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_n\)</span>, thus observing <span class="math inline">\(\hat R(\lambda_1), \hat R(\lambda_2), \dots, \hat R(\lambda_n)\)</span>. This allows us to make inference about <span class="math inline">\(\hat R\)</span>. In particular, we can try guessing what next <span class="math inline">\(\lambda_{n+1}\)</span> will maximize <span class="math inline">\(\hat R\)</span> or improve our knowledge of <span class="math inline">\(\hat R\)</span>. Once we’ve observed <span class="math inline">\(\hat R(\lambda_{n+1})\)</span>, we repeat the process, trying to guess which <span class="math inline">\(\lambda_{n+2}\)</span> to pick to improve the procedure. That is the entire idea behind <strong>sequential model-based optimization</strong>.</p>
<p>To make this work in practice, we need the following ingredients:</p>
<ol type="1">
<li>An inferential model for <span class="math inline">\(\hat R\)</span>. That could be a Bayesian nonparametric model, like a Gaussian Process, or something else, like a Tree-structure Parzen Estimator.</li>
<li>A method to guess the next best hyperparameter value to pick. Typically, <span class="math inline">\(\lambda_{n+1}\)</span> is chosen to maximize the <strong>expected improvement criterion</strong>. This chooses <span class="math inline">\(\lambda\)</span> to maximize the expected value of <span class="math inline">\(\max\{\hat R(\lambda) - R^*, 0\}\)</span>, where <span class="math inline">\(R^*\)</span> is the current observed performance maximum. In other words, we want to maximize the potential for improving the current optimum, without penalizing for the possibility of observing a lower performance. This allows us to optimize <span class="math inline">\(\hat R\)</span> while still exploring the hyperparameter space. I refer the reader to <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">here</a> for a review of a few other selection criterions.</li>
</ol>
<p>When a Bayesian inferential framework is chosen, then sequential model-based optimization is called <strong>Bayesian optimization</strong> or <strong>Bayesian search</strong>. It is beyond of the scope of this blog post to go into the details of gaussian processes, but below I show howthe scikit-optimize library can be used to perform Bayesian optimization based on Gaussian Processes and the expected improvement criterion:</p>
<div id="8d881370" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">from</span> skopt <span class="im">import</span> gp_minimize</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="im">from</span> skopt.utils <span class="im">import</span> use_named_args</span>
<span id="cb14-3"><a href="#cb14-3"></a></span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="at">@use_named_args</span>(space)</span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="kw">def</span> objective(<span class="op">**</span>params):</span>
<span id="cb14-6"><a href="#cb14-6"></a>  model.set_params(<span class="op">**</span>params)</span>
<span id="cb14-7"><a href="#cb14-7"></a>  </span>
<span id="cb14-8"><a href="#cb14-8"></a>  <span class="cf">return</span> <span class="op">-</span>np.mean(cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb14-9"><a href="#cb14-9"></a>                                  scoring<span class="op">=</span><span class="st">"neg_mean_absolute_error"</span>))</span>
<span id="cb14-10"><a href="#cb14-10"></a></span>
<span id="cb14-11"><a href="#cb14-11"></a>res_gp <span class="op">=</span> gp_minimize(objective, space, n_calls<span class="op">=</span><span class="dv">54</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-12"><a href="#cb14-12"></a></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="co">## 0.46</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>With Bayesian optimization, we see that much more time is spent sampling performant models.</p>
<div id="85949e1e" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>plt.clf()</span>
<span id="cb15-2"><a href="#cb15-2"></a>p <span class="op">=</span> plt.hist(res_gp.func_vals, bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb15-3"><a href="#cb15-3"></a>p <span class="op">=</span> plt.title(<span class="st">"Score distribution for evaluated hyperparameters"</span>, loc<span class="op">=</span><span class="st">"left"</span>)</span>
<span id="cb15-4"><a href="#cb15-4"></a>p <span class="op">=</span> plt.xlabel(<span class="st">"Cross-validated average absolute error"</span>)</span>
<span id="cb15-5"><a href="#cb15-5"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-13-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-13-output-1.png" width="579" height="449" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Furthermore, we can see that the algorithm quickly converges towards performant models.</p>
<div id="55a08ddb" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">from</span> skopt.plots <span class="im">import</span> plot_convergence</span>
<span id="cb16-2"><a href="#cb16-2"></a></span>
<span id="cb16-3"><a href="#cb16-3"></a>plot_convergence(res_gp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-14-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html/cell-14-output-1.png" width="598" height="449" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">3 Summary</h2>
<p>This blog post provided a basic overview of hyperparameter optimization and of what can be gained from these techniques. We reviewed grid search, the simplest brute force approach. We reviewed random search, which improves upon grid search when some hyperparameter dimensions are not influencial. Finally, we reviewed sequential model-based optimization, which much more effectively samples models with good performance.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-copyright"><h2 class="anchored quarto-appendix-heading">Copyright</h2><div class="quarto-appendix-contents"><div>Olivier Binette</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/olivierbinette\.ca");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="olivierbinette/olivierbinette.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkxNTk0Mzc2MjQ=" data-category="General" data-category-id="DIC_kwDOCYDTOM4CA7V-" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright © 2024 Olivier Binette
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","openEffect":"zoom","descPosition":"bottom","closeEffect":"zoom","loop":false});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>