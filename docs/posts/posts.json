[
  {
    "path": "posts/2020-11-16-new-website/",
    "title": "New website!",
    "description": "Welcome to my new blog and website! I've imported a few posts from my past blogs as well.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2020-11-16",
    "categories": [],
    "contents": "\nI’m using Distill, “a publication format for scientific and technical writing, native to the web.”\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T21:15:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-theory-of-gibbs-posterior-concentration/",
    "title": "Theory of Gibbs posterior concentration",
    "description": "Notes on some research in progress.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2019-10-11",
    "categories": [],
    "contents": "\nConsider the statistical learning framework where we have data \\(X\\sim Q\\) for some unknown distribution \\(Q\\), a model \\(\\Theta\\) and a loss function \\(\\ell_\\theta(X)\\) measuring a cost associated with fitting the data \\(X\\) using a particular \\(\\theta\\in\\Theta\\). Our goal is to use the data to learn about parameters which minimize the risk \\(R(\\theta) = \\mathbb{E}[\\ell_\\theta(X)]\\). Here are two standard examples.\nDensity estimation. Suppose we observe independent random variables \\(X_1, X_2, \\dots, X_n\\). Here the model \\(\\Theta\\) parametrizes a set \\(\\mathcal{M} = \\{p_\\theta : \\theta \\in \\Theta \\}\\) of probability density functions (with respect to some dominating measure on the sample space), and our loss for \\(X = (X_1, \\dots, X_n)\\) is defined as \\[\n\\ell_\\theta(X) = - \\sum_{i=1}^n \\log p_\\theta(X_i).\n\\] If, for instance, the variables \\(X_i\\) are independent with common distribution with density function \\(p_{\\theta_0}\\) for some \\(\\theta_0 \\in \\mathbb{\\Theta}\\), then it follows from the positivity of the Kullback-Leibler divergence that \\(\\theta_0 \\in \\arg\\min _ \\theta \\mathbb{E}[\\ell _ \\theta(X)]\\). That is, under identifiability conditions, our learning target is the true data-generating distribution.\nIf the model is misspecified, roughly meaning that there is no \\(\\theta_0\\in \\Theta\\) such that \\(p_{\\theta_0}\\) is a density of \\(X_i\\), then our framework sets up the learning problem to be about the parameter \\(\\theta_0\\) which is such that \\(p_{\\theta_0}\\) mininizes the Kullback-Leibler divergence between \\(p_{\\theta_0}\\) and the true marginal distribution of the \\(X_i\\)’s.\nRegression. Here our observations take the form \\((Y_i, X_i)\\), the model \\(\\Theta\\) parameterizes regression functions \\(f_\\theta\\) and we can consider a sum of squared errors loss \\[\n\\ell_\\theta(X) = \\sum_{i=1}^n(Y_i - f_\\theta(X_i))^2.\n\\]\nGibbs posterior distributions\nGibbs Learning approaches this problem from a pseudo Bayesian point of view. While typically a Bayesian approach would require the specification of a full data-generating model, here we replace the likelihood function by the pseudo-likelihood function \\(\\theta \\mapsto e^{-\\ell_\\theta(X)}\\). Given a prior \\(\\pi\\) on \\(\\Theta\\), the Gibbs posterior distribution is then given by \\[\n\\pi(\\theta \\mid X) \\propto e^{-\\ell_\\theta(X)} \\pi(\\theta)\n\\] and satisfies \\[\n\\pi(\\cdot \\mid X) \\in \\text{argmin}_{\\hat \\pi} \\left\\{ \\mathbb{E}_{\\theta \\sim \\hat \\pi}[\\ell_\\theta(X)] + D_{\\text{KL}}(\\hat \\pi \\mid \\pi) \\right\\}\n\\] whenever these expressions are well defined.\nIn the context of integrable pseudo-likelihoods, the above can be re-interpreted as a regular posterior distributions built from density functions \\(f _ \\theta(x) \\propto e^{-\\ell _ \\theta(x)}\\) and with a prior \\(\\tilde \\pi\\) satisfying \\[\n\\frac{d\\tilde \\pi}{d\\pi}(\\theta) \\propto \\int e^{-\\ell_\\theta(x)}\\,dx =: c(\\theta).\n\\] However, the reason we cannot apply standard asymptotic theory to the analysis of Gibbs posterior is that the quantity \\(c(\\theta)\\) will typically be sample-size dependent. That is, if \\(X=X^n=(X_1, X_2, \\dots, X_n)\\) for i.i.d. random variables \\(X_i\\) and if the loss \\(\\ell_\\theta\\) separates as the sum \\[\n\\ell_\\theta(X) = \\sum_{i=1}^nl_{\\theta}(X_i),\n\\] then \\(c(\\theta) = \\left(\\int e^{-l_\\theta(x_1)} \\, dx_1\\right)^n\\). This data-dependent prior, tilting \\(\\pi\\) by the function \\(c(\\theta)^n\\), is what allows Gibbs learning to target general risk-minimizing parameters rather than likelihood Kullback-Leibler minimizers.\nSome of my ongoing research, presented as a poster at the O’Bayes conference in Warwick last summer, focused on understand the theoretical behaviour of Gibbs posterior distributions. I studied the posterior convergence and finite sample concentration properties of Gibbs posterior distributions under the large sample regime with additive losses \\(\\ell_\\theta^{(n)}(X_1, \\dots, X_n) = \\sum_{i=1}^n\\ell_\\theta(X_i)\\). I’ve attached the poster (joint work with Yu Luo) below and you can find the additional references here.\nNote that this is very preliminary work. We’re still in the process of exploring interesting directions (and I have very limited time this semester with the beginning of my PhD at Duke).\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T21:19:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-the-credibility-of-confidence-intervals/",
    "title": "The Credibility of confidence intervals",
    "description": "When p < 0.05 provides evidence in favor of the null...",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2019-09-11",
    "categories": [],
    "contents": "\nAndrew Gelman and Sander Greenman went “head to head” in a discussion on the interpretation of confidence intervals in The BMJ. Greenman stated the following, which doesn’t seem quite right to me.\n\nThe label “95% confidence interval” evokes the idea that we should invest the interval with 95/5 (19:1) betting odds that the observed interval contains the true value (which would make the confidence interval a 95% bayesian posterior interval\\(^{11}\\)). This view may be harmless in a perfect randomized experiment with no background information to inform the bet (the original setting for the “confidence” concept); more often, however […]\n\nIt’s not true that “this view may is harmless in perfect randomized experiments”, and I’m not sure where this “original setting of the confidence concept” is coming from. In fact, even in the simplest possible cases, the posterior probability of a \\(95\\%\\) confidence interval can be pretty much anything.\nImagine a “perfect randomized experiment”, where we use a test of the hypothesis \\(H_0: \\mu = 0\\) for which, for some reason, has zero power. If \\(p < 0.05\\), meaning that the associated confidence interval excludes \\(0\\), then we are certain that \\(H_0\\) holds and the posterior probability of the confidence interval is zero.\nLet this sink in. For some (albeit trivial) statistical tests, observing \\(p < 0.05\\) brings evidence in favor of the null.\nThe power of the test carries information, and the posterior probability of a confidence interval (or of an hypothesis), depends on this power among other things, even in perfect randomized experiments.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-bayesian-optimalities/",
    "title": "Bayesian Optimalities",
    "description": "Some notes regarding various 'optimalities' of posterior distributions.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2019-05-24",
    "categories": [],
    "contents": "\n1. Point estimation and minimal expected risk\nThis first section is not directly about properties of the posterior distribution, but it is rather concerned with some summaries of the posterior which have nice statistical properties in different contexts.\nSquared error loss\nSuppose \\(\\pi\\) is a prior on an euclidean parameter space \\(\\Theta \\subset \\mathbb{R}^d\\) with norm \\(\\|\\theta\\|^2 = \\theta^T \\theta\\) defined through the dot product. Given a likelihood \\(p _ \\theta(X)\\) for data \\(X\\), the posterior distribution is defined as\n\\[\n\\pi(A \\mid X) \\propto \\int _ A p _ \\theta(X) \\pi(d\\theta)\n\\]\nand the mean of the posterior distribution is\n\\[\n\\hat \\theta _ {\\pi} = \\int \\theta \\,\\pi(d\\theta \\mid X) = \\mathbb{E} _ {\\theta \\sim \\pi}[\\theta \\mid X].\n\\]\nIf we define the risk of an estimator \\(\\hat \\theta\\) for the estimation of a parameter \\(\\theta _ 0\\) as\n\\[\nR(\\hat \\theta; \\theta _ 0) = \\mathbb{E} _ {X \\sim p _ \\theta}[\\|\\theta _ 0 - \\hat \\theta(X)\\|^2],\n\\]\nand if\n\\[\nB _ \\pi(\\hat \\theta) = \\mathbb{E} _ {\\theta _ 0 \\sim \\pi}[R(\\hat \\theta; \\theta _ 0)]\n\\]\nis the expected risk of \\(\\hat \\theta\\) with respect to the prior \\(\\pi\\) (also called the Bayes risk), then we have that the posterior mean estimate \\(\\hat \\theta _ {\\pi}\\) satisfies\n\\[\nB _ \\pi(\\hat \\theta) \\geq B _ \\pi(\\hat \\theta _ \\pi)\n\\]\nfor any estimator \\(\\hat \\theta\\). That is, the posterior mean estimate minimizes the expected risk.\nThe proof follows from the fact that\n\\[\n\\| \\theta _ 0 - \\hat \\theta(X) \\|^2 \\geq \\|\\theta _ 0 - \\hat \\theta _ \\pi(X) \\|^2 + \\langle \\theta _ 0 - \\hat \\theta _ \\pi(X), \\hat \\theta _ \\pi(X) - \\hat \\theta(X)\\rangle.\n\\]\nWriting the expected risk as an expected posterior loss, i.e. using the fact that\n\\[\n\\mathbb{E} _ {\\theta _ 0 \\sim \\pi}\\left[\\mathbb{E} _ {X \\sim p _ {\\theta _ 0}}[\\,\\cdot\\,]\\right] = \\mathbb{E} _ {X \\sim m}\\left[\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\,\\cdot\\,]\\right]\n\\]\nwhere \\(m\\) has density \\(m(x) = \\int p _ \\theta(x) \\pi(\\theta)\\,d\\theta\\), and since\n\\[\n\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}\\left[\\langle \\theta _ 0 - \\hat \\theta _ \\pi(X), \\hat \\theta _ \\pi(X) - \\hat \\theta(X)\\rangle\\right] = 0,\n\\]\nwe obtain the result.\nA few remarks:\nThe expected risk has stability properties. If \\(\\tilde \\pi\\) and \\(\\pi\\) are two priors that are absolutely continuous with respect to each other, and if \\(\\|\\log \\frac{d\\tilde \\pi}{d\\pi}\\| _ \\infty \\leq C\\), then\n\\[\n   e^{-C}B _ \\pi(\\hat\\theta) \\leq B _ {\\tilde \\pi}(\\hat \\theta) \\leq e^C B _ {\\pi}(\\hat \\theta).\n\\]\nIf the risk \\(R(\\hat \\theta; \\theta _ 0)\\) is uniformly bounded by some constant \\(M\\) over \\(\\theta _ 0\\in \\Theta\\), then\n\\[\n   B _ {\\tilde \\pi}(\\hat \\theta) \\leq \\sqrt{M B _ {\\pi}(\\hat \\theta)} \\left\\|d\\tilde\\pi/d\\pi\\right\\| _ {L^2(\\pi)}.\n\\]\nThis shows how small chances in the prior does not result in a dramatic change in the expected loss of an estimator, as long as the priors have “compatible tails” (i.e. a manageable likelihood ratio).\nIt is sometimes advocated to choose the prior \\(\\pi\\) so that the risk \\(R(\\hat \\theta _ \\pi; \\theta _ 0)\\) is constant over \\(\\theta _ 0\\): the resulting estimator \\(\\hat \\theta _ \\pi\\) is then agnostic, from a risk point of view, to \\(\\theta _ 0\\). This may result in a sample-size dependent prior (which is arguably not in the Bayesian spirit), but the fun thing is that it makes the expected risk maximal and the Bayes estimator \\(\\hat \\theta _ \\pi\\) minimax: \\(\\hat \\theta _ \\pi \\in \\arg\\min _ {\\hat\\theta} \\sup _ {\\theta _ 0}R(\\hat \\theta;\\theta _ 0)\\). Indeed, in that case we have for any estimator \\(\\hat \\theta\\) that \\(\\sup _ {\\theta _ 0} R(\\hat \\theta; \\theta _ 0) \\geq B _ \\pi(\\hat \\theta) \\geq B _ \\pi(\\theta _ \\pi) = \\sup _ {\\theta _ 0}R(\\hat \\theta _ \\pi;\\theta _ 0)\\), from which it follows that \\(\\hat \\theta _ \\pi\\) is minimax.\nThe idea of minimizing expected risk is not quite Bayesian, since it required us to first average over all data possibilities when computing the risk. One of the main advantage of the Bayesian framework is that it allows us to condition over the observed data, rather than pre-emptively considering all possibilities, and we can try to make use of that. Define the posterior expected loss (or posterior risk) or an estimator \\(\\hat \\theta\\), conditionally on \\(X\\), as\n\\[\n   R _ \\pi(\\hat \\theta\\mid X) = \\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}\\left[(\\hat \\theta(X) - \\theta _ 0)^2\\right].\n\\]\n\nIt is clear from the previous computations that the posterior mean estimate minimizes the posterior risk, and hence the two approaches are equivalent. It turns out that, whatever the loss function we consider (under some regularity condition ensuring that stuff is finite and minimizers exist), minimizing the posterior risk is equivalent to minimizing the Bayes risk. In other words, we have that for any loss function (again under some regularity conditions ensuring finiteness and existence of stuff), we have\n\n\\[\n   \\arg\\min _ {\\hat \\theta}\\mathbb{E} _ {X \\sim m}\\left[\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\ell(\\hat \\theta(X), \\theta _ 0)]\\right] = \\arg\\min _ {\\hat\\theta}\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\ell(\\hat \\theta(X), \\theta _ 0)].\n\\]\n\nThis is roughly self-evident if we think about it. An interesting consequence is that any estimator minimizing a Bayes risk is a function of the posterior distribution.\n\n2. Randomized estimation and information risk minimization\nLet \\(\\Theta\\) be a model, let \\(X \\sim Q\\) be some data and let \\(\\ell _ \\theta(X)\\) be a loss associated with using \\(\\theta\\) to fitting the data \\(X\\). For instance, we could have \\(\\Theta = \\{f:\\mathcal{X} \\rightarrow \\mathbb{R}\\}\\) a set of functions, \\(X =\\{(U _ i, Y _ i)\\} _ {i=1}^n \\subset \\mathcal{X}\\times \\mathbb{R}\\) a set of features with associated responses, and \\(\\ell _ \\theta(X) = \\sum _ {i}(Y _ i -\\theta(U _ i))^2\\) the sum of squared loss.\nThere may be a parameter \\(\\theta _ 0\\in\\Theta\\) minimizing the risk \\(R(\\theta) = \\mathbb{E} _ {X\\sim Q}[\\ell _ \\theta(X)]\\), which will then be our learning target. Now we consider randomized estimators taking the form \\(\\theta\\sim \\hat \\pi _ X\\), where \\(\\hat\\pi _ X\\) is a data-dependent distribution, and the performance of this estimation method can then be evaluated by the empirical risk \\(R _ X (\\hat\\pi _ X) = \\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\ell _ \\theta(X)]\\).\nHere we should be raising an eyebrow. There is typically no point in having the estimator \\(\\theta\\) being random, i.e. we typically will prefer to take \\(\\hat \\pi _ X\\) a point mass rather than anything else. But bear with me for a sec. The cool thing is that if we choose\n\\[\n\\hat \\pi _ X = \\arg\\min _ {\\hat \\pi _ X} \\left\\{R(\\hat \\pi _ X) + D(\\hat \\pi _ X \\| \\pi)\\right\\}, \\tag{$*$}\n\\]\nwhere \\(D(\\hat \\pi _ X\\| \\pi) = \\int \\log \\frac{d\\hat \\pi _ X}{d\\pi} \\,d\\hat \\pi _ X\\) is the Kullback-Leibler divergence, then this distribution will satisfy\n\\[\nd\\hat \\pi _ X(\\theta) \\propto e^{-\\ell _ \\theta(X)}d\\pi(\\theta).\n\\]\nThat is, Bayesian-type posteriors arise by minimizing the empirical risk of a randomized estimation scheme penalized by the Kullback-Leibler divergence form prior to posterior (Zhang, 2006).\nFor the proof, write\n\\[\nR _ X(\\hat \\pi _ X) + D(\\hat \\pi _ X \\| \\pi) = \\int \\left(\\ell _ \\theta(X) + \\log\\frac{d\\hat \\pi _ X(\\theta)}{d\\pi(\\theta)}\\right) d\\hat \\pi _ X (\\theta)=\\int\\left(\\log\\frac{d\\hat\\pi _ X(\\theta)}{e^{-\\ell _ \\theta(X)}d\\pi(\\theta)}\\right)d\\hat \\pi _ X(\\theta)\n\\]\nwhich is also equal to \\(D(d\\hat \\pi _ X \\| e^{-\\ell _ \\theta(X)} d\\pi)\\) and, by properties of the Kullback-Leibler divergence, obviously minimized at \\(d\\hat \\pi _ X \\propto e^{\\ell _ \\theta(X)}d\\pi(\\theta)\\).\nIs this practically useful and insightful? Possibly. But at least this approach is suited to a general theory, as shown in Zhang (2006) and as I reproduce below.\nLet us introduce a Rényi-type generalization error defined, for \\(\\alpha \\in (0,1)\\), by\n\\[\nd _ \\alpha(\\theta; Q) = -\\alpha^{-1}\\log\\mathbb{E} _ {X' \\sim Q}[e^{-\\alpha \\ell _ \\theta(X')}].\n\\]\nThis is a measure of loss associated with the use of a parameter \\(\\theta\\) to fit new data \\(X' \\sim Q\\). We also write\n\\[\nd _ \\alpha(\\hat \\pi _ X; Q) = -\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}\\left[ \\alpha^{-1}\\log\\mathbb{E} _ {X' \\sim Q}[e^{-\\alpha \\ell _ \\theta(X')}] \\right]\n\\]\nfor the expected Rényi generalization error when using the randomization scheme \\(\\theta \\sim \\hat \\pi _ X\\).\nIn order to get interesting bounds on this generalization error, we can follow the approach of Zhang (2006).\nChange of measure inequality\nWe’ll need the change of measure inequality, which states that for any function \\(f\\) and distributions \\(\\pi\\), \\(\\hat \\pi\\) on \\(\\Theta,\\)\n\\[\n\\mathbb{E} _ {\\theta \\sim \\hat\\pi}[f(\\theta)] \\leq D(\\hat \\pi \\| \\pi) + \\log \\mathbb{E} _ {\\theta \\sim \\pi}\\left[e^{f(\\theta)}\\right].\n\\]\nIndeed, with some sloppyness and Jensen’s inequality we can compute\n\\[\n\\log \\int e^{f(\\theta)}\\pi(d\\theta)\\geq \\int f(\\theta)\\log(d\\pi/d\\hat\\pi(\\theta))d\\hat \\pi = \\mathbb{E} _ {\\theta \\sim \\hat \\pi}[f(\\theta)] - D(\\hat \\pi\\|\\pi).\n\\]\nGeneralization error bound\nWe can now attempt bounding \\(d _ \\alpha(\\hat \\pi _ X;Q)\\). Consider the difference \\(\\Delta _ X (\\theta) = d _ \\alpha(\\theta;Q) - \\ell _ \\theta(X)\\) between the generalization error and the empirical loss corresponding to the use of a fixed parameter \\(\\theta\\). Then by the change of measure inequality,\n\\[\n\\exp\\{\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\Delta _ X(\\theta)] - D(\\hat \\pi _ X\\|\\pi)\\} \\leq \\mathbb{E} _ {\\theta \\sim \\pi}\\left[e^{\\Delta _ X(\\theta)}\\right]\n\\]\nand hence for any \\(\\pi\\),\n\\[\n\\mathbb{E} _ {X \\sim Q}\\left[\\exp\\left\\{\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\Delta _ X(\\theta)] - D(\\hat \\pi _ X\\|\\pi)\\right\\}\\right] \\leq \\mathbb{E} _ {X \\sim Q}\\left[\\mathbb{E} _ {\\theta \\sim \\pi}\\left[e^{\\Delta _ X(\\theta)}\\right]\\right] = 1\n\\]\nBy Markov’s inequality, this implies that \\(\\forall t > 0\\),\n\\[\n\\mathbb{P}\\left(\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\Delta _ X(\\theta)] - D(\\hat \\pi _ X\\|\\pi) \\geq t\\right) \\leq e^{-t}.\n\\]\nRewriting yields\n\\[\nd _ \\alpha(\\hat \\pi _ X;Q) \\leq R _ X(\\hat \\pi _ X) + D(\\hat \\pi _ X\\|\\pi) + t\n\\]\nwith probability at least \\(1-e^{-t}\\). To recap: the term \\(d _ \\alpha(\\hat \\pi _ X;Q)\\) is understood as a generalization error, on the right hand side \\(R _ X(\\hat \\pi _ X) = \\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\ell _ \\theta(X)]\\) is the empirical risk, the Kullback-Leibler divergence \\(D(\\hat \\pi _ X\\|\\pi)\\) penalizes the complexity of \\(\\hat\\pi _ X\\) seen as a divergence from a “prior” \\(\\pi\\), and \\(t\\) is a tuning parameter.\n3. Online learning, regret and Kullback-Leibler divergence\nFollowing Barron (1998), suppose we sequentially observe data points \\(X _ 1, X _ 2, X _ 3, \\dots\\) which are say i.i.d. with common distribution \\(Q\\) with density \\(q\\). At each time step \\(n\\), the goal is to predict \\(X _ {n+1}\\) using the data \\(X^n = (X _ 1, \\dots, X _ n)\\). Our prediction is not a point estimate of \\(X _ {n+1}\\), but somewhat similarly as in the randomized estimation scenario we output a density estimate \\(\\hat p _ n = p(\\cdot \\mid X^n)\\), the goal being that \\(p(X _ {n+1}\\mid X^n)\\) be as large as possible. A bit more precisely, we individually score a density estimate \\(\\hat p _ n\\) through the risk \\(\\ell _ q(\\hat p _ n) = \\mathbb{E} _ {X _ {n+1}\\sim q}[\\log(q(X _ {n+1})/\\hat p _ n(X _ {n+1} ))] = D(q\\| \\hat p _ n)\\) which is the Kullback-Leibler divergence between \\(\\hat p _ n\\) and \\(q\\). The regret over times \\(n=1, 2,\\dots, N\\) is the sum of the risk over the whole process, i.e.\n\\[\n\\text{regret} = \\sum _ {n=1}^N D(q\\| \\hat p _ n).\n\\]\nFormally, this process is equivalent to estimating the distribution of \\(X^N\\) all at once: our density estimate \\(\\hat p^N\\) of \\(X^N\\) would simply be\n\\[\n\\hat p^N(X^N) = \\prod _ {n=1}^N \\hat p _ n(X _ n)\n\\]\nand the regret is, by the chain rule, simply \\(D(q^N \\| \\hat p^N)\\), where \\(q^N\\) is the \\(N\\)th independent product of \\(q\\).\nGiven a prior \\(\\pi\\) over a space of distributions for \\(q\\), our problem then to minimize the Bayes risk\n\\[\nB _ \\pi(\\hat p^N) = \\mathbb{E} _ {q\\sim \\pi} D(q^N\\|\\hat p^N).\n\\]\nThis is achieved by choosing \\(\\hat p^N(x) = \\hat p _ \\pi^N(x) = \\int q^N(x) \\pi(dq)\\) the prior predictive density. This is equivalent to using, at each time step \\(n\\), the poterior predictive density \\(\\hat p _ {n, \\pi}(x) = \\int q(x) \\,\\pi(dq\\mid \\{X _ i\\} _ {i=1}^n)\\).\nTo see this minimizing property of the Bayes average, it suffices to write\n\\[\nB _ \\pi(\\hat p^N) = \\mathbb{E} _ {q \\sim \\pi} \\left[D(q^N\\| \\hat p _ \\pi^N)\\right] + D(\\hat p _ \\pi^N \\| \\hat p^N).\n\\]\nNote that an consequence of this analysis is also that the posterior predictive distribution \\(\\hat p _ {n, \\pi}\\) will minimize the expected posterior risk:\n\\[\n\\hat p _ {n, \\pi} \\in \\arg\\min _ {\\hat p _ {n}} \\mathbb{E} _ {q \\sim \\pi(\\cdot\\mid X^n)}\\left[D(q\\|\\hat p _ n)\\right].\n\\]\nFollowing section 1, this furthermore means that the posterior predictive distribution minimizes the Bayes risk associated with the Kullback-Leibler loss.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-global-bounds-for-the-jensen-functional/",
    "title": "Global bounds for the Jensen functional",
    "description": "Some techniques to bound the Jensen functional.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2019-05-19",
    "categories": [],
    "contents": "\nGiven a convex function \\(\\varphi : \\mathbb{R} \\rightarrow \\mathbb{R}\\) and \\(X\\) a random variable on \\(\\mathbb{R}\\), the Jensen functional of \\(\\varphi\\) and \\(X\\) is defined as\n\\[\n\\mathcal{J}(\\varphi, X) = \\mathbb{E}[\\varphi(X)] -\\varphi(\\mathbb{E}[X]).\\tag{1}\n\\]\nThe well-known Jensen inequality states that \\(\\mathcal{J}(\\varphi, X) \\geq 0\\). For instance, if \\(\\varphi(x) = x^2\\), then \\(\\mathcal{J}(\\varphi, X) = \\text{Var}(X) \\geq 0\\). If \\(\\mu\\) and \\(\\nu\\) are two probability measures, \\(X \\sim \\nu\\) and \\(\\varphi\\) is convex with \\(\\varphi(1) = 0\\), then \\(\\mathcal{J}(\\varphi, \\tfrac{d\\mu}{d\\nu}(X)) =: D_\\varphi(\\mu, \\nu)\\) is a so-called \\(f\\)-divergence between probability measures such as the total variation distance, the Kullback-Leibler divergence, the \\(\\chi^2\\) divergence, etc.\nIf \\(X\\) is bounded, then a converse to the Jensen inequality can be easily obtained as follows: let \\(m\\) and \\(M\\) be the infimum and maximum of \\(X\\), and write \\(X = \\alpha m + (1-\\alpha)M\\) for some random variable \\(\\alpha\\) taking values in \\([0,1]\\). Then \\(\\mathbb{E}[\\alpha] = (M - \\mu)/(M-m)\\) and consequently with \\(\\mu:= \\mathbb{E}[X]\\),\n\\[\n\\mathcal{J}(\\varphi, X) \\leq \\mathbb{E}[\\alpha\\phi(m) + (1-\\alpha)\\phi(M)] - \\varphi(\\mu)\\\\\n\\qquad= \\frac{(M-\\mu)\\varphi(m) + (\\mu-m)\\varphi(M)}{M-m}- \\varphi(\\mu).\\tag{2}\n\\]\n\nWhen \\(\\mu\\) is unknown in practice, then maximizing the above over all possibilities is the bound \\[\n\\mathcal{J}(\\varphi, X) \\leq \\max_{p \\in [0,1]} \\left\\{p\\varphi(m) + (1-p)\\varphi(M) - \\varphi(pm + (1-p) M)\\right\\}\\tag{3}\n\\]\nwhich is Theorem C in Simic (2011).\nSome examples\nVariance bound. Consider for example the case where \\(\\varphi(x) = x^2\\), so that \\(\\mathcal{J}(\\varphi, X) = \\text{Var}(X)\\). Then for \\(X\\) taking values in say \\([0,1]\\), the above bounds read as\n\\[\n\\text{Var}(X) \\leq \\mu(1-\\mu) \\leq 1/4\n\\]\nwhich is a well-known elementary result.\n\\(f\\)-divergence bounds. In (Binette, 2019), I show how we can use similar ideas to get best-possible reverse Pinsker inequalities: upper bounds on \\(f\\)-divergences in terms of the total variation distance and likelihood ratio extremums. In particular, with \\(D(\\mu\\|\\nu) = \\int \\log\\left(\\frac{d\\mu}{d\\nu}\\right) d\\mu\\) the Kullback-Leibler divergence between the probability measures \\(\\mu\\) and \\(\\nu\\), we find that if \\(a = \\inf \\frac{d\\nu}{d\\mu}\\) and \\(b = \\sup \\frac{d\\nu}{d\\mu}\\), then \\[\nD(\\mu|\\nu) \\leq \\sup_A|\\mu(A) - \\nu(A)| \\left(\\frac{\\log(a)}{a-1} +\\frac{\\log(b)}{1-b}\\right).\n\\]\nApplying again the Jensen functional bound to \\(\\sup_A \\lvert \\mu(A)-\\nu(A) \\rvert = \\frac{1}{2}\\int\\left \\lvert \\frac{d\\mu}{d\\nu} - 1\\right \\rvert d\\nu\\), we obtain\n\\[\n\\sup_A|\\mu(A) - \\nu(A)| \\leq \\frac{(M-1)(1-m)}{M-m}\n\\]\nand this implies the range of values theorem\n\\[\nD(\\mu|\\nu) \\leq \\frac{(a-1)\\log(b) + (1-b)\\log(a)}{b-a}.\n\\]\nVariations\nIn cases where \\(\\mu\\) is unknown and optimizing over all possibilities is not quite feasible, we can use the following trick.\nLet \\(f(x) = x\\varphi(m) + (1-x)\\varphi(M) - \\varphi(x m +(1-x)M)\\) be the term involved in the maximization step of \\((3)\\). Then \\(f\\) is concave with \\(f(0) = f(1) = 0\\), and hence for any \\(p \\in (0,1)\\) we have that\n\\[\n\\max_{x \\in [0,1]} f(x) \\leq (\\min\\{p, 1-p\\})^{-1}f\\left(pm +(1-p)M\\right).\n\\]\nIn particular, taking \\(p = 1/ 2\\), we obtain the result of Simic (2008) stating that\n\\[\n\\mathcal{J}(\\varphi, X) \\leq \\varphi(m) + \\varphi(M) - 2\\varphi\\left(\\frac{m+M}{2}\\right).\n\\]\nWhen \\(\\varphi\\) is differentiable (this assumption is not strictly necessary but it facilitate the statements), then we can use the concavity of \\(f\\) (using the fact that \\(f(0) = f(1) = 0\\)) to very easily obtain\n\\[\n\\mathcal{J}(\\varphi, X) \\leq \\frac{f'(1)f'(0)}{f'(1)-f'(0)} \\leq \\frac{1}{4}(f'(0)-f'(1)) = \\frac{1}{4}(M-m)(\\varphi'(M) - \\varphi'(m))\n\\]\nwhich is an inequality attributed to S.S. Dragomir (1999), although I haven’t managed to find the original paper yet.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-two-sampling-algorithms-for-trigonometric-densities/",
    "title": "Two sampling algorithms for trigonometric densities",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2019-04-15",
    "categories": [],
    "contents": "\nTrigonometric densities (or non-negative trigonometric sums) are probability density functions of circular random variables (i.e. \\(2\\pi\\)-periodic densities) which take the form \\[\nf(u) = a_0 + \\sum_{k=1}^n(a_k \\sin(k u) + b_k\\cos(ku)) \\tag{1}\n\\]\nfor some real coefficients \\(a_k, b_k \\in \\mathbb{R}\\) which are such that \\(f(u) \\geq 0\\) and \\(a_0 = \\frac{1}{2\\pi} \\int f(u)\\,du = (2\\pi)^{-1}\\). These provide flexible models of circular distributions. Circular density modelling comes up in studies about the mechanisms of animal orientation and also come up in bio-informatics in relationship to the protein structure prediction problem (the secondary structure of a protein - the way its backbone folds - is determined by a sequence of angles).\nHere I am discussing two simple sampling algorithms for such trigonometric densities. The first is the rejection sampling algorithm proposed in Fernández-Durán et al. (2014) and the second uses negative mixture sampling.\n\nParametrizing trigonometric densities\nBy Féjer’s Theorem, the conditions on the coefficients \\(a_k\\) and \\(b_k\\) can be stated as follows: there exists a vector of complex coefficients \\(c = (c_0, c_1, \\dots, c_n)\\) with \\(\\|c\\|^2 = (2\\pi)^{-1}\\) and satisfying\n\\[\nf(u) = \\left\\| \\sum_{k=0}^n c_k e^{ik u} \\right\\|^2. \\tag{2}\n\\]\nThis provides an explicit parametrization of the space of trigonometric densities in terms of a complex hypersphere. See Fernandez-Duran (2004) for more details.\nDensity basis of the trigonometric polynomials\nIn Binette & Guillotte (2019), we studied the De la Vallée Poussin density basis of the trigonometric polynomials given by\n\\[\nC_{j,n}(u) = \\frac{2^n}{2\\pi {2n \\choose n}} \\left(1+\\cos\\left(u - \\tfrac{2\\pi j}{2n+1}\\right)\\right)^n,\\quad j\\in \\{0,1,\\dots, 2n\\}. \\tag{3}\n\\]\nThese can be used to express trigonometric densities as mixtures of probability density functions (instead of the functions \\(\\cos\\) and \\(\\sin\\), and the change of basis formula follows from the expression\n\\[\nC_{j,n}(u) = T_{j,n}\\,\\left[e^{-i nu}\\; \\cdots\\; e^{-i u}\\; 1\\; e^{i u}\\; \\cdots\\; e^{i nu}\\right]^{T}\n\\]\nwhere\n\\[\nT_{j,n} = \\left[\\exp\\left\\{ -i\\frac{2\\pi j p}{2n+1} u \\right\\}{2n \\choose n-p} \\Big / {2n \\choose n}\\right]_{p \\in \\{-n, \\dots, n\\}}.\n\\]\nWe’re using the complex functions \\(e^{i2\\pi k u}\\) instead of \\(\\sin\\) and \\(\\cos\\) simply because they are neater to work with; it doesn’t change much otherwise.\nWe also show in our paper that if \\(V \\sim \\text{Ber}(1 / 2)\\) and \\(W \\sim \\text{Beta}(1 / 2, 1 / 2+n)\\), then\n\\[\n(1-2V)\\arccos(1-2W) +\\tfrac{2\\pi j}{2n+1} \\sim C_{j,n}.\n\\]\nThis provide an easily formula to sample from the basis functions \\(C_{j,n}\\) and their mixtures.\nAlgorithm 1: Naive rejection sampling\nGiven an uniform upper bound \\(C\\) on the family \\(\\mathcal{V}_n\\) of trigonometric densities, we can sample from a given \\(f\\in \\mathcal{V}_n\\) using simple rejection sampling as follows:\nLet \\((x, y)\\) be uniformly distributed over \\([0, 2\\pi) \\times [0, C]\\);\nIf \\(y \\leq f(x)\\), then return \\(x\\); otherwise return to step 1.\nNow the problem is to figure out a good upper bound \\(C\\). The most basic idea is to do as in Fernandez-Duran et al. (2014) and to apply the Cauchy-Schwarz inequality\n\\[\nf(u) = \\left\\| \\sum_{k=0}^n c_k e^{i k u} \\right\\|^2 \\leq \\|c\\|^2 \\sum_{k=0}^n|e^{iku}| = \\frac{n+1}{2\\pi}.\n\\]\nCan we find a better bound? I think that \\(C = \\sqrt{n}/\\pi\\) would work, but I have no clue how to prove it….\nLet’s implement this in R.\nImplementation\nFirst we need a trigonometric density model.\ntrig_function <- function(c_real, complex=NULL) {\n  # Returns the trigonometric function defined as either:\n  #     f(u) = 1/(2\\pi) + \\sum_{k=1}^{n} c_real[2*k-1] \\sin(k u) + c_real[2*k] \\cos(ku),\n  # or\n  #   f(u) = \\| \\sum_{k=0}^n complex e^{i k u} \\|^2,\n  # where n is the degree of the polynomial.\n  #\n  # Args\n  #   c_real: Vector of 2*n real numbers, where n is the degree of \n  #           the trigonometric polynomial.\n  #   complex: Vector of (n+1) complex numbers.\n  \n  if (!is.null(complex)) {\n    lambd <- function(u) {\n      n = length(complex) - 1\n      k = 0:n\n      return(abs(sum(complex * exp(u * k * 1i)))**2)\n    }\n  }\n  else {\n    lambd <- function(u) {\n      n = length(c_real)/2\n      k = 1:n\n      return(1/(2*pi) + sum(c_real[2*k - 1] * cos(k*u)) + sum(c_real[2*k] * cos(k*u)))\n    }\n  }\n  return(Vectorize(lambd));\n}\nWe can also generate random trigonometric densities of a fixed degree as follows.\nrtrig <- function(n) {\n  u = rnorm(n);\n  v = rnorm(n);\n  c_comp = u + v*1i;\n  c_comp = c_comp / (sqrt(2*pi*sum(abs(c_comp)**2)));\n  return(trig_function(complex=c_comp))\n}\nUsage is like this:\nu = seq(0, 2*pi, 0.005)\nplot(u, rtrig(10)(u), type=\"l\")\n\nAnd finally we can implement the naive rejection sampling algorithm.\nnaive_rejection_sampling <- function(f, n) {\n  # Returns a random variate following the trigonometric density f of degree n.\n  drawn = FALSE\n  while(!drawn) {\n    x = runif(1)*2*pi\n    y = runif(1)*(n+1) / (2*pi)\n    if (y < f(x)) {\n      drawn = TRUE\n    }\n  }\n  return(x);\n}\nAlgorithm 2: Negative Mixture Sampling\nAnother approach to simulate from trigonometric densities relies on the De la Vallée Poussin mixture representation. That is, any \\(f\\in \\mathcal{V}_n\\) can be written as\n\\[\nf = \\alpha f_a - (\\alpha - 1) f_b,\\qquad f_a = \\sum_{j=0}^{2n} a_j C_{j_n}, \\quad f_b = \\sum_{j=0}^{2n} b_j C_{j,n},\n\\]\nwhere \\(\\alpha \\geq 1\\), \\(a_j, b_j \\geq 0\\) and \\(\\sum_ j a_j = \\sum_ j b_ j = 1\\). We can assume that \\(a_j b_j = 0\\) for every \\(j\\); i.e. there is no redundancy in the components of \\(f_a\\) and \\(f_b\\). The density \\(f_b\\) accounts for negative weights in the mixture representation of \\(f\\) using the De la Vallée Poussin densities \\((3)\\).\nWe can now sample from \\(f\\) using samples from \\(f_a\\) and a simple rejection method.\nAlgorithm 2.\nLet \\(x \\sim f_a\\).\nReturn \\(x\\) with probability \\(\\frac{f(x)}{\\alpha f_a(x)}\\); otherwise return to step 1.\nImplementation\nDe la Vallée Poussin densities and its random variate generator.\ndvallee <- function(u, j, n) {\n  # De la Vallée Poussin density $C_{j,n}(u)$\n  \n  return(2^n * (1+cos(u - (2*pi*j)/(2*n+1)))^n / (2*pi*choose(2*n, n)))\n}\nrvallee <- function(j, n, m) {\n  # Returns m random variates following the De la Vallée Poussin density $C_{j,n}$.\n  \n  V = runif(m) > 0.5\n  W = rbeta(m, 1/2, 1/2 + n)\n  return((1-2*V)*acos(1-2*W) + (2*pi*j)/(2*n + 1))\n}\nUsage:\ns = rvallee(2, 5, 10000)\nu = seq(-pi, pi, 0.05)\nhist(s, prob=TRUE, xlim=c(-pi,pi))\nlines(u, dvallee(u, 2, 5), col=2)\n\nDe la Vallée Poussin mixtures.\ndValleeMixture <- function(coeffs) {\n  # De la Vallée Poussin mixture densities\n  \n  n = (length(coeffs) - 1)/2;\n  \n  lambd <- function(u) {\n    j = 0:(2*n)\n    return(sum(dvallee(u, j, n) * coeffs))\n  }\n  \n  return(Vectorize(lambd))\n}\nrValleeMixture <- function(coeffs) {\n  # Random sample from a De la Vallée Poussin mixture density. The mixture weights are allowed to take negative values.\n  \n  f = dValleeMixture(coeffs)\n  n = (length(coeffs) - 1)/2\n\n  a = coeffs * (coeffs > 0)\n  b = coeffs * (coeffs < 0)\n  \n  alpha = sum(a)\n  a = a / alpha\n  b = b / (1-alpha)\n  fa = dValleeMixture(a)\n  \n  drawn = FALSE\n  while(!drawn) {\n    # Sample from f_a\n    i = sample(0:(2*n), 1, prob = a)\n    x = rvallee(i, n, 1)\n    if ( runif(1) <  f(x)/(alpha*fa(x))) {\n      drawn = TRUE\n    }\n  }\n  \n  return(x %% (2*pi))\n}\nExample:\ncoeffs = c(0.55, -0.15, 0.55, 0, 0, 0,0.05)\nf = dValleeMixture(coeffs)\nu = seq(0, 2*pi, 0.05)\ns = replicate(50000, rValleeMixture(coeffs))\nhist(s, prob=T, ylim=c(0, 0.6))\nlines(u, f(u), col=2)\n\nOther things we could do:\nThe black box Lipschitz sampling algorithm can also be used to sample from trigonometric densities. This requires to compute good upper bounds on the Lipchitz constant on the density, which should be doable using the De la Vallée Poussin mixture representation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-the-significance-of-the-adjusted-r-squared-coefficient/",
    "title": "The Significance of the adjusted R squared coefficient",
    "description": "A sound re-interpretation of the adjusted $R^2$ value for model comparison.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2019-04-10",
    "categories": [],
    "contents": "\nMy friend Anthony Coache and I have been curious about uses and misuses of the adjusted \\(R^2\\) coefficient which comes up in linear regression for model comparison and as a measure of “goodness of fit”. We were underwhelmed by the depth of the literature arguing for its use, and wanted to show exactly how it behaves under certain sets of assumptions. Investigating the issue brought us to re-interpret the adjusted \\(R^2\\) and to highlight a new distribution-free perspective on nested model comparison which is equivalent, under Gaussian assumptions, to Fisher’s classical \\(F\\)-test. This generalizes to nested GLMs comparison and provides exact comparison tests that are not based on asymptotic approximations. We still have many questions to answer, but here’s some of what we’ve done.\n\nSo, in the context of least squares linear regression, the model for relating a vector of \\(n\\) observed responses \\(Y\\) to \\(p-1\\) independent covariates is \\(Y = X \\beta + \\varepsilon\\), where \\(X\\) is the design matrix and \\(\\varepsilon\\) is the vector of random errors. One of many summary statistics arising from data analyses based on this model is the adjusted \\(R^2\\) coefficient, defined as\n\\[R^2_a(Y, X) = 1 - \\frac{\\|\\hat \\varepsilon\\|^2}{\\left\\|Y - \\bar Y \\right\\|^2}\\frac{n-1}{n-p},\\]\nwhere \\(\\hat \\varepsilon\\) is the vector of residual errors and \\(\\bar Y\\) is the mean of \\(Y\\) (Cramer, 1987; Ohtani, 2004). The \\(R^2\\) coefficient and its adjusted counterpart are widely used as measures of goodness of fit, as model selection criteria and as estimators of the squared multiple correlation coefficient \\(\\rho^2\\) of the parent population. While their properties have been thoroughly studied in these contexts (Olkin, 1958; Helland, 1987; Cramer, 1987; Meepagala, 1992; Ohtani, 2004), the literature is scarce in explanations as to what, exactly, \\(R^2_a\\) adjusts for in non-trivial cases. It is not an unbiased estimator of \\(\\rho^2\\) and the degrees of freedom adjustment heuristic (Theil, 1971) is of limited depth.\nHere we show in what sense the adjusted \\(R^2\\) coefficient may be considered “unbiased”. For nested models comparison, we also suggest how to test the significance of a \\(R^2_a\\) difference between two nested models which is equivalent to Fisher’s \\(F\\)-test under Gaussian assymptions. The \\(R^2_a\\) test is however done from a largely distribution-free perspective which is conditional on the observation of \\(Y\\). The results are then reinterpreted under classical Gaussian assumptions, which emphasize the dual perspectives between those two tests.\nModel and notations\nGiven a matrix \\(A\\), let \\(\\text{Span}(A)\\) denote the subspace spanned by its columns and \\(P_A\\) be the (orthogonal) projection on \\(\\text{Span}(A)\\).\nThe commonly used linear regression model is \\[Y = X \\beta + \\varepsilon,\\] where \\(Y\\) is a \\(n \\times 1\\) vector of observed responses, the design matrix \\(X\\) consists of a constant column vector followed by \\(p-1\\) column vectors of covariates, \\(\\beta\\) is the vector of parameters to be estimated and \\(\\varepsilon\\) is the vector of random errors. The fixed design matrix is supposed to be non-random and of full rank \\(p\\). Let also \\(\\hat \\varepsilon = Y - P_X Y\\) denote the residuals errors obtained by linear least squares fitting.\nTesting for an increase of \\(R^2_a\\)\nSuppose we have two design matrices \\(X\\) and \\(\\tilde X\\), where \\(\\text{Span}(X) \\subset \\text{Span}(\\tilde X)\\). Let \\(p=\\text{rank}(X)\\) and \\(\\tilde p = \\text{rank}(\\tilde X) = p+k\\). Given the vector of observations \\(Y\\), we observe two values \\(R^2 _ a (Y, X)\\) and \\(R^2_ a (Y, \\tilde X)\\) associated to the nested models. The classical way to test for a significant increase of \\(R^2 _ a\\) is to carry out Fisher’s \\(F\\)-test based on the statistics\n\\[\nF = \\frac{\\| \\hat Y_0 - \\hat Y \\|^2}{\\| Y - \\hat Y \\|^2} \\frac{n - \\tilde p }{k},\n\\]\nwhere \\(\\hat Y_0 = P_X Y\\) and \\(\\hat Y = P_ {\\tilde X}Y\\). This is a function of both \\(R^2_a(Y, X)\\) and \\(R^2_a(Y, \\tilde X)\\), which, under the assumption\n\\[\nH_0: \\quad Y = X \\beta + \\varepsilon\n\\]\nfor \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), has an \\(F\\)-distribution.\nThis is, however, a rather convoluted way of going about comparing the two numbers \\(R^2_a(Y, X)\\) and \\(R^2_a(Y, \\tilde X)\\). Can we do simpler, and can we drop the Gaussian assumption? The answer is yes, although we’ll have to change a bit our point of view on the problem.\nA Dual perspective on nested model comparison\nThe whole point of nested model comparison is to see if the new covariates in \\(\\tilde X\\), i.e. those that are not part of \\(\\text{Span}(X)\\), bring new information about \\(Y\\). In the context of an exploratory analysis where the observations and predictors are all observed, we propose to change our perspective to the following testing procedure:\ncondition on the observation of \\(Y\\) and \\(X\\) (consider them fixed, observed values);\ntests if the new covariates in \\(\\tilde X\\) are random noise.\nHence, rather than testing the model \\(Y = X\\beta + \\varepsilon\\) under a Gaussian noise assumption, we test for covariate randomness, our null hypothesis becomes\n\\[\nH_0':\\quad \\text{the complement of $\\text{Span}(X)$ in $\\text{Span}(\\tilde X)$ is a random subspace.}\n\\]\nThis test can be carried out using any test statistic \\(T\\), and obviously the distribution of \\(T\\) under \\(H_0'\\) (and conditionally on \\(Y\\)), will not depend on the unknown parameter \\(\\beta\\) nor on the noise structure \\(\\varepsilon\\) (which has been conditionned out of randomness). In particular, we can take \\(T = R^2_a(Y, \\tilde X)\\).\nDoes it make any sense? Well it does not change anything! The test obtained in this framework is entirely equivalent to Fisher’s \\(F\\)-test we reviewed before: for any given observation of \\(Y\\), \\(X\\) and \\(\\tilde X\\), the two tests will give the same results.\nLet me make all of this more precise.\nSome precisions\nLet \\(\\tilde X = [X \\; W] \\in \\mathbb{R}^{n \\times \\tilde p}\\) be the concatenation of \\(X\\) with a matrix \\(W = [W_1 \\, \\cdots \\, W_k]\\) of \\(k\\) new covariates. The goal is to test whether or not \\(R^2_a(Y, \\tilde X)\\) has significantly increased from \\(R^2_a(Y, X)\\). Henceforth, we shall assume that both \\(Y\\) and \\(X\\) are fixed and the null hypothesis is\n\\[\nH_0':\\; \\text{ the } W_i \\text{ are independent and of uniformly distributed directions.}\n\\]\nBy saying that \\(W_i\\) has a uniformly distributed direction, we mean that \\(W_i/\\|W_i\\|\\) is uniformly distributed on the \\(n\\)-sphere. This is satisfied, for instance, if \\(W_i \\sim N(0, \\sigma_i^2 I_n)\\) and this represents the augmentation of the covariate space through random directions. It is equivalent to saying that the complement of \\(\\text{Span}(X)\\) in \\(\\text{Span}(\\tilde X)\\) is a random subspace. The following proposition shows that the expected value of \\(R^2_a(Y, \\tilde X)\\) is invariant under the addition of such covariates and provides the distribution of \\(R^2_a(Y, \\tilde X)\\) under \\(H_0'\\).\nProposition 1. Let \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^{n \\times p}\\) be fixed and let \\(\\tilde X = [X \\; W_1 \\, \\cdots \\, W_k]\\) be the concatenation of \\(X\\) with \\(k \\leq n- p\\) independent random vectors \\(W_1, \\ldots, W_k\\) of uniformly distributed directions. Then\n\\[\n\\mathbb{E}\\left[ R^2_a(Y, \\tilde X) \\right] = R^2_a(Y, X).\n\\]\nand, more precisely, under \\(H_0\\) we have that \\(R^2_a(Y, \\tilde X)\\) is distributed as\n\\[\n1-  \\frac{(n-1)\\| \\hat \\varepsilon \\|^2}{(n-\\tilde p) \\| Y - \\bar Y \\|^2}\\text{Beta}\\left(\\tfrac{n-\\tilde p}{2}, \\tfrac{k}{2} \\right)\n\\]\nwhere \\(\\text{Beta}\\left(\\tfrac{(n-\\tilde p)}{2}, \\tfrac{k}{2} \\right)\\) is a Beta random variable of parameters \\((n-\\tilde p)/2\\) and \\(k/2\\).\nProof. Let \\(\\omega\\) be the projection of \\([W_1 \\, \\cdots \\, W_k]\\) on the orthogonal \\(V\\) of \\(\\text{Span}(X)\\) and denote by \\(P_\\omega\\) the orthogonal projection onto \\(V_\\omega = \\text{Span}(\\omega)\\). By the Pythagorean theorem we have \\(\\|Y - P_ {\\tilde X} Y \\|^2 + \\|P_\\omega \\hat \\varepsilon\\|^2 = \\|\\hat \\varepsilon\\|^2\\) and hence we may write\n\\[\n    R^2_a(Y, \\tilde X) = 1- \\frac{(n-1)\\|\\hat \\varepsilon\\|^2}{(n-\\tilde p) \\left\\| Y - \\bar Y\\right\\|^2} \\left(1 - \\frac{\\|P_\\omega \\hat \\varepsilon\\|^2}{\\|\\hat \\varepsilon\\|^2} \\right).\n\\]\nWe now derive the distribution of \\(\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2\\). This term is the squared norm of projection of the unit vector \\(\\hat \\varepsilon / \\|\\hat \\varepsilon\\| \\in V\\) on the random subspace \\(V_\\omega \\subset V\\). Let us now introduce a random unitary matrix \\(U\\) obtained by orthonormalizing \\(\\dim(V) = n - p\\) random vectors of uniformly distributed directions, so that \\(P_\\omega \\hat \\varepsilon\\) is distributed as the first \\(k\\) components of the vector \\(U \\hat \\varepsilon\\). Since \\(U\\hat \\varepsilon / \\|\\hat \\varepsilon\\|\\) is uniformly distributed on the unit sphere of \\(V\\), it follows that the squared norm of its first \\(k\\) components has a \\(\\text{Beta}(k/2, (n-\\tilde p)/2)\\) distribution. In other words, we have shown that \\(\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2 \\sim \\text{Beta}\\left(\\tfrac{k}{2}, \\tfrac{n-\\tilde p}{2} \\right)\\).\nThe expectation of \\(R^2_a(Y, \\tilde X)\\) is obtained from this distributional expression. \\(\\Box\\)\nReinterpretation under Gaussian hypotheses\nWhile the preceding analysis was conditional on the observation of \\(Y\\), suppose now that \\(Y = X \\beta + \\varepsilon\\), where \\(\\varepsilon \\sim N(0, \\sigma^2)\\) for some \\(\\sigma^2 > 0\\). The distribution of \\(R^2_a(Y, X)\\) is then intricately related to the unknown parameter \\(\\beta\\), preventing a direct analysis.\nHowever, as shown in Cramer (1987), the adjusted \\(R^2\\) coefficient can still be understood as compensating for irrelevant covariates: in a correctly specified model, its expected value is invariant under the addition of covariates. This is formalized in Proposition 2 below. We preferred a more elementary proof than found therein, avoiding the rather involved explicit expression of the expected value that depends on the unknown parameter \\(\\beta\\).\nProposition 2. Suppose \\(Y = X \\beta + \\varepsilon\\), where \\(\\beta \\in \\mathbb{R}^{p}\\) and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\) is Gaussian noise. If \\(\\tilde X\\) is another design matrix of rank \\(\\tilde p\\) such that \\(\\text{Span}(X) \\subset \\text{Span}(\\tilde X)\\), then\n\\[\n\\mathbb{E}\\left[ R^2_a(Y, \\tilde X) \\right] = \\mathbb{E}\\left[R^2_a(Y, X)\\right].\n\\]\nRemark. More precisely, we know the conditional distribution of \\(R^2_a(Y, \\tilde X)\\) given \\(R^2_a(Y, X)\\): it is the same as the distribution which appears in the context of Proposition 1. The above results then follows from a simple computation.\nProof. Let \\(\\hat \\varepsilon^{*} = Y - P_ {\\tilde X} Y\\) and write \\(\\lambda = \\left\\|\\mathbb{E}\\left[Y - \\bar Y\\right]\\right\\|^2/\\sigma^2\\). Then \\(\\frac{\\|\\hat \\varepsilon^{*}\\|^2}{\\left\\|Y - \\bar Y\\right\\|^2}\\) is distributed as \\[\n    \\frac{\\sum_ {i=1}^{n - \\tilde p} Z_i^2}{\\sum_ {i=1}^{n - \\tilde p} Z_i^2 + \\chi^2_ {\\tilde p -1} (\\lambda)}\n\\]\nfor independent \\(Z_i \\sim N(0,1)\\) and \\(\\chi^2_ {\\tilde p - 1} (\\lambda)\\) a noncentral \\(\\chi^2\\) random variable of parameter \\(\\lambda\\). Hence\n\\[\n\\mathbb{E}\\left[ \\frac{\\|\\hat \\varepsilon^{}\\|^2}{\\left\\|Y - \\bar Y\\right\\|^2} \\right] = (n-\\tilde p) \\mathbb{E}\\left[\\frac{Z_1^2}{\\sum{i=1}^{n - \\tilde p} Z_i^2 + \\chi^2{\\tilde p -1} (\\lambda)}\\right]\n    = (n-\\tilde p)K,\n\\]\nwhere \\(K = \\mathbb{E}\\left[\\frac{Z_1^2}{Z_1^2 + \\chi^2_ {n - 2} (\\lambda)}\\right]\\) and \\(\\chi^2_ {n-2}(\\lambda)\\) is a new and independent noncentral \\(\\chi^2\\) random variable. It follows that\n\\[\n    \\mathbb{E} \\left[R^2_a(Y, \\tilde X) \\right] = 1 - (n-1)K\n\\]\ndepends on \\(\\tilde X\\) only through \\(X\\) and must equal \\(\\mathbb{E} \\left[R^2_a(Y, X) \\right]\\). \\(\\Box\\)\nRelationship with Fisher’s \\(F\\)-test\nIn the context of Proposition 2, suppose in particular that \\(\\tilde X = [X \\; W]\\), where \\(W = [W_1\\, \\cdots \\, W_k]\\) is a matrix of additional fixed regressors. Recall that the \\(F\\)-statistic for Fisher’s test with nested models of \\(p\\) and \\(\\tilde p = p + k\\) parameters respectively is given by\n\\[\nF = \\frac{\\| \\hat Y_0 - \\hat Y \\|^2}{\\| Y - \\hat Y \\|^2} \\frac{n - \\tilde p }{k},\n\\]\nwhere \\(\\hat Y_0 = P_X Y\\) and \\(\\hat Y = P_{\\tilde X} Y\\) are the vector of predicted values for the models corresponding to \\(X\\) and \\(\\tilde X\\). The test of significance devised in Section 2, based on \\(R^2_a(Y, \\tilde X)\\), is then equivalent to Fisher’s \\(F\\)-test of the hypothesis\n\\[\nH_0^{\\text{Gauss}}:\\; Y = X\\beta + \\varepsilon\\, \\text{ where }\\,\\varepsilon \\sim N(0, \\sigma^2 I_n).\n\\]\nTo see this, let \\(\\omega\\) be, as in the proof of Proposition 1, the projection of \\([W_1\\,\\cdots\\,W_k]\\) on the orthogonal of \\(\\text{Span}(X)\\) and denote by \\(P_\\omega\\) the projection on \\(\\text{Span}(\\omega)\\). Then the \\(F\\)-statistic can be written as\n\\[\n    F = \\frac{\\|P_\\omega \\hat \\varepsilon\\|^2}{\\|\\hat \\varepsilon - P_\\omega \\hat \\varepsilon \\|^2} \\frac{n-\\tilde p}{k} = \\frac{\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2}{1 - \\|P_\\omega \\hat \\varepsilon \\|^2/\\|\\hat \\varepsilon\\|^2} \\frac{n-\\tilde p}{k}.\n\\]\nThis is a monotonous invertible transform of \\(\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2\\) which, under \\(H_0^{\\text{Gauss}}\\), follows a Beta distribution of parameters \\(k/2\\) and \\((n-\\tilde p)/2\\). Yet in the framework of Section 2 and under \\(H_0\\), where now \\(\\omega\\) is random and \\(\\hat \\varepsilon\\) fixed, the test statistic \\(R^2_ a(Y, \\tilde X)\\) is also a monotonous invertible function of \\(\\|P_ \\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2 \\sim \\text{Beta}(k/2, (n-\\tilde p)/2)\\). This shows that the two unilateral tests are equivalent: the same observations yield the same \\(p\\)-values.\nDiscussion\nWe have highlighted dual perspectives on nested models comparison. An increase of \\(R^2\\) may be due to random noise that correlates with fixed regressors, or to random regressors that correlate with fixed observations. Fisher’s test of the first hypothesis is equivalent to the \\(R^2_a\\) test of the second. Furthermore, we showed that \\(R^2_a\\) compensates properly, on the average, for both types of inflation of \\(R^2\\). We suggest this provides a clear explanation of what \\(R^2_a\\) exactly adjusts for and how it can properly be used for models comparison.\nFurthermore, the fact that random covariate tests, conditional on the observations, can be carried out exactly using any measure of goodness of fit (e.g. the likelihood or the AIC) suggests that our approach may be helpful in devising nested model comparison tests for GLMs. Testing at a chosen confidence level also provides more flexibility than using a rule-based procedure such as the AIC.\nReferences\nCramer, J. S. (1987). Mean and variance of r2 in small and moderate samples. Journal of Econometrics 35(2), 253 – 266.\nHelland, I. S. (1987). On the interpretation and use of r2 in regression analysis. Biometrics 43(1), 61–69.\nMeepagala, G. (1992). The small sample properties of r2 in a misspecified regression model with stochastic regressors. Economics Letters 40(1), 1 – 6.\nOhtani, K. and H. Tanizaki (2004). Exact distributions of r2 and adjusted r2 ina linear regression model with multivariate t error terms. Journal of the Japan Statistical Society 34(1), 101–109.\nOlkin, I. and J. W. Pratt (1958). Unbiased estimation of certain correlation coefficients. The Annals of Mathematical Statistics 29(1), 201–211.\nTheil, H. (1971). Principles of econometrics (1 ed.). New York: J. Wiley.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-3d-data-visualization-with-webglthreejs/",
    "title": "3D data visualization with WebGL/Three.js",
    "description": {},
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2019-01-06",
    "categories": [],
    "contents": "\n\n\nI wanted to make a web tool for high-dimensional data exploration through spherical multidimensional scaling (S-MDS). The basic idea of S-MDS is to map a possibly high-dimensional dataset on the sphere while approximately preserving a matrix of pairwise distances (or divergences). An interactive visualization tool could help explore the mapped dataset and translate observations back to the original data domain. I’m not quite finished, but I made a frontend prototype. The next step would be to implement the multidimensional scaling algorithm in Javascript. I may get to this if I find the time.\n\n\n\n\nIn the current applet, you can visualize the positions and depths of earthquakes of magnitude greater than 6 from January 1st 2014 up to January 1st 2019. Data is from the US Geological Survey (usgs.gov). Code is on GitHub.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T21:43:51-05:00",
    "input_file": "3d-data-visualization-with-webglthreejs.knit.md"
  },
  {
    "path": "posts/2020-11-15-ism-at-the-eureka-science-festival/",
    "title": "ISM at the Eureka! Science Festival!",
    "description": {},
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2018-06-07",
    "categories": [],
    "contents": "\nWe’ve been hard at work getting ready for the Eureka! science festival held this weekend at the Montreal Science Centre. Come check it out!\n\nAt the festival:\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-closed-curve-reconstruction-using-periodic-splines-fused-ridge-and-minimal-hamiltonian-cycles/",
    "title": "Closed curve reconstruction using periodic splines, fused ridge and minimal hamiltonian cycles",
    "description": {},
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2018-04-15",
    "categories": [],
    "contents": "\n\nCode on Github.\nThere are a number of “open” problems in curve/surface reconstruction, such as reconstructing self-intersections, quantifying uncertainty and topological guarantees in the presence of noise.\nIn the computer graphics/CAGD literature, it is often assumed that the set of observed points forms a dense exact sample from the object of interest. Topological properties of reconstruction algorithms are then analysed from this deterministic point of view. However, this framework is difficult to adapt to the presence of noise and can at best provide a sampling theory for \\(\\varepsilon\\)-neighbourhoods.\nThe “implicit” approaches to reconstruction supposes the existence of a smooth function \\(f\\) modelling the object of interest as the level set \\(f^{-1}(0)\\). If the observed points also carry gradient information, then there is hope that we may approximate \\(f\\) in a neighbourhood of the curve. In fact, as I show in this post (a particular case of the Nash-Tognoli theorem), approximating both \\(f\\) and its gradient \\(\\nabla f\\) in the sup norm entails the ability to approximate the surface \\(f^{-1}(0)\\) in the Hausdorff + diffeomorphic topology of compact manifold space. Somewhat similar ideas have been exploited in Kolluri (2008) to obtain topological guarantees for a moving least squares implicit method of reconstruction.\nThere has also been a few statistical approches to the problem (e.g. Gu et al. (2014)). Those I am aware of, while pertinent and original, do not bring considerably more profound insight. Likelihood based inference currently seems to bring more computational and theorical difficulties than solutions.\nIdeas and brainstorming:\nHow can we assess whether a reconstruction has been successful? Assume we are trying to reconstruct a smooth manifold and let \\(\\gamma\\) be the reconstructed curve (or hypersurface). Let \\(\\nu: \\gamma \\rightarrow \\mathbb{R}^k\\) be the normal function (i.e. \\(\\nu(x)\\) is a unit normal to \\(\\gamma\\) at the point \\(x \\in \\gamma\\)). We may be given a credible region for \\(\\gamma\\) as the set \\(C = \\{ x + \\varepsilon f(x) \\nu(x) \\mid \\varepsilon \\in [-1,1] \\}\\) where \\(f\\) quantifies pointwise normal uncertainty. Points in \\(C\\) that are closest to more than one point on \\(\\gamma\\) may be considered problematic: their existence entails that some plausible deformations of \\(\\gamma\\) (as defined by \\(f\\)) are not manifolds.\nHow can we reconstruct self-intersections? I’d like to try this: \\(\\arg \\min_\\gamma \\sum_{i} \\text{dist}(y_i, \\gamma) + \\lambda \\int \\|\\gamma''\\|\\) where \\(\\gamma\\) is parametrized by arc length and constrained to a suitable space of curves, and where \\(\\lambda\\) is a regularization parameter. This is sort of an orthogonal regression with total curvature penalty. The real issue here is figuring out if this can be solved reasonably efficiently.\nFused ridge implicit reconstruction. Solve \\(\\arg \\min_f\\sum_{i} \\text{dist}\\left(y_i, f^{-1}(0)\\right) + \\lambda \\int \\|\\nabla f\\|^2\\). Here we may restrict \\(f\\) to be a polynomial of known degree. I don’t know if this would work well. Maybe \\(\\text{dist}(y_i, f^{-1}(0))\\) can be more easily approximated here using the gradient of \\(f\\).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-bayesian-numerical-analysis/",
    "title": "Bayesian numerical analysis",
    "description": {},
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2017-11-19",
    "categories": [],
    "contents": "\nDistributing points \\(\\{x_i\\}_{i=1}^n\\) on the sphere as to minimize the mean square error\n\\(\\mathbb{E}\\left[\\left(q_n(f) - \\int_{\\mathbb{S}^2}f(s)\\,ds\\right)^2\\right]\\)\nof the quadrature formula \\(q_n(f) =\\frac{1}{n}\\sum_{i=1}^n f(x_i)\\), where \\(f\\) is a centered Gaussian process with covariance function \\(C(x,y) = \\exp(\\langle x, y \\rangle)\\). Shown is \\(n=6, 12, 23\\).\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T21:46:33-05:00",
    "input_file": "bayesian-numerical-analysis.knit.md"
  },
  {
    "path": "posts/2020-11-15-analysis-problem/",
    "title": "An analysis problem",
    "description": {},
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2017-06-14",
    "categories": [],
    "contents": "\nProblem from Félix Locas:\nLet \\(r(n) = \\lfloor \\log_2 \\frac{n}{\\log_2 n} \\rfloor\\). Show that $\\(\\lim_{n \\rightarrow \\infty} \\left( \\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} \\right)^n = 1.\\)\nMy solution:\nThe series \\(\\sum_{k=1}^{\\infty} \\frac{1}{k(k+1) 2^k}\\) is easy to calculate. It is, for instance, the difference between the integrals of geometric series: \\[\\sum_{k=1}^\\infty \\frac{1}{k(k+1) 2^k} = \\sum_{k=1}^\\infty \\frac{1}{k 2^k} - \\sum_{k=1}^\\infty \\frac{1}{(k+1) 2^k} = 1-\\log 2.\\]\nFurthermore, abbreviating \\(r = r(n)\\), \\[r^{3/2} 2^{r} \\sum_{k=r+1}^\\infty \\frac{1}{k(k+1) 2^{k}} \\le \\sum_{k=0}^\\infty \\frac{1}{\\sqrt{r} 2^k} \\xrightarrow{r \\rightarrow \\infty} 0\\]\nimplies that for \\(n\\) sufficiently large we have $_{k=r+1}^ < r^{-3/2} 2^{-r} $ and \\[\\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} = 1-\\sum_{k=r+1}^{\\infty} \\frac{1}{k(k+1) 2^k} \\geq 1 - r^{-3/2} 2^{-r}. \\qquad (*)\\]\nFinally, since \\(r = \\log_2 \\frac{n}{\\log_2 n} - \\varepsilon_n\\) for some \\(0 \\le\\varepsilon_n < 1\\), we have \\[n r^{-3/2} 2^{-r} = \\frac{2^{\\varepsilon_n}\\log_2 n}{(\\log_2 n - \\log_2 \\log_2 n - \\varepsilon_n)^{3/2}} \\rightarrow 0\\]\nwhich implies that \\[\\left( 1 - r^{-3/2} 2^{-r} \\right)^n \\rightarrow 1.\\]\nSince also \\(\\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} \\le 1\\) comparing this with \\((*)\\) yields \\[\\lim_{n \\rightarrow \\infty} \\left( \\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} \\right)^n = 1.\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T22:10:37-05:00",
    "input_file": "analysis-problem.knit.md"
  },
  {
    "path": "posts/2020-11-15-constructive-approximation-of-compact-hypersurfaces/",
    "title": "Constructive approximation of compact hypersurfaces",
    "description": {},
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2017-04-29",
    "categories": [],
    "contents": "\nUse this link to the pdf text.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-short-proof-critical-points-in-invariant-domains/",
    "title": "Short proof: critical points in invariant domains",
    "description": "\"When mixing coffee in your favorite mug, at every instant a particule is staying still.\"",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2017-04-29",
    "categories": [],
    "contents": "\nLet \\(f : \\mathbb{R}^k \\rightarrow \\mathbb{R}^k\\) be a \\({}\\mathcal{C}^1\\) vector field and denote by \\(\\phi(x): t \\mapsto \\phi_t(x)\\) its stream. That is, \\(\\phi_0(x) = x\\) and \\(\\frac{d}{dt}\\phi_t(x) = f(\\phi_t(x))\\). A domain \\(D \\subset \\mathbb{R}^k\\) is said to be invariant (under the stream of \\(f\\)) if \\(\\phi_t(x) \\in D\\) for all \\(x \\in D\\) and \\(t \\geq 0\\). The curve \\(\\{  \\phi_t(x) \\,|\\, t \\in \\mathbb{R} \\}\\) is said to be a closed orbit of \\(f\\) if there exists \\(T \\gt; 0\\) such that \\(\\phi_0(x) = \\phi_T(x)\\).\nTheorem. If \\(D \\subset \\mathbb{R}^k\\) is invariant and diffeomorphic to a closed ball of \\(\\mathbb{R}^k\\), then \\(f\\) has a zero in \\(D\\).\nCorollary. If \\(k=2\\), then any closed orbit of \\(f\\) encloses a zero of \\(f\\).\nProof of the theorem. Suppose that \\(\\|f(x)\\| \\gt; \\alpha \\gt; 0\\) for all \\(x \\in D\\) and let \\(M = \\sup_{x \\in D} \\|f(x)\\|\\). Since \\(f\\) is uniformly continuous on \\(D\\), there exists \\(\\delta \\gt; 0\\) such that \\(\\|x-y\\| \\lt; \\delta\\) implies \\(\\|f(x) - f(y)\\| \\lt; \\alpha\\). Also, by Brouwer’s fixed point theorem, there exists \\(x_0 \\in D\\) such that \\(\\phi_{\\delta / M}(x_0) = x_0\\). This yields a closed orbit \\(\\Gamma = \\{\\phi_t(x_0) \\,|\\, t \\geq 0\\}\\) such that any two points on \\(\\Gamma\\) are at distance at most \\(\\delta\\) from each other. Since \\(\\Gamma\\) is closed, there must exist \\(a,b \\in \\Gamma\\) such that \\(\\langle f(a), f(b) \\rangle \\leq 0\\). Hence we find that \\(\\|f(a) - f(b)\\| \\gt; \\|f(a)\\| \\gt; \\alpha\\), even though \\(\\|a-b\\| \\lt; \\delta\\). This is impossible. Thus \\(\\|f\\|\\) is not bounded away from zero and \\(f\\) must have a zero in the compact \\(D\\). \\(\\Box\\)\nProof of the corollary. When \\(k=2\\), the Jordan-Brouwer theorem implies that closed orbits separate the plane in two connected components, one of which is bounded. Schoenflies’ theorem, strengthening the above, ensures that the union of bounded component with the closed orbit is diffeomorphic to the closed disk. Invariance follows from the unicity of the solution to initial value problems when \\(f\\) is \\(\\mathcal{C}^1\\). \\(\\Box\\)\nThis can be generalized as follows. For the sake of mixing things up, we state the result in topological terms.\nTheorem (Particular case of the Poincaré-Hopf theorem.)\nLet \\(M\\) be a compact submanifold of \\(\\mathbb{R}^k\\) with non-zero Euler characteristic \\(\\chi(M)\\), and let \\(\\phi : [0,1] \\times M \\rightarrow M : (t,x) \\mapsto \\phi_t(x)\\) be a smooth isotopy. Then for all \\(t \\in [0,1]\\), there exists distinct points \\(x_1, x_2, \\dots x_{|\\chi(M)|}\\) such that\n\n\\(\\frac{d}{dt}\\phi_t(x_i) = 0.\\)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-loomis-whitney-type-inequality-for-quasi-balls/",
    "title": "Loomis-Whitney type inequality for quasi-balls?",
    "description": {},
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2017-02-10",
    "categories": [],
    "contents": "\nConsider the problem of estimating the volume of a tumor, given X-ray scans along orthogonal axes. It may be known that the tumor has a somewhat spherical shape. To formalise this idea, let \\(T \\subset \\mathbb{R}^3\\) be the tumor, \\(s\\) the area of its surface, \\(m\\) its volume and \\(C = s^3/m^2\\). From the isoperimetric inequality, we have \\(C \\geq 6^2 \\pi\\), with equality iff \\(T\\) is a ball. Correspondingly, we say that \\(T\\) is a quasi-ball if \\(C \\approx 6^2 \\pi\\). In reality, \\(C\\) is unknown but its distribution may be determined.\nWe are now given the areas \\(m_1\\), \\(m_2\\) and \\(m_3\\) of the projections of \\(T\\) along orthogonal axes. From the Loomis-Whitney inequality (or Cauchy-Schwarz in this case), we have the following estimate of the volume \\(m\\) of \\(T\\).\nTheorem. We have\n\\[\\max_i \\sqrt{\\frac{2^3 m_i^3}{C}} \\le m \\le \\sqrt{m_1 m_2 m_3}.\\]\nProblem. Can we find such an estimate of \\(m\\) that is close to sharp when \\(T\\) is close to a ball?\nReferences. Loomis, L. H.; Whitney, H. An inequality related to the isoperimetric inequality. Bull. Amer. Math. Soc. 55 (1949), no. 10, 961–962. http://projecteuclid.org/euclid.bams/1183514163.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-15-tubular-neighborhoods/",
    "title": "Tubular neighborhoods",
    "description": "Complete proof of the tubular neighborhood theorem for submanifolds of euclidean space. I was unable to find an elementary version in the litterature.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": {}
      }
    ],
    "date": "2016-12-02",
    "categories": [],
    "contents": "\nLet me introduce some notations. A \\(\\mathcal{C}^l\\) submanifold of dimension \\(m\\) of \\(\\mathbb{R}^k\\) is a subset \\(M\\) that is locally \\(\\mathcal{C}^l\\)-diffeomorphic to open balls of \\(\\mathbb{R}^m\\). Similarily, a \\(\\mathcal{C}^l\\) manifold with boundary is locally diffeomorphic to open balls of the half space \\(\\mathbb{H}^k = \\{(x_1, \\dots, x_m)\\in \\mathbb{R}^m | x_m \\geq 0\\}\\). If \\(f : M \\rightarrow N\\) is a differentiable map between manifolds, we denote by \\(df_x: T_x M \\rightarrow T_{f(x)} N\\) the differential of \\(f\\) at \\(x\\). Each tangent space \\(T_x M\\) has an orthogonal complement \\(N_x M\\) in \\(\\mathbb{R}^k\\); the normal bundle of \\(M\\) is \\(N(M) = \\{(x, v) \\in \\mathbb{R}^{2k} | x \\in M,\\, v \\in N_x M\\}\\). In the following, we assume \\(M\\) is compact.\nGiven \\(\\varepsilon \\gt; 0\\), we let \\(V_\\varepsilon = \\{(x, v) \\in N(M) \\,|\\, |v| \\le \\varepsilon\\}\\) and \\(P_\\varepsilon = \\{y \\in \\mathbb{R}^k | d(y, M) \\le \\varepsilon \\}\\), where \\(d\\) is the euclidean distance. The set \\(V_\\varepsilon\\) is an \\(\\varepsilon\\)-neighborhood of the cross-section \\(M \\times \\{0\\}\\) in \\(N(M)\\), and \\(P_\\varepsilon\\) is a tubular neighborhood of \\(M\\) in \\(\\mathbb{R}^k\\). We will prove the following theorem.\nTubular neighborhood theorem. Let \\(M\\) be a compact submanifold of \\(\\mathbb{R}^k\\), without boundary. For \\(\\varepsilon \\gt; 0\\) sufficiently small, \\(V_\\varepsilon\\) and \\(P_\\varepsilon\\) are manifolds, diffeomorphic under the map \\(F : V_\\varepsilon \\rightarrow P_\\varepsilon : (x, v) \\mapsto x+v\\).\nCorollary 1. If \\(\\varepsilon \\gt; 0\\) is sufficiently small, then for each \\(w \\in P_\\varepsilon\\) there exists an unique closest point to \\(w\\) on \\(M\\).\nNote, however, that this corollary may not hold when \\(M\\) is only a \\(\\mathcal{C}^1\\) manifold. We will require \\(M\\) to be at least \\(\\mathcal{C}^2\\). The proof will make clear why this is necessary, but I also present a counter-example.\nCounterexample (\\(\\mathcal{C}^1\\) manifolds). Let \\(M\\) be the graph of \\(f: [-1,1] \\rightarrow \\mathbb{R} : t \\mapsto t^{4/3}\\) in \\(\\mathbb{R}^2\\). It is indeed a compact \\(\\mathcal{C}^1\\) manifold since \\(f\\) is \\(\\mathcal{C}^1\\). However, the points \\(w_\\varepsilon = (0, \\varepsilon)\\) have, for all \\(\\varepsilon \\gt; 0\\), two closest points on \\(M\\). To see this, first note that if \\(M\\) had an unique closest point to \\(w_\\varepsilon\\), then that point would be \\((0,0)\\), by the parity of \\(f\\). Now, consider the function \\(g_\\varepsilon :[-\\varepsilon, \\varepsilon] \\rightarrow \\mathbb{R}: t \\mapsto \\varepsilon - \\sqrt{\\varepsilon^2 - t^2 }\\), its graph being the lower half of a circle centered at \\(w_\\varepsilon\\) and crossing \\((0,0)\\). We find \\[\\lim_{t \\rightarrow 0} g_\\varepsilon'(t)/f'(t) = \\lim_{t \\rightarrow 0} \\frac{3}{4}\\frac{t^{2/3}}{\\sqrt{\\varepsilon^2 - t^2}} = 0,\\] meaning that the graph of \\(g_\\varepsilon\\) is under \\(M\\) near \\((0,0)\\). This is a contradiction, as me may thus shrink the circle to find two intersection points on \\(M\\) closer to \\(w_\\varepsilon\\) than \\((0,0)\\).\nProof of the theorem.\nIn the following, \\(M\\) is a compact \\(\\mathcal{C}^2\\) submanifold of \\(\\mathbb{R}^k\\) of dimension \\(m\\).\nLemma 1. The normal bundle \\(N(M)\\) is a \\(\\mathcal{C}^1\\) submanifold of \\(\\mathbb{R}^{2k}\\) and \\(T_{(x,v)} N(M) = T_xM \\times N_x M\\).\nProof. Let \\((x_0, 0) \\in N(M)\\) and consider a neighborhood \\(\\mathcal{U}\\) of \\(x_0\\) in \\(\\mathbb{R}^k\\). It may be chosen so that \\(M\\cap \\mathcal{U} = \\phi^{-1}(0)\\), for some \\(\\phi : \\mathcal{U} \\rightarrow \\mathbb{R}^{k-m}\\) with \\(d\\phi_x\\) surjective. Restricting \\(\\mathcal{U}\\) some more, we can find a \\(\\mathcal{C}^2\\) diffeomorphism \\(\\psi : \\mathbb{R}^m \\rightarrow M\\cap \\mathcal{U}\\). Using \\(\\phi\\) and and \\(\\psi\\), we construct a \\(\\mathcal{C}^1\\) map \\(f : \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^k\\) having \\(0\\) as a regular value and such that \\(N(M\\cap \\mathcal{U}) = f^{-1}(0)\\). It will follow from the preimage theorem that \\(N(M \\cap \\mathcal{U})\\) is a \\(\\mathcal{C}^1\\) submanifold of dimension \\(k\\). Furthermore, \\(N(M \\cap \\mathcal{U})\\) is an open neighborhood of \\((x_0, v)\\) for all \\(v \\in T_{x_0}M\\) and we will have found that \\(N(M)\\) is a \\(\\mathcal{C}^1\\) manifold.\nThe map \\(f\\) is defined as \\(f: \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^k\\), \\(f(x, v) = \\left(\\phi(x), u(x, v)\\right)\\), where \\(u : \\mathbb{R}^{2k} \\rightarrow \\mathbb{R}^m : (x,v) \\mapsto (\\langle v, d\\psi_{\\psi^{-1}(x)}(e_1) \\rangle, \\dots, \\langle v, d\\psi_{\\psi^{-1}(x)}(e_m) \\rangle)\\) and \\((e_i)\\) is a basis of \\(\\mathbb{R}^m\\). Because the vectors \\(d\\psi_{\\psi^{-1}(x)}(e_i)\\) form a basis of \\(T_x M\\), the zero set \\(u^{-1}(0)\\) is precisely \\(N_x M\\) and we find that \\(f^{-1}(0) = M \\cap \\mathcal{U}\\). To differentiate \\(f\\), we use the fact that \\(\\psi\\) is \\(\\mathcal{C}^2\\). In its matrix form,\n\\[\n  df_{(x,v)} = \\left[\\begin{array}{cc}d\\phi_x& 0\\\\ *& \\partial_2 u_{(x,v)}\\end{array}\\right]\n\\]\nwhere both \\(d\\phi_x\\) and \\(\\partial_2 u_(x,v) = u(x, \\cdot)\\) are surjective whenever \\(x \\in M\\). Thus \\(df_{(x,v)}\\) is indeed surjective for all \\((x,v) \\in f^{-1}(0)\\). The assertion \\(T_{(x,v)} N(M) = T_xM \\times N_x M\\) follows from \\(T_{(x,v)} N(M) = ker(df_{(x,v)}\\).  QED.\nLemma 2. For all \\(\\varepsilon \\gt; 0\\), \\(V_\\varepsilon \\subset N(M)\\) is a submanifold with boundary.\nProof. Let \\(f : N(M) \\rightarrow \\mathbb{R} : (x,v) \\mapsto ||v||^2\\). For any \\((x, v) \\in f^{-1}(\\varepsilon^2)\\), we have \\(df_{(x,v)}: T_x M \\times N_x M \\rightarrow \\mathbb{R} : (y, u) \\mapsto 2\\langle u, v \\rangle\\) is surjective. By the preimage theorem, we find that \\(f^{-1}((-\\infty, \\varepsilon^2])\\) is a submanifold of \\(N(M)\\) with boundary \\(f^{-1}(\\varepsilon^2)\\). QED.\nLemma 3. The map \\(F: V_\\varepsilon \\rightarrow \\mathbb{R}^k\\) is a local diffeomorphism onto its image \\(N_\\varepsilon\\).\nProof. The differential of \\(F\\) is simply \\(dF_{(x,v)} : T_xM \\times N_xM \\rightarrow \\mathbb{R}^k : (a, b) \\mapsto a+b\\), an isomorphism since \\(\\mathbb{R}^k\\) is the direct sum of \\(T_xM\\) and \\(N_xM\\). By the inverse function theorem, it follows that \\(F\\) is a local diffeomorphism onto its image. Now, it is clear that \\(F(V_\\varepsilon) \\subset N_\\varepsilon\\). If \\(w \\in N_\\varepsilon\\), then by compacity of \\(M\\) we can find a closest point \\(x \\in M\\). It is straightforward to verify that \\(w-x \\in N_xM\\), and thus \\((x, w-x)\\in V_\\varepsilon\\), \\(F(x, w-x) = w\\). QED.\nLemma 4 (See Spivak, 1970). If \\(\\varepsilon \\gt; 0\\) is taken sufficiently small, then \\(F : V_\\varepsilon \\rightarrow N_\\varepsilon : (x, v) \\mapsto x+v\\) is a diffeomorphism.\nProof. It suffices to show that \\(F\\) is bijective, whenever \\(\\varepsilon \\gt; 0\\) is sufficiently small. The local diffeomorphism will then be a global diffeomorphism. Note that \\(F\\) is injective on \\(M\\times \\{0\\}\\). Let \\(A = \\{(a, b) \\in N(M)^2 | a \\not = b,\\, F(a) = F(b)\\}\\) be the set of points showcasing the non-injectivity of \\(F\\). This set is disjoint from the compact set \\((M \\times \\{0\\})^2\\). Therefore, if we can show that \\(A\\) is closed, we will find \\(d(A, (M \\times \\{0\\})^2) \\gt; 0\\) and taking \\(\\varepsilon \\lt; d(A, (M \\times \\{0\\})^2)\\) will suffice. Let \\(\\{(a_n, b_n)\\}\\) be a sequence of points in \\(A\\) converging to some \\((a,b)\\). By continuity of \\(F\\), we must have \\(F(a) = F(b)\\). We cannot have \\(a = b\\), as this would contradict the fact that \\(F\\) is a local diffeomorphism. Therefore \\(a \\not = b\\) and \\((a,b) \\in A\\). QED.\nReferences:\nMilnor, J.W. (1965) Topology from a differentiable point of view. Spivak, M. (1970) A comprehensive introduction to differential geometry.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T20:41:06-05:00",
    "input_file": {}
  }
]
