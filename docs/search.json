[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Welcome!",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\nüëã Welcome!\nI‚Äôm Olivier, a data scientist at American Institutes for Research. I have a Ph.D.¬†in Statistics from Duke University.\nI specialize in applied data science R&D, in the statistical evaluation of AI/machine learning systems, and in entity resolution for data integration, cleaning, and enrichment. My work combines engineering, machine learning, and statistics to address applied problems in these areas."
  },
  {
    "objectID": "pages/blog.html",
    "href": "pages/blog.html",
    "title": "Skywriting",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 4, 2024\n\n\nProduct Development Is Hard\n\n\ngeneral, product-development\n\n\n\n\nAug 24, 2024\n\n\nTest-Driven Development is Free\n\n\ntechnical, python\n\n\n\n\nAug 15, 2024\n\n\nPersonal Knowledge Management\n\n\ngeneral, knowledge-management\n\n\n\n\nAug 15, 2024\n\n\nMeasurement and Management\n\n\ngeneral, management\n\n\n\n\nMar 18, 2024\n\n\nComment on The Sample Size Required in Importance Sampling\n\n\ntechnical, math, statistics\n\n\n\n\nDec 23, 2021\n\n\nRecord Linkage at the Duke GPSG Community Pantry\n\n\ntechnical, python, statistics\n\n\n\n\nApr 29, 2017\n\n\nShort Proof: Critical Points in Invariant Domains\n\n\ntechnical, math\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/posts/2024-09-04-product-development-is-hard/2024-09-04-product-development-is-hard.html",
    "href": "pages/posts/2024-09-04-product-development-is-hard/2024-09-04-product-development-is-hard.html",
    "title": "Product Development Is Hard",
    "section": "",
    "text": "I am mostly a ‚Äútechnical‚Äù person. This means I tend to work on technology problems that have technology solutions. I‚Äôm interested in non-technological things as well, but it‚Äôs not my expertise.\nIn my field, learning about a new technology can feel like gaining a superpower. Think about being able to build a custom ChatGPT - it‚Äôs exciting!\nWith this comes the thought: ‚ÄúWouldn‚Äôt it be nice if I solved problem Y using technology X?‚Äù\nUnfortunately, the answer to this question is typically a resounding ‚Äúno.‚Äù\nIt‚Äôs not that problem Y is not important. Or that technology X can‚Äôt help with problem Y. The problem is that product development is hard.\nIf I went about building a solution fueled only by my technological enthusiasm, then I would likely fail. It has happened to me before.\nMost people don‚Äôt care about technology. They care about a job to be done. They want to gain a superpower of their own.\n\n\n\nhttps://jtbd.info/2-what-is-jobs-to-be-done-jtbd-796b82081cca\n\n\nBuilding a good product requires understanding what your customer/client wants to get done. To understand where, when, and why they might want to use your product.\nThis is a science of its own. It‚Äôs not a technological problem, it‚Äôs a human problem. And it‚Äôs not my expertise.\nAs technologists, we need to embrace our backline role. We need to call on non-technologists to guide the creation of great products that empower others, or learn the skills we need to get this done through training from experts or experience working with experts.\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/research.html",
    "href": "pages/research.html",
    "title": "Publications",
    "section": "",
    "text": "Binette, O., & Reiter, J. P. (2024). Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework. arXiv e-prints, arXiv:2406.10366. [link]\nBinette, O. and J. P. Reiter (2023) ER-Evaluation: End-to-End Evaluation of Entity Resolution Systems. Journal of Open Source Software, 8(91), 5619. [link]\nBinette, O., S. A. York, E. Hickerson, Y. Baek, S. Madhavan and C. Jones. (2023) Estimating the Performance of Entity Resolution Algorithms: Lessons Learned Through PatentsView.org. The American Statistician [link]\nBinette, O., S. Madhavan, J. Butler, B.A. Card, E. Melluso and C. Jones. (2023) PatentsView-Evaluation: Evaluation Datasets and Tools to Advance Research on Inventor Name Disambiguation. arXiv e-prints, arxiv:2301.03591 [link]\nBinette, O. and R.C. Steorts (2021) On the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery. Journal of the Royal Statistical Society, Series A [link]"
  },
  {
    "objectID": "pages/research.html#ml-evaluation",
    "href": "pages/research.html#ml-evaluation",
    "title": "Publications",
    "section": "",
    "text": "Binette, O., & Reiter, J. P. (2024). Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework. arXiv e-prints, arXiv:2406.10366. [link]\nBinette, O. and J. P. Reiter (2023) ER-Evaluation: End-to-End Evaluation of Entity Resolution Systems. Journal of Open Source Software, 8(91), 5619. [link]\nBinette, O., S. A. York, E. Hickerson, Y. Baek, S. Madhavan and C. Jones. (2023) Estimating the Performance of Entity Resolution Algorithms: Lessons Learned Through PatentsView.org. The American Statistician [link]\nBinette, O., S. Madhavan, J. Butler, B.A. Card, E. Melluso and C. Jones. (2023) PatentsView-Evaluation: Evaluation Datasets and Tools to Advance Research on Inventor Name Disambiguation. arXiv e-prints, arxiv:2301.03591 [link]\nBinette, O. and R.C. Steorts (2021) On the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery. Journal of the Royal Statistical Society, Series A [link]"
  },
  {
    "objectID": "pages/research.html#entity-resolution",
    "href": "pages/research.html#entity-resolution",
    "title": "Publications",
    "section": "Entity Resolution",
    "text": "Entity Resolution\n\nBai, E., O. Binette and J. P. Reiter (2023) Optimal F-score Clustering for Bipartite Record Linkage. arxiv-eprints. arxiv:2311.13923 [link]\nBinette, O. and R. C. Steorts (2021) (Almost) All of Entity Resolution. Science Advances 8 (12) [link]"
  },
  {
    "objectID": "pages/research.html#bayesian-nonparametrics",
    "href": "pages/research.html#bayesian-nonparametrics",
    "title": "Publications",
    "section": "Bayesian Nonparametrics",
    "text": "Bayesian Nonparametrics\n\nBinette, O., D. Pati, and D. B. Dunson (2020) Bayesian Closed Surface Fitting Through Tensor Products. Journal of Machine Learning Research 21 (119) pp.¬†1-26 [link]\nBinette, O. (2019). A Note on Reverse Pinsker Inequalities. IEEE Transactions on Information Theory 65 (7). pp.4094-4096. [link]\nBinette, O. and S. Guillotte (2019). Bayesian Nonparametrics for Directional Statistics. arXiv e-prints. arxiv:1807.00305. [link]"
  },
  {
    "objectID": "pages/software.html",
    "href": "pages/software.html",
    "title": "Software",
    "section": "",
    "text": "Lightweight csv read/write, keeping track of csv dialect and other metadata.\npip install csvmeta\n\n\n\nSurvey components for Streamlit apps.\npip install streamlit-survey\n\n\n\nAn End-to-End Evaluation Framework for Entity Resolution Systems.\npip install er-evaluation\n\n\n\nStringCompare is a Python package (implemented in C++ through pybind11) for efficient string similarity computation and approximate string matching.\npip install git+https://github.com/OlivierBinette/stringcompare.git"
  },
  {
    "objectID": "pages/software.html#python-packages",
    "href": "pages/software.html#python-packages",
    "title": "Software",
    "section": "",
    "text": "Lightweight csv read/write, keeping track of csv dialect and other metadata.\npip install csvmeta\n\n\n\nSurvey components for Streamlit apps.\npip install streamlit-survey\n\n\n\nAn End-to-End Evaluation Framework for Entity Resolution Systems.\npip install er-evaluation\n\n\n\nStringCompare is a Python package (implemented in C++ through pybind11) for efficient string similarity computation and approximate string matching.\npip install git+https://github.com/OlivierBinette/stringcompare.git"
  },
  {
    "objectID": "pages/software.html#r-packages",
    "href": "pages/software.html#r-packages",
    "title": "Software",
    "section": "R packages",
    "text": "R packages\n\nFingermatchR\nFingerprint matching tools based on NIST‚Äôs Biometric Image Software, on FingerJet minutiae extraction tool, and on the libfmr library.\ndevtools::install_github(\"forensic-science/fingermatchR\")\n\n\nMSETools\nCode and analyses for the paper titled ‚ÄúOn the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery‚Äù (Binette and Steorts, 2021).\ndevtools::install_github(\"OlivierBinette/MSETools\")\n\n\ncache\n\nSimple interface to caching which works across interactive R sessions, R scripts and Rmarkdown documents.\ninstall.packages(\"cache\")\n\n\nassert\n\nLightweight validation tool for checking function arguments and data analysis scripts.\ninstall.packages(\"assert\")\n\n\ndgaFast\nMultiple Systems Estimation Using Decomposable Graphical Models. This is an efficient re-implementation and extension of the dga R package (it is now part of dga).\ndevtools::install_github(\"OlivierBinette/dgaFast\")\n\n\nTessTools\nTools for the use of Tesseract OCR in R and for the analysis of historical newspaper archives.\ndevtools::install_github(\"OlivierBinette/TessTools\")"
  },
  {
    "objectID": "pages/software.html#javascript-apps",
    "href": "pages/software.html#javascript-apps",
    "title": "Software",
    "section": "Javascript apps",
    "text": "Javascript apps\n\nFractals\nHigh resolution visualization for the Mandelbrot set. A Java version with more features is also available.\n\n\nEarthquakes\nVisualize earthquakes on the globe."
  },
  {
    "objectID": "pages/software.html#other-software",
    "href": "pages/software.html#other-software",
    "title": "Software",
    "section": "Other software",
    "text": "Other software\n\nlipsample: Sampling from arbitrary Lipschitz continuous densities on the interval in matlab."
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "",
    "text": "Figure from https://gpsg.duke.edu/resources-for-students/community-pantry/"
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#introduction",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#introduction",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "Introduction",
    "text": "Introduction\nDuke‚Äôs Graduate and Professional Student Government (GPSG) has been operating a community food pantry for about five years. The pantry provides nonperishable food and basic need items to graduate and professional students on campus. There is a weekly bag program, where students order customized bags of food to be picked up on Saturdays, as well as an in-person shopping program open on Thursdays and Saturdays.\n\nFigure 1: Weekly number of customers at the Pantry. The black line is a moving average of weekly visits.\n\n\nThe weekly bag program, which began in May 2018 and is still the most popular pantry offering, provides quite a bit of data regarding pantry customers and their habits. Some customers have ordered more than 80 times in the past 2 years, while others only ordered once or twice. For every bag order, we have the customer‚Äôs first name and last initial, an email address (which became mandatory around mid 2018), a phone number in a few cases, an address in some cases (for delivery), we have demographic information some cases, and we have the food order information. Available quasi-identifying information is shown in Table 1 below.\n\nTable 1: Quasi-identifying information provided on Qualtrics bag order forms. Note that phone number and address were only required while delivery was offered. Furthermore, most customers stop answering demographic questions after a few orders.\n\n\n\n\n\n\n\n\nQuestion no.\nQuestion\nAnswer form\nMandatory?\n\n\n\n\n-\nIP address\n-\nYes\n\n\n2\nFirst name and last initial\nFree form\nYes\n\n\n3\nDuke email\nFree form\nYes\n\n\n4\nPhone number\nFree form\nNo\n\n\n6\nAddress\nFree form\nNo\n\n\n8\nFood allergies\nFree form\nNo\n\n\n9\nNumber of members in household\n1-2 or 3+\nYes\n\n\n10\nWant baby bag?\nYes or no\nYes\n\n\n30\nDegree\nMultiple choices or Other\nNo\n\n\n31\nSchool\nMultiple choices or Other\nNo\n\n\n32\nYear in graduate school\nMultiple choices\nNo\n\n\n33\nNumber of adults in household\nMultiple choices\nNo\n\n\n34\nNumber of children in household\nMultiple choices\nNo\n\n\n\nGaining the most insight from this data requires linking order records from the same customer. Identifying individual customers and associating them with an order history allows us to investigate shopping recurrence patterns and identify potential issues with the pantry‚Äôs offering. For instance, we can know who stopped ordering from the pantry after the home delivery program ended. These are people who, most likely, do not have a car to get to the pantry but might benefit from new programs, such as a ride-share program or a gift card program.\nThis blog post describes the way in which records are linked at the Community Pantry. As we will see, the record linkage problem is not particularly difficult. It is not trivial either, however, and it does require care to ensure that it runs reliably and efficiently, and that it is intelligible and properly validated. This post goes in detail into these two aspects of the problem.\nRegarding efficiency and reliability of the software system, I describe the development of a Python module, called GroupByRule, for record linkage at the pantry. This Python module is maintainable, documented and tested, ensuring reliability of the system and the potential for its continued use throughout the years, even as technical volunteers change at the pantry. Regarding validation of the record linkage system, I describe simple steps that can be taken to evaluate model performance.\nBefore jumping into the technical part, let‚Äôs take a step back to discuss the issue of food insecurity on campus.\n\nFood Insecurity on Campus\nIt is often surprising to people that some Duke students might struggle having access to food. After all, Duke is one of the richest campuses in the US with its 12 billion endowment, high tuition and substantial research grants. Prior to the covid-19 pandemic, this wealth could be seen on campus and benefit many. Every weekday, there were several conferences and events with free food. Me and many other graduate students would participate in these events, earning 3-4 free lunches every week. Free food on campus is now a thing of the past, for the most part.\nHowever, free lunch or not, it‚Äôs important to realize the many financial challenges which students can face. International students on F-1 and J-1 visas have limited employment opportunities in the US. Many graduate students are married, have children or have other dependents which may not be eligible to work in the US either. Even if they are lucky enough to be paid a 9 or 12-month stipend, this stipend doesn‚Äôt go very far. For other students, going to Duke means living on a mixture of loans, financial aid, financial support from parents, and side jobs. Any imbalance in this rigid system can leave students having to compromise between their education and their health.\nA 2019 study from the World Food Policy Center reported that about 19% of graduate and professional students at Duke experienced food insecurity in the past year. This means they were unable to afford a balanced and sufficient diet, they were afraid of not having enough money for food, or they skipped meals and went hungry due to lack of money. The GPSG Community Pantry has been leading efforts to expand food insecurity monitoring on campus ‚Äì we are hoping to have more data in 2022 and in following years."
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#the-record-linkage-approach",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#the-record-linkage-approach",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "The Record Linkage Approach",
    "text": "The Record Linkage Approach\nThe bag order form contains email addresses which are highly reliable for linkage. If two records have the same email, we know for certain that they are from the same customer. However, customers do not always enter the same email address when submitting orders. Despite the request to use a Duke email address, some customers use personal emails. Furthermore, Duke email addresses have two forms. For instance, my duke email is both ob37@duke.edu and olivier.binette@duke.edu. Emails are therefore not sufficient for linkage. Phone numbers can be used as well, but these are only available for the period when home delivery was available.\nFirst name and last initial can be used to supplement emails and phone numbers. Again, agreement on first name and last initial provides strong evidence for match. On the other hand, people do not always enter their names in the same way.\nCombining the use of emails, phone numbers, and names, we may therefore link records which agree on any one of these attributes. This is a simple deterministic record linkage approach which should be reliable enough for the data analysis use of the pantry.\n\nDeterministic Record Linkage Rule\nTo be more precise, record linkage proceeds as follows:\n\nRecords are processed to clean and standardize the email, phone and name attributes. That is, leading and trailing whitespace are removed, capitalization is standardized, phone numbers are validated and standardized, and punctuation is removed from names.\nRecords which agree on any of their email, phone or name attributes are linked together.\nConnected components of the resulting graph are computed in order to obtain record clusters.\n\nThis record linkage procedure is extremely simple. It relies the fact that all three attributes are reliable indicators of a match and that, for two matching records, it is likely that at least one of these three attributes will be in agreement.\nAlso, the simplicity of the approach allows the use of available additional information (such as IP address and additional questions) for model validation. If the use of this additional information does not highlight any flaws with the simple deterministic approach, then this means that the deterministic approach is already good enough. We will come back to this when discussing model validation techniques.\n\n\nImplementation\nOur deterministic record linkage system is implemented in Python with some generality. The goal is for the system to be able to adapt to changes in data or processes.\nThe fundamental component of the system is a LinkageRule class. LinkageRule objects can be fitted to data, providing either a clustering or a linkage graph. For instance, a LinkageRule might be a rule to link all records which agree on the email attribute. Another LinkageRule might summarize a set of other rules, such as taking the union or intersection of their links.\nThe interface is as follows:\nfrom abc import ABC, abstractmethod\n\n\nclass LinkageRule(ABC):\n    \"\"\"\n    Interface for a linkage rule which can be fitted to data.\n\n    This abstract class specifies three methods. The `fit()` method fits the \n    linkage rule to a pandas DataFrame. The `graph` property can be used after \n    `fit()` to obtain a graph representing the linkage fitted to data.  The \n    `groups` property can be used after `fit()` to obtain a membership vector \n    representing the clustering fitted to data.\n    \"\"\"\n    @abstractmethod\n    def fit(self, df):\n        pass\n\n    @property\n    @abstractmethod\n    def graph(self):\n        pass\n\n    @property\n    @abstractmethod\n    def groups(self):\n        pass\nNote that group membership vectors, our representation for cluster groups, are meant to be a numpy integer array with entries indicating what group (cluster) a given record belongs to. Such a ‚Äúgroups‚Äù vector should not contain NA values; rather it should contain distinct integers for records that are not in the same cluster.\nWe will now define two other classes, Match and Any, which allow us to implement deterministic record linkage. The Match class implements an exact matching rule, while Any is the logical disjunction of a given set of rules. Our deterministic record linkage rule for the pantry will therefore be defined as follows:\nrule = Any(Match(\"name\"), Match(\"email\"), Match(\"phone\"))\nFollowing the LinkageRule interface, this rule will then be fitted to the data and used as follows:\nrule.fit(data)\ndata.groupby(rule.groups).last() # Get last visit data for all customers.\nThe benefit of this general interface is that it is extendable. By default, the Any class will return connected components when requesting group clusters. However, other clustering approaches could be used. Exact matching rules could also be relaxed to fuzzy matching rules based on string distance metrics or probabilistic record linkage. All of this can be implemented as additional LinkageRule subclasses in a way which is compatible with the above.\nLet‚Äôs now work on the Match class. For efficiency, we‚Äôll want Match to operate at the groups level. That is, if Match is called on a set of rules, then we‚Äôll first compute groups for these rules, before computing the intersection of these groups. This core functionality is implemented in the function _groups_from_rules() below. The function _groups() is a simple wrapper to interpret strings as a matching rule on the corresponding column.\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom igraph import Graph\n\ndef _groups(rule, df):\n    \"\"\"\n    Fit linkage rule to dataframe and return membership vector.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule to be fitted to the data. If `rule` is a string, then this \n        is interpreted as an exact matching rule for the corresponding column.\n    df: DataFrame\n        pandas Dataframe to which the rule is fitted.\n\n    Returns\n    -------\n    Membership vector (i.e. integer vector) u such that u[i] indicates the \n    cluster to which dataframe row i belongs. \n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Groups specified by distinct first names:\n    &gt;&gt;&gt; _groups(\"fname\", df)\n    array([2, 1, 0], dtype=int8)\n\n    Groups specified by same last names:\n    &gt;&gt;&gt; _groups(\"lname\", df)\n    array([0, 0, 3], dtype=int8)\n\n    Groups specified by a given linkage rule:\n    &gt;&gt;&gt; rule = Match(\"fname\")\n    &gt;&gt;&gt; _groups(rule, df)\n    array([2, 1, 0])\n    \"\"\"\n    if (isinstance(rule, str)):\n        arr = np.array(pd.Categorical(df[rule]).codes, dtype=np.int32) # Specifying dtype avoids overflow issues\n        I = (arr == -1)  # NA value indicators\n        arr[I] = np.arange(len(arr), len(arr)+sum(I))\n        return arr\n    elif isinstance(rule, LinkageRule):\n        return rule.fit(df).groups\n    else:\n        raise NotImplementedError()\n\n\ndef _groups_from_rules(rules, df):\n    \"\"\"\n    Fit linkage rules to data and return groups corresponding to their logical \n    conjunction.\n\n    This function computes the logical conjunction of a set of rules, operating \n    at the groups level. That is, rules are fitted to the data, membership \n    vector are obtained, and then the groups specified by these membership \n    vectors are intersected.\n\n    Parameters\n    ----------\n    rules: list[LinkageRule]\n        List of strings or Linkage rule objects to be fitted to the data. \n        Strings are interpreted as exact matching rules on the corresponding \n        columns.\n\n    df: DataFrame\n        pandas DataFrame to which the rules are fitted.\n\n    Returns\n    -------\n    Membership vector representing the cluster to which each dataframe row \n    belongs.\n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n    &gt;&gt;&gt; _groups_from_rules([\"fname\", \"lname\"], df)\n    array([2, 1, 0])\n    \"\"\"\n\n    arr = np.array([_groups(rule, df) for rule in rules]).T\n    groups = np.unique(arr, axis=0, return_inverse=True)[1]\n    return groups\nWe can now implement Match as follows. Note that the Graph representation of the clustering is only computed if and when needed.\nclass Match(LinkageRule):\n    \"\"\"\n    Class representing an exact matching rule over a given set of columns.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n    \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Link records which agree on both the \"fname\" and \"lname\" fields.\n    &gt;&gt;&gt; rule = Match(\"fname\", \"lname\")\n\n    Fit linkage rule to the data.\n    &gt;&gt;&gt; _ = rule.fit(df)\n\n    Construct deduplicated dataframe, retaining only the first record in each cluster.\n    &gt;&gt;&gt; _ = df.groupby(rule.groups).first()\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Match` object represents the logical conjunction of the set of \n            rules given in the `args` parameter. \n        \"\"\"\n        self.rules = args\n        self._update_graph = False\n        self.n = None\n\n    def fit(self, df):\n        self._groups = _groups_from_rules(self.rules, df)\n        self._update_graph = True\n        self.n = df.shape[0]\n\n        return self\n\n    @property\n    def groups(self):\n        return self._groups\nOne more method is needed to complete the implementation of a LinkageRule, namely the graph property. This property returns a Graph object corresponding to the matching rule. The graph is built as follows. First, we construct an inverted index for the clustering. That is, we construct a dictionary associating to each cluster the nodes which it contains. Then, an edge list is obtained by linking all pairs of nodes which belong to the same cluster. Note that the pure Python implementation below if not efficient for large clusters. This is not a problem for now since we will generally avoid computing this graph.\n# Part of the definition of the `Match` class:\n    @property\n    def graph(self) -&gt; Graph:\n        if self._update_graph:\n            # Inverted index\n            clust = pd.DataFrame({\"groups\": self.groups}\n                                 ).groupby(\"groups\").indices\n            self._graph = Graph(n=self.n)\n            self._graph.add_edges(itertools.chain.from_iterable(\n                itertools.combinations(c, 2) for c in clust.values()))\n            self._update_graph = False\n        return self._graph\nFinally, let‚Äôs implement the Any class. It‚Äôs purpose is to take the union (i.e.¬†logical disjunction) of a set of rules. Just like for Match, we can choose to operate at the groups or graph level. Here we‚Äôll work at the groups level for efficiency. That is, given a set of rules, Any will first compute their corresponding clusters before merging overlapping clusters.\nThere are quite a few different ways to efficiently merge clusters. Here we‚Äôll merge clusters by computing a ‚Äúpath graph‚Äù representation, taking the union of these graphs, and then computing connected components. For a given clustering, say containing records a, b, and c, the ‚Äúpath graph‚Äù links records as a path a‚Äìb‚Äìc.\nFirst, we define the functions needed to compute path graphs:\ndef pairwise(iterable):\n    \"\"\"\n    Iterate over consecutive pairs:\n        s -&gt; (s[0], s[1]), (s[1], s[2]), (s[2], s[3]), ...\n\n    Note\n    ----\n    Current implementation is from itertools' recipes list available at \n    https://docs.python.org/3/library/itertools.html\n    \"\"\"\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\ndef _path_graph(rule, df):\n    \"\"\"\n    Compute path graph corresponding to the rule's clustering: cluster elements \n    are connected as a path.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule for which to compute the corresponding path graph \n        (strings are interpreted as exact matching rules for the corresponding column).\n    df: DataFrame\n        Data to which the linkage rule is fitted.\n\n    Returns\n    -------\n    Graph object such that nodes in the same cluster (according to the fitted \n    linkage rule) are connected as graph paths.\n    \"\"\"\n    gr = _groups(rule, df)\n    \n    # Inverted index\n    clust = pd.DataFrame({\"groups\": gr}\n                         ).groupby(\"groups\").indices\n    graph = Graph(n=df.shape[0])\n    graph.add_edges(itertools.chain.from_iterable(\n        pairwise(c) for c in clust.values()))\n\n    return graph\nWe can now implement the Any class:\nclass Any(LinkageRule):\n    \"\"\"\n    Class representing the logical disjunction of linkage rules.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Any` object represents the logical disjunction of the set of \n            rules given by `args`. \n        \"\"\"\n        self.rules = args\n        self._graph = None\n        self._groups = None\n        self._update_groups = False\n\n    def fit(self, df):\n        self._update_groups = True\n        graphs_vect = [_path_graph(rule, df) for rule in self.rules]\n        self._graph = igraph.union(graphs_vect)\n        return self\n\n    @property\n    def groups(self):\n        if self._update_groups:\n            self._update_groups = False\n            self._groups = np.array(\n                self._graph.clusters().membership)\n        return self._groups\n\n    @property\n    def graph(self) -&gt; Graph:\n        return self._graph\nThe complete Python module (still under development) implementing this approach can be found on Github at OlivierBinette/GroupByRule.\n\n\nLimitations\nThere are quite a few limitations with this simple deterministic approach. We‚Äôll see in the model evaluation section that these do not affect performance to a large degree. However, for a system used with more data or over a longer timeframe, these should be carefully considered.\nFirst, the deterministic linkage does not allow the consideration of contradictory evidence. For instance, if long-form Duke email addresses are provided on two records and do not agree (e.g.¬†‚Äúolivier.binette@duke.edu‚Äù and ‚Äúolivier.bonhomme@duke.edu‚Äù are provided), then we know for sure that the records do not correspond to the same individual, even if the same name was provided (here Olivier B.). The consideration of such evidence could rely on probabilistic record linkage, where each record pair is associated a match probability.\nSecond, the use of connected components to resolve transitivity can be problematic, as a single spurious link could connect two large clusters by mistake. More sophisticated graph clustering techniques, in combination with probabilistic record linkage, would be required to mitigate the issue."
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#model-evaluation",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#model-evaluation",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nI cannot share any of the data which we have at the Pantry. However, I can describe general steps to be taken to evaluate model performance in practice.\n\nPairwise Precision and Recall\nHere we will evaluate linkage performance using pairwise precision \\(P\\) and recall \\(R\\). The precision \\(P\\) is defined as the proportion of predicted links which are true matches, whereas \\(R\\) is the proportion of true matches which are correctly predicted. That is, if \\(TP\\) is the number of true positive links, \\(P\\) the number of predicted links, and \\(T\\) the number of true matches, then we have \\[\nP = TP/P, \\quad R = TP/T.\n\\]\n\nEstimating Precision\nIt is helpful to express precision and recall in cluster form, where cluster elements are all interlinked. Let \\(C\\) be the set of true clusters and let \\(\\hat C\\) be the set of predicted clusters. For a given cluster \\(\\hat c \\in \\hat C\\), let \\(C \\cap \\hat c\\) be the restriction of the clustering \\(C\\) to \\(\\hat c\\). Then we have \\[\n  P = \\frac{\\sum_{\\hat c \\in \\hat C} \\sum_{e \\in C \\cap \\hat c} {\\lvert e\\rvert \\choose 2} }{ \\sum_{\\hat c \\in \\hat C} {\\lvert \\hat c \\rvert \\choose 2}}.\n\\]\nThe denominator can be computed exactly, while the numerator can be estimated by randomly sampling clusters \\(\\hat c \\in \\hat C\\), breaking them up into true clusters \\(e \\in C \\cap \\hat c\\), and then computing the sum of the combinations \\({\\lvert e\\rvert \\choose 2}\\). Importance sampling could be used to reduce the variance of the estimator, but it does not seem necessary for the scale of the data which we have at the pantry, where each predicted cluster can be examined quite quickly.\nIn practice, the precision estimation process can be carried out as follows:\n\nSample predicted clusters at random (in the case of the pantry, we can take all predicted clusters).\nMake a spreadsheet with all the records corresponding to the sampled clusters.\nSort the spreadsheet by predicted cluster ID.\nAdd a new empty column to the spreadsheet, called ‚ÄútrueSubClusters‚Äù.\nSeparately look at each predicted cluster. If the cluster should be broken up in multiple parts, use the ‚ÄútrueSubClusters‚Äù column to provide identifiers for true cluster membership. Note that these identifiers do not need to match across predicted clusters.\n\nThe spreadsheet can then be read-in and processed in a straightforward way to obtain an estimated precision value.\n\n\nEstimating Recall\nEstimating recall is a bit trickier than estimating precision, but we can make one assumption to simplify the process. Assume that precision is exactly 1, or very close to 1, so that all predicted clusters can roughly be taken at face value. Estimating recall then boils to the problem of identifying which predicted clusters should be merged together.\nIndeed, using the same notations as above, we can write \\[\nR = \\frac{\\sum_{ c \\in  C} \\sum_{e \\in \\hat C \\cap  c} {\\lvert e\\rvert \\choose 2} }{ \\sum_{ c \\in  C} {\\lvert  c \\rvert \\choose 2}}.\n\\] If precision is 1, then the denominator can be computed from the sizes of predicted clusters which are identified to be merged. On the other hand, the nominator simplifies to \\(\\sum_{e \\in \\hat C}{\\lvert e \\rvert \\choose 2}\\) which can be computed exactly from the sizes of predicted clusters. In the case of the Pantry, wrongly separated clusters are likely to be due to small differences in names and emails. Our procedure to identify clusters which should have been merged together is as follows:\n\nMake a spreadsheet containing canonical customer records (one representative record for each predicted individual customer).\nCreate a new empty column named ‚ÄútrueClustersA‚Äù.\nSort the spreadsheet by name.\nGo through the spreadsheet from top to bottom, looking at whether or not consecutive predicted clusters should be merged together. If so, write a corresponding cluster membership ID in the ‚ÄútrueClustersA‚Äù column.\nCreate a new empty column named ‚ÄútrueClustersB‚Äù.\nSort the spreadsheet by email\nGo through the spreadsheet from top to bottom, looking at whether or not consecutive predicted clusters should be merged together. If so, write a corresponding cluster membership ID in the ‚ÄútrueClustersB‚Äù column.\n\nThis process might not catch all wrongly separated clusters, but it is likely to find many of the errors due to different ways of writing names and different email addresses. The resulting spreadsheet can then easily be processed to obtain an estimated recall. If we were working with a larger dataset, we‚Äôd have to use further blocking to restrict our consideration to a more manageable subset of the data.\n\n\n\nResults\nI used the above procedures to estimate precision and recall of our simple deterministic approach to deduplicate the Pantry‚Äôs data. There was a total of 3281 bag order records for 689 estimated customers. The results are below.\nEstimated Precision: 92%\nPrecision is somewhat low due to about 3 relatively large clusters (around 30-50 records each) which should have been broken up in a few parts. 2% precision was lost due to a couple that shared a phone number, where each had about 20 order records. The vast majority of spurious links were tied to bag orders for which only the first name was provided (e.g.¬†‚ÄúSam‚Äù). The use of negative evidence to distinguish between individuals would help resolve these cases.\nEstimated Recall: 99.6%\nThis is certainly an overestimate, but it does show that missing links are not obviously showing up. Given the structure of the Pantry data, it is likely that recall is indeed quite high."
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#final-thoughts",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#final-thoughts",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "Final thoughts",
    "text": "Final thoughts\nThere are many ways in which the record linkage approach could be improved. As previously discussed, probabilistic record linkage would allow the consideration of negative evidence and the use of additional quasi-identifying information (such as IP addresses and other responses on the bag order forms). I‚Äôm looking forward to building on the GroupByRule Python module to provide a user-friendly and unified interface to more flexible methodology.\nHowever, it is important to ensure that any record linkage approach is intelligible and rooted in a good understanding of the underlying data. In this context, the use of a well-thought deterministic approach can provide good performance, at least as a first step or baseline for comparison. Furthermore, it is important to spend sufficient time investigating the results of the linkage to evaluate performance. I have highlighted simple steps which can be taken to estimate precision and make a good effort at identifying missing links. This is highly informative for model validation, improvement, and for the interpretation of any following results."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html",
    "title": "Test-Driven Development is Free",
    "section": "",
    "text": "Test-driven development (TDD) is the practice of writing tests before starting to write functional code.\nIt‚Äôs sounds a bit formal, but it‚Äôs very close to what we do when developing interactively in a Python notebook: starting with a working example before refactoring code in a general-purpose function, and iterating on the process of creating examples, testing, and developing. The practice started in the early days of programming, which is why some of the guides on the topic can seem complicated. But, in short:\nTDD was interactive development, before interactive development was a thing!\nNow there are advantages to formalizing TDD, without needing to move away from interactive development. I won‚Äôt list all of them here, but I will point out the ones that support my argument that TDD is free."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#why-tdd-is-free",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#why-tdd-is-free",
    "title": "Test-Driven Development is Free",
    "section": "Why TDD Is Free",
    "text": "Why TDD Is Free\nHere‚Äôs a key assumption I‚Äôm making: doing things right the first time is free. If you‚Äôre not doing it right the first time, you‚Äôll have to come back to it later anyway. And not doing it right the first time is likely to create many unnecessary costs along the way.\nSo, how do you do something right the first time? There are 2 parts to this:\n\nYou need to know what‚Äôs the ‚Äúright‚Äù thing you want to do.\nYou need to check that you actually did it right.\n\nPoint (2) is testing. You‚Äôll have to test, whether it is at the beginning, throughout, or at the end.\nPoint (1) is having clear requirements. Sure, you can write down requirements specification in detail and work off of that. But you know what else is a clear requirement? A test case.\nYou can save time by combining points (1) and (2) together in test cases. Just keep in mind that you‚Äôll have to write tests first in order to satisfy point (1).\nSo, TDD is free: it‚Äôs not doing anything that you wouldn‚Äôt have to do anyway, and it‚Äôs saving you from extra work now and in the future.\nNote that there is a learning curve to TDD. You need to find a TDD workflow that works for you. That takes a bit of time. But afterwards, you are saving time."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#this-isnt-a-new-idea",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#this-isnt-a-new-idea",
    "title": "Test-Driven Development is Free",
    "section": "This Isn‚Äôt a New Idea",
    "text": "This Isn‚Äôt a New Idea\nYou‚Äôre already doing TDD:\n\nIn agile development, we use ‚ÄúUser Stories‚Äù to describe specifications. These are high-level test case descriptions: ‚Äúgiven starting point X, I want to do Y to achieve Z.‚Äù User stories don‚Äôt tell you how to code things - that‚Äôs the functional implementation. It‚Äôs something you figure out afterwards, once you know what the input looks like, what the function is meant to do, and what the result should look like.\nAs mentioned earlier, interactive development is informal TDD. How can you formalize TDD in interactive development, without losing the benefits of interactive development? Simply bring the tests to your interactive development workflow. It can be done by staying organized, or you can use tools like the ‚Äúipytest‚Äù library for unit testing in Python notebooks."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#next-steps",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#next-steps",
    "title": "Test-Driven Development is Free",
    "section": "Next Steps",
    "text": "Next Steps\nYou‚Äôre already doing TDD, but maybe you‚Äôre not doing it in the most effective way. If you answer yes to some of the questions below, then it might be worth it to improve your TDD practices:\n\nCould you save time by catching bugs earlier?\nCould you save time by writing examples/tests, instead of long-form documentation?\nCould you save time by keeping track of the experiments, tests, and examples you use in a notebook as you develop?\nCould you save time by clicking a single button to run all tests in your notebook, instead of backtracking to execute notebook cells one by one?\nDo you often have to go back to fix bugs in your code or other people‚Äôs code?\n\nThere are lots of guides online about TDD. But remember: you need to create a workflow that works for you. TDD is not about formality, complicated testing, or full-coverage testing. TDD is about speeding up your development and building things right the first time."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#tdd-myths",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#tdd-myths",
    "title": "Test-Driven Development is Free",
    "section": "TDD Myths",
    "text": "TDD Myths\nBe careful not to fall into the following traps:\n\n‚ÄúAll tests need to be written upfront.‚Äù No.¬†Your TDD tests only need to cover what you want to code up in the next 5-30 minutes. They‚Äôre meant to help you develop, not give you analysis paralysis.\n‚ÄúTests can‚Äôt change.‚Äù No.¬†TDD tests are there to help you develop. Change them as much as you like.\n‚ÄúI can‚Äôt add more test after I‚Äôm done implementing.‚Äù No.¬†TDD is an iterative process. Create a test, make sure it runs (and generally fails), develop, create more tests, check what fails, develop, and keep going until you are satisfied.\n‚ÄúI don‚Äôt need QA if I do TDD.‚Äù No.¬†TDD is all about development. It helps develop faster and better. It‚Äôs about you, as a developer, building what you want to build right the first time. But, as often happens, it‚Äôs not because something is built right that it is the right thing for your customer!"
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#practical-example",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#practical-example",
    "title": "Test-Driven Development is Free",
    "section": "Practical Example",
    "text": "Practical Example\nHere‚Äôs what TDD looks like in practice.\nSay I want to code a function ‚Äúfibonacci‚Äù that computes the first n numbers of the standard Fibonacci sequence.\n\nStep 1: A first simple example and test\nFirst, I‚Äôll write an example or what I want to do. This defines requirements for my function and lets me check it. The first tests should be simple and useful for development. If I don‚Äôt know in advance what the output should be, that‚Äôs OK: I can do a smoke test instead (just check that the function runs without error and show its output).\n# Input\ninput_n = 5\n\n# Output\nexpected_output = [1, 2, 3, 5, 8]\nThen I keep track of this as a test case, so it‚Äôs easy to execute.\ndef test_fibonacci():\n  assert fibonnaci(input_n) == expected_output\nNotice that this first step is very simple and directly related to my current development task: develop a function that gets the logic right. I don‚Äôt want to worry about edge cases and every detail right now, so I don‚Äôt write tests/examples for that.\n\n\nStep 2: Implement and check\nNow I code the function and test it.\ndef fibonacci(n):\n  result = [1, 2]\n  while len(result) &lt; n:\n    result.append(result[-1], result[-2])\n  \n  return result\n\ntest_fibonacci()\nIf it doesn‚Äôt pass, make changes until it does. When it passes, great! We have the right logic. Now we can think about edge cases and iterate.\n\n\nStep 3: Iterate\nFirst, create examples/test cases. Again, this specifies what we want to achieve, and makes it easy for us to check it.\ndef test_fibonacci_edge_cases():\n  assert fibonacci(0) = []\n  assert fibonacci(1) = [1]\n  # etc \nThen, make changes to your function and run the tests:\ndef fibonacci(n):\n  ...\n\ntest_fibonacci() # Make sure I didn't break anything\ntest_fibonacci_edge_cases() # New tests\nA large number of tests can quickly become unwieldy. This is where testing frameworks like pytest become handy. They keep track of test suites and let you run all tests in a single click."
  },
  {
    "objectID": "pages/posts/2024-08-15-personal-knowledge-management/2024-08-15-personal-knowledge-management.html",
    "href": "pages/posts/2024-08-15-personal-knowledge-management/2024-08-15-personal-knowledge-management.html",
    "title": "Personal Knowledge Management",
    "section": "",
    "text": "Essentially all of my work involves reading and writing. I write papers and proposals, code, documentation, emails, and I jot down thoughts in problem-solving sessions. And all of that is in relation to the writings and ideas of an incredibly large number of people.\nKeeping up with all this information requires knowledge management systems. They are often integrated into our online experiences - we have bookmarks, searchable email inboxes, online code repositories, etc.\nBut some effort is needed to use these systems effectively, without being overwhelmed by all of these disparate systems. That‚Äôs where personal knowledge management comes in.\nIt‚Äôs not a new idea. For millennia, beginning at least with Aristotle, writers have been using ‚Äúcommonplace‚Äú books to organize their notes, quotes, and ideas. Stephen Johnson, in the book Where Good Ideas Come From, relates Darwin‚Äôs notebooks to this tradition:\n\nDarwin‚Äôs notebooks lie at the tail end of a long and fruitful tradition that peaked in Enlightenment-era Europe, particularly in England: the practice of maintaining a ‚Äòcommonplace‚Äô book. Scholars, amateur scientists, aspiring men of letters - just about anyone with intellectual ambition in the seventeenth and eighteenth centuries was likely to keep a commonplace book. The great minds of the period - Milton, Bacon, Locke - were zealous believers in the memory-enhancing powers of the commonplace book.\n\nSomething as simple as the ‚Äúnotes‚Äù app on your phone, or sending yourself emails, can work well enough for note-taking. But we can get much more out of our notes by using technology to help index notes, create connections between them, and help summarize and extract relevant information when needed.\nTechnology can also help us overcome the challenges of determining how to organize notes. Personally, I cannot keep any file tree well organized. There is an alternative: instead of a hierarchical tree, we can organize notes in a graph using tags and links. This is how Wikipedia is structured. You don‚Äôt find a wiki page by going down a file tree. Rather, you do keyword searches and follow links between pages.\nMy favorite tool for this is Obsidian (at work I use Confluence). Previously I used Notion, and before that I only used paper. Obsidian is free, easy-to-use, private (it‚Äôs a desktop app!), and responsive. I use it to keep track of everything that isn‚Äôt my paper notepad, emails, or LaTeX/Word documents.\n\nThere are lots of other tools available:\n\n\n\nhypothes.is for web annotation\nRoam\nNotion\n\n\n\nLogseq\nDendron\nDatabyss\n\n\n\nIn short, it‚Äôs easy to take modern digital features like hypertext or search for granted. But it‚Äôs really amazing how far we‚Äôve come to get here, and I think we can do even more amazing things if we can use these features to their full extent or push them even further.\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html",
    "title": "Measurement and Management",
    "section": "",
    "text": "W. Edwards Deming pioneered the use of measurement and statistics in manufacturing industries, using data to improve processes. Some even credit part of the success of the post-WWII Japanese auto industry (e.g.¬†Toyota) to Deming‚Äôs japanese career, where he taught and popularized the use of Statistical Process Control (SPC) [1].\nUnfortunately, Deming‚Äôs work and ideas are widely misunderstood. And Deming was aware of this. Much of his later writings emphasize how a naive understanding of quality management is counterproductive. 1\n\n\n\n\nW. Edwards Deming"
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#dont-manage-by-numbers.",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#dont-manage-by-numbers.",
    "title": "Measurement and Management",
    "section": "Don‚Äôt manage by numbers.",
    "text": "Don‚Äôt manage by numbers.\nIt‚Äôs a bit confusing: Deming encouraged the use of measurement, metrics, data, and statistics, as a key tool for process improvement and quality control. And yet he also painstakingly tried to drive in points like this:\n\n‚ÄúIt is wrong to suppose that if you can‚Äôt measure it, you can‚Äôt manage it ‚Äì a costly myth.‚Äù\n‚ÄúEliminate management by numbers and numerical goals.‚Äù\n\nHow can this be? How can he simultaneously be pro-measurement, pro-data, and against data-driven management?"
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#how-can-we-resolve-this-false-paradox",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#how-can-we-resolve-this-false-paradox",
    "title": "Measurement and Management",
    "section": "How can we resolve this false paradox?",
    "text": "How can we resolve this false paradox?\nAs a statistician, Deming was aware how important what you can‚Äôt measure is to making valid inferences. Statistics is not about data. It‚Äôs about combining data and context to make valid inferences. Data on its own has no meaning. Missing data - including both the data you wish you had and the data you don‚Äôt even know you‚Äôre missing - is more important than the data you have. A statistician‚Äôs work is to help learn about such unknowns. It‚Äôs a fallacy to make decisions based only on available data - the McNamara fallacy.\n\n‚ÄúBut when the McNamara discipline is applied too literally, the first step is to measure whatever can be easily measured. The second step is to disregard that which can‚Äôt easily be measured or given a quantitative value. The third step is to presume that what can‚Äôt be measured easily really isn‚Äôt important. The fourth step is to say that what can‚Äôt be easily measured really doesn‚Äôt exist. This is suicide.‚Äù ‚Äî Daniel Yankelovich, ‚ÄúInterpreting the New Life Styles‚Äù, Sales Management (1971)\n\nThe problem isn‚Äôt data or measurement. In fact, you should aim to measure as much as you can, as often as you can. You should build measurement and observability as core components of your systems and infrastructures. You should work to continually improve your approach to measurement of what matters. And you should have statisticians or data scientists make sense of these numbers through their context, given specific goals.\nBut here‚Äôs the thing: measurement is not management.\nAs a manager, your job is to create and maintain structures that drive customer value and continuous improvement. To achieve this, you need to think about knowns (i.e., data, metrics) and unknowns. Statisticians or data scientists can help you contextualize data and shed light on unknowns, athough it‚Äôs not always an easy process."
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#in-short",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#in-short",
    "title": "Measurement and Management",
    "section": "In Short",
    "text": "In Short\nThere are many misconceptions surrounding data and its use in management. It is important for all to understand both the importance of data and its limitations. We can do so by learning from resources such as the Deming Institute‚Äôs website:\n\n\nDeming advocated for structures that removed fear in workers, fostered continuous improvement, and enabled taking pride in one‚Äôs work."
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#footnotes",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#footnotes",
    "title": "Measurement and Management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDeming started working in Japan in 1947, bringing knowledge of the theory of Statistical Process Control (SPC) that was pioneered by Walter A. Shewhart at Bell Laboratories a few decades earlier. During post-war reconstruction, the Union of Japanese Scientists and Engineers (JUSE) invited Deming to teach SPC to engineers and managers. He went on to work with private enterprises and received multiple awards for his contributions.‚Ü©Ô∏é"
  },
  {
    "objectID": "pages/posts/2017-03-18-comment-on-sample-size-for-importance-sampling/2017-03-18-comment-on-sample-size-for-importance-sampling.html",
    "href": "pages/posts/2017-03-18-comment-on-sample-size-for-importance-sampling/2017-03-18-comment-on-sample-size-for-importance-sampling.html",
    "title": "Comment on The Sample Size Required in Importance Sampling",
    "section": "",
    "text": "The problem is to evaluate\n\n\\(I = I(f) = \\int f d\\mu,\\)\n\nwhere $$ is a probability measure on a space \\(\\mathbb{M}\\) and where \\(f: \\mathbb{M} \\rightarrow \\mathbb{R}\\) is measurable. The Monte-Carlo estimate of \\(I\\) is\n\n\\(\\frac{1}{n}\\sum_{i=1}^n f(x_i), \\qquad x_i \\sim \\mu.\\)\n\nWhen it is too difficult to sample \\(\\mu\\), for instance, other estimates can be obtained. Suppose that \\(\\mu\\) is absolutely continuous with respect to another probability measure \\(\\lambda\\), and that the density of \\(\\mu\\) with respect to \\(\\lambda\\) is given by \\(\\rho\\). Another unbiaised estimate of \\(I\\) is then\n\n\\(I_n(f) = \\frac{1}{n}\\sum_{i=1}^n f(y_i)\\rho(y_i),\\qquad y_i \\sim \\lambda.\\)\n\nThis is the general framework of importance sampling, with the Monte-Carlo estimate recovered by taking \\(\\lambda = \\mu\\). An important question is the following.\n\nHow large should \\(n\\) be for \\(I_n(f)\\) to be close to \\(I(f)\\)?\n\nAn answer is given, under certain conditions, by Chatterjee and Diaconis (2015). Their main result can be interpreted as follows. If \\(X \\sim \\mu\\) and if \\(\\log \\rho(X)\\) is concentrated around its expected value \\(L=\\text{E}[\\log \\rho(X)]\\), then a sample size of approximately \\(n = e^{L}\\) is both necessary and sufficient for \\(I_n\\) to be close to \\(I\\). The exact sample size needed depends on \\(\\|f\\|_{L^2(\\mu)}\\) and on the tail behavior of \\(\\log\\rho(X)\\). I state below their theorem with a small modification.\nTheorem 1. (Chatterjee and Diaconis, 2015) As above, let \\(X \\sim \\mu\\). For any \\(a \\gt; 0\\) and \\(n \\in \\mathbb{N}\\),\n\n\\(\\mathbb{E} |I_n(f) - I(f)| \\le \\|f\\|_{L^2(\\mu)}\\left( \\sqrt{a/n} + 2\\sqrt{\\mathbb{P} (\\rho(X) \\gt; a)} \\right).\\)\n\nConversely, for any \\(\\delta \\in (0,1)\\) and \\(b \\gt; 0\\),\n\n\\(\\mathbb{P}(1 - I_n(1) \\le \\delta) \\le \\frac{n}{b} + \\frac{\\mathbb{P}(\\rho(X) \\le b)}{1-\\delta}.\\)\n\nRemark 1. Suppose \\(\\|f\\|_{L^2(\\mu)} \\le 1\\) and that \\(\\log\\rho(X)\\) is concentrated around \\(L = \\mathbb{E} \\log\\rho(X)\\), meaning that for some \\(t \\gt; 0\\) we have that \\(\\mathbb{P}(\\log \\rho(X) \\gt; L+t/2)\\) and \\(\\mathbb{P}(\\log\\rho(X) \\lt; L-t/2)\\) are both less than an arbitrary \\(\\varepsilon \\gt; 0\\). Then, taking \\(n \\geq e^{L+t}\\) we find\n\n$ |I_n(f) - I| e^{-t/4} + 2.$\n\nHowever, if $n e^{L-t} $, we obtain\n\n$ (1 - I_n(1) ) e^{-t/2} + 2 .$\n\nmeaning that there can be a high probability that \\(I(1)\\) and \\(I_n(1)\\) are not close.\nRemark 2. Let \\(\\lambda = \\mu\\), so that \\(\\rho = 1\\). In that case, \\(\\log\\rho(X)\\) only takes its expected value \\(0\\). The theorem yields\n\n\\(\\mathbb{E} |I_n(f) - I(f)| \\le \\frac{\\|f\\|_{L^2(\\mu)}}{\\sqrt{n}}\\)\n\nand no useful bound on \\(\\mathbb{P}(1-I_n(1) \\le \\delta)\\).\nComment. For the theorem to yield a sharp cutoff, it is necessary that \\(L = \\mathbb{E} \\log\\rho(X)\\) be relatively large and that \\(\\log\\rho(X)\\) be highly concentrated around \\(L\\). The first condition is not aimed at in the practice of importance sampling. This difficulty contrasts with the broad claim that ‚Äúa sample of size approximately \\(e^{L}\\) is necessary and sufficient for accurate estimation by importance sampling‚Äù. The result in conceptually interesting, but I‚Äôm not convinced that a sharp cutoff is common.\n\nExample\n\nI consider their example 1.4. Here \\(\\lambda\\) is the exponential distribution of mean \\(1\\), \\(\\mu\\) is the exponential distribution of mean 2, \\(\\rho(x) = e^{x/2}/2\\) and \\(f(x) = x\\). Thus \\(I(f) = 2\\). We have \\(L = \\mathbb{E}\\log\\rho(X) = 1-\\log(2) \\approxeq 0.3\\), meaning that the theorem yields no useful cutoff. Furthermore, \\({}\\mathbb{P}(\\rho(X) \\gt; a) = \\tfrac{1}{2a}\\) and \\(\\|f\\|_{L^2(\\mu)} = 2\\). Optimizing the bound given by the theorem yields\n\n\\(\\mathbb{E}|I_n(f)-2| \\le \\frac{4\\sqrt{2}}{(2n)^{1/4}}.\\)\n\nThe figure below shows \\(100\\) trajectories of \\(I_k(f)\\). The shaded area bounds the expected error.\n\nThis next figure shows \\(100\\) trajectories for the Monte-Carlo estimate of \\(2 = \\int x d\\mu\\), taking \\(\\lambda = \\mu\\) and \\(\\rho = 1\\). Here the theorem yields\n\n\\(\\mathbb{E}|I_n(f)-2| \\le \\frac{2}{\\sqrt{n}}.\\)\n\n\nReferences.\nChatterjee, S. and Diaconis, P. The Sample Size Required in Importance Sampling. https://arxiv.org/abs/1511.01437v2\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2017-04-29-critical-points-in-invariant-domains/2017-04-29-critical-points-in-invariant-domains.html",
    "href": "pages/posts/2017-04-29-critical-points-in-invariant-domains/2017-04-29-critical-points-in-invariant-domains.html",
    "title": "Short Proof: Critical Points in Invariant Domains",
    "section": "",
    "text": "Let \\(f : \\mathbb{R}^k \\rightarrow \\mathbb{R}^k\\) be a \\({}\\mathcal{C}^1\\) vector field and denote by \\(\\phi(x): t \\mapsto \\phi_t(x)\\) its stream. That is, \\(\\phi_0(x) = x\\) and \\(\\frac{d}{dt}\\phi_t(x) = f(\\phi_t(x))\\). A domain \\(D \\subset \\mathbb{R}^k\\) is said to be invariant (under the stream of \\(f\\)) if \\(\\phi_t(x) \\in D\\) for all \\(x \\in D\\) and \\(t \\geq 0\\). The curve \\(\\{¬† \\phi_t(x) \\,|\\, t \\in \\mathbb{R} \\}\\) is said to be a closed orbit of \\(f\\) if there exists \\(T \\gt; 0\\) such that \\(\\phi_0(x) = \\phi_T(x)\\).\n\n\nTheorem.If \\(D \\subset \\mathbb{R}^k\\) is invariant and diffeomorphic to a closed ball of \\(\\mathbb{R}^k\\), then \\(f\\) has a zero in \\(D\\).\n\n\nCorollary.If \\(k=2\\), then any closed orbit of \\(f\\) encloses a zero of \\(f\\).\n\n\nProof of the theorem.Suppose that \\(\\|f(x)\\| \\gt; \\alpha \\gt; 0\\) for all \\(x \\in D\\) and let \\(M = \\sup_{x \\in D} \\|f(x)\\|\\). Since \\(f\\) is uniformly continuous on \\(D\\), there exists \\(\\delta \\gt; 0\\) such that \\(\\|x-y\\| \\lt; \\delta\\) implies \\(\\|f(x) - f(y)\\| \\lt; \\alpha\\). Also, by Brouwer‚Äôs fixed point theorem, there exists \\(x_0 \\in D\\) such that \\(\\phi_{\\delta / M}(x_0) = x_0\\). This yields a closed orbit \\(\\Gamma = \\{\\phi_t(x_0) \\,|\\, t \\geq 0\\}\\) such that any two points on \\(\\Gamma\\) are at distance at most \\(\\delta\\) from each other. Since \\(\\Gamma\\) is closed, there must exist \\(a,b \\in \\Gamma\\) such that \\(\\langle f(a), f(b) \\rangle \\leq 0\\). Hence we find that \\(\\|f(a) - f(b)\\| \\gt; \\|f(a)\\| \\gt; \\alpha\\), even though \\(\\|a-b\\| \\lt; \\delta\\). This is impossible. Thus \\(\\|f\\|\\) is not bounded away from zero and \\(f\\) must have a zero in the compact \\(D\\). \\(\\Box\\)\n\n\nProof of the corollary.When \\(k=2\\), the Jordan-Brouwer theorem implies that closed orbits separate the plane in two connected components, one of which is bounded. Schoenflies‚Äô theorem, strengthening the above, ensures that the union of bounded component with the closed orbit is diffeomorphic to the closed disk. Invariance follows from the unicity of the solution to initial value problems when \\(f\\) is \\(\\mathcal{C}^1\\). \\(\\Box\\)\n\n\nThis can be generalized as follows. For the sake of mixing things up, we state the result in topological terms.\n\n\nTheorem¬†(Particular case of the Poincar√©-Hopf theorem).Let \\(M\\) be a compact submanifold of \\(\\mathbb{R}^k\\) with non-zero Euler characteristic \\(\\chi(M)\\), and let \\(\\phi : [0,1] \\times M \\rightarrow M : (t,x) \\mapsto \\phi_t(x)\\) be a smooth¬† isotopy. Then for all \\(t \\in [0,1]\\), there exists distinct points \\(x_1, x_2, \\dots x_{|\\chi(M)|}\\) such that \\(\\frac{d}{dt}\\phi_t(x_i) = 0.\\)\n\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  }
]