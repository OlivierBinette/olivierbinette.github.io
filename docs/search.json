[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Welcome!",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\nüëã Welcome!\nI‚Äôm Olivier, a data scientist at American Institutes for Research. I have a Ph.D.¬†in Statistics from Duke University.\nI specialize in applied data science, AI/ML evaluation, and data linkage. My work combines engineering, machine learning, and statistics to address applied problems in these areas.\n\n\n\n\n\nLet‚Äôs Meet"
  },
  {
    "objectID": "pages/blog.html",
    "href": "pages/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nNov 15, 2024\n\n\nBuy or Build?\n\n\ngeneral, management\n\n\n\n\nSep 4, 2024\n\n\nProduct Development Is Hard\n\n\ngeneral, product-development\n\n\n\n\nSep 4, 2024\n\n\nStrategic Project Management Made Simple\n\n\ngeneral, management\n\n\n\n\nSep 1, 2024\n\n\nThe Pareto Principle and Project Failures\n\n\ngeneral, management\n\n\n\n\nAug 29, 2024\n\n\nThe NABCs of Innovation\n\n\ngeneral, management\n\n\n\n\nAug 24, 2024\n\n\nTest-Driven Development is Free\n\n\ntechnical, python\n\n\n\n\nAug 15, 2024\n\n\nPersonal Knowledge Management\n\n\ngeneral, knowledge-management\n\n\n\n\nAug 15, 2024\n\n\nMeasurement and Management\n\n\ngeneral, management\n\n\n\n\nMar 18, 2024\n\n\nComment on The Sample Size Required in Importance Sampling\n\n\ntechnical, math, statistics\n\n\n\n\nDec 12, 2023\n\n\nWhat is the Reality-Ideality-Gap in Entity Resolution?\n\n\ntechnical, record-linkage\n\n\n\n\nAug 7, 2022\n\n\nPotential of Privacy-Preserving Record Linkage for the Statistics of Hidden Population\n\n\ntechnical, statistics, machine learning, entity resolution, privacy\n\n\n\n\nJan 29, 2022\n\n\nIntro to Hyperparameter Optimization for Machine Learning\n\n\ntechnical, statistics, machine learning\n\n\n\n\nDec 23, 2021\n\n\nRecord Linkage at the Duke GPSG Community Pantry\n\n\ntechnical, python, statistics\n\n\n\n\nNov 15, 2020\n\n\nValidating function arguments in R\n\n\ntechnical, r-programming\n\n\n\n\nOct 11, 2019\n\n\nPosterior Concentration in terms of the Separation Alpha-Entropy\n\n\ntechnical, math, statistics\n\n\n\n\nOct 11, 2019\n\n\nTheory of Gibbs posterior concentration\n\n\ntechnical, math, statistics\n\n\n\n\nSep 11, 2019\n\n\nThe Credibility of confidence intervals\n\n\nstatistics\n\n\n\n\nMay 24, 2019\n\n\nBayesian Optimalities\n\n\ntechnical, math, statistics\n\n\n\n\nMay 19, 2019\n\n\nGlobal Bounds for the Jensen Functional\n\n\ntechnical, math\n\n\n\n\nApr 15, 2019\n\n\nTwo sampling algorithms for trigonometric densities\n\n\ntechnical, statistics\n\n\n\n\nApr 10, 2019\n\n\nThe Significance of the adjusted R squared coefficient\n\n\ntechnical, statistics\n\n\n\n\nJan 6, 2019\n\n\n3D data visualization with WebGL/Three.js\n\n\ngeneral, data-viz\n\n\n\n\nNov 19, 2017\n\n\nBayesian numerical analysis\n\n\ntechnical, math, statistics\n\n\n\n\nNov 5, 2017\n\n\nSampling Lipschitz Continuous Densities\n\n\ntechnical, statistics\n\n\n\n\nJun 14, 2017\n\n\nAn analysis problem\n\n\ntechnical, math\n\n\n\n\nApr 29, 2017\n\n\nShort Proof: Critical Points in Invariant Domains\n\n\ntechnical, math\n\n\n\n\nDec 2, 2016\n\n\nTubular neighborhoods\n\n\ntechnical, math\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/posts/2024-09-04-product-development-is-hard/2024-09-04-product-development-is-hard.html",
    "href": "pages/posts/2024-09-04-product-development-is-hard/2024-09-04-product-development-is-hard.html",
    "title": "Product Development Is Hard",
    "section": "",
    "text": "I am mostly a ‚Äútechnical‚Äù person. This means I tend to work on technology problems that have technology solutions. I‚Äôm interested in non-technological things as well, but it‚Äôs not my expertise.\nIn my field, learning about a new technology can feel like gaining a superpower. Think about being able to build a custom ChatGPT - it‚Äôs exciting!\nWith this comes the thought: ‚ÄúWouldn‚Äôt it be nice if I solved problem Y using technology X?‚Äù\nUnfortunately, the answer to this question is typically a resounding ‚Äúno.‚Äù\nIt‚Äôs not that problem Y is not important. Or that technology X can‚Äôt help with problem Y. The problem is that product development is hard.\nIf I went about building a solution fueled only by my technological enthusiasm, then I would likely fail. It has happened to me before.\nMost people don‚Äôt care about technology. They care about a job to be done. They want to gain a superpower of their own.\n\n\n\nhttps://jtbd.info/2-what-is-jobs-to-be-done-jtbd-796b82081cca\n\n\nBuilding a good product requires understanding what your customer/client wants to get done. To understand where, when, and why they might want to use your product.\nThis is a science of its own. It‚Äôs not a technological problem, it‚Äôs a human problem. And it‚Äôs not my expertise.\nAs technologists, we need to embrace our backline role. We need to call on non-technologists to guide the creation of great products that empower others, or learn the skills we need to get this done through training from experts or experience working with experts.\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/research.html",
    "href": "pages/research.html",
    "title": "Publications",
    "section": "",
    "text": "Binette, O., & Reiter, J. P. (2024). Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework. arXiv e-prints, arXiv:2406.10366. [link]\nBinette, O. and J. P. Reiter (2023) ER-Evaluation: End-to-End Evaluation of Entity Resolution Systems. Journal of Open Source Software, 8(91), 5619. [link]\nBinette, O., S. A. York, E. Hickerson, Y. Baek, S. Madhavan and C. Jones. (2023) Estimating the Performance of Entity Resolution Algorithms: Lessons Learned Through PatentsView.org. The American Statistician [link]\nBinette, O., S. Madhavan, J. Butler, B.A. Card, E. Melluso and C. Jones. (2023) PatentsView-Evaluation: Evaluation Datasets and Tools to Advance Research on Inventor Name Disambiguation. arXiv e-prints, arxiv:2301.03591 [link]\nBinette, O. and R.C. Steorts (2021) On the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery. Journal of the Royal Statistical Society, Series A [link]"
  },
  {
    "objectID": "pages/research.html#ml-evaluation",
    "href": "pages/research.html#ml-evaluation",
    "title": "Publications",
    "section": "",
    "text": "Binette, O., & Reiter, J. P. (2024). Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework. arXiv e-prints, arXiv:2406.10366. [link]\nBinette, O. and J. P. Reiter (2023) ER-Evaluation: End-to-End Evaluation of Entity Resolution Systems. Journal of Open Source Software, 8(91), 5619. [link]\nBinette, O., S. A. York, E. Hickerson, Y. Baek, S. Madhavan and C. Jones. (2023) Estimating the Performance of Entity Resolution Algorithms: Lessons Learned Through PatentsView.org. The American Statistician [link]\nBinette, O., S. Madhavan, J. Butler, B.A. Card, E. Melluso and C. Jones. (2023) PatentsView-Evaluation: Evaluation Datasets and Tools to Advance Research on Inventor Name Disambiguation. arXiv e-prints, arxiv:2301.03591 [link]\nBinette, O. and R.C. Steorts (2021) On the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery. Journal of the Royal Statistical Society, Series A [link]"
  },
  {
    "objectID": "pages/research.html#entity-resolution",
    "href": "pages/research.html#entity-resolution",
    "title": "Publications",
    "section": "Entity Resolution",
    "text": "Entity Resolution\n\nBai, E., O. Binette and J. P. Reiter (2023) Optimal F-score Clustering for Bipartite Record Linkage. arxiv-eprints. arxiv:2311.13923 [link]\nBinette, O. and R. C. Steorts (2021) (Almost) All of Entity Resolution. Science Advances 8 (12) [link]"
  },
  {
    "objectID": "pages/research.html#bayesian-nonparametrics",
    "href": "pages/research.html#bayesian-nonparametrics",
    "title": "Publications",
    "section": "Bayesian Nonparametrics",
    "text": "Bayesian Nonparametrics\n\nBinette, O., D. Pati, and D. B. Dunson (2020) Bayesian Closed Surface Fitting Through Tensor Products. Journal of Machine Learning Research 21 (119) pp.¬†1-26 [link]\nBinette, O. (2019). A Note on Reverse Pinsker Inequalities. IEEE Transactions on Information Theory 65 (7). pp.4094-4096. [link]\nBinette, O. and S. Guillotte (2019). Bayesian Nonparametrics for Directional Statistics. arXiv e-prints. arxiv:1807.00305. [link]"
  },
  {
    "objectID": "pages/software.html",
    "href": "pages/software.html",
    "title": "Open-Source Software",
    "section": "",
    "text": "Project\nDescription\nRole\n\n\n\n\nPatentsView-Code-Examples\nTraining resources for working with PatentsView APIs and Open Data.\nLead\n\n\nDeepchecks‚Äô PerformanceBias\nContributed the performance_bias module to Deepchecks‚Äô open-source package (3.7k+ stars on Github).\nCore Contributor\n\n\nStreamlit-Survey\nSurvey components for Streamlit apps.\nCreator\n\n\nCSVMeta\nLightweight csv read/write, keeping track of csv dialect and other metadata.\nCreator\n\n\nER-Evaluation\nEnd-to-end evaluation of entity resolution systems.\nCreator\n\n\nStringCompare\nEfficient string similarity using C++ and PyBind11.\nCreator"
  },
  {
    "objectID": "pages/software.html#python-packages",
    "href": "pages/software.html#python-packages",
    "title": "Open-Source Software",
    "section": "",
    "text": "Project\nDescription\nRole\n\n\n\n\nPatentsView-Code-Examples\nTraining resources for working with PatentsView APIs and Open Data.\nLead\n\n\nDeepchecks‚Äô PerformanceBias\nContributed the performance_bias module to Deepchecks‚Äô open-source package (3.7k+ stars on Github).\nCore Contributor\n\n\nStreamlit-Survey\nSurvey components for Streamlit apps.\nCreator\n\n\nCSVMeta\nLightweight csv read/write, keeping track of csv dialect and other metadata.\nCreator\n\n\nER-Evaluation\nEnd-to-end evaluation of entity resolution systems.\nCreator\n\n\nStringCompare\nEfficient string similarity using C++ and PyBind11.\nCreator"
  },
  {
    "objectID": "pages/software.html#r-packages",
    "href": "pages/software.html#r-packages",
    "title": "Open-Source Software",
    "section": "R Packages",
    "text": "R Packages\n\n\n\nProject\nDescription\nRole\n\n\n\n\nMSETools\nCode and analyses for the paper titled ‚ÄúOn the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery‚Äù (Binette and Steorts, 2021).\nCreator\n\n\nFingermatchR\nFingerprint matching tools based on NIST‚Äôs Biometric Image Software, on FingerJet minutiae extraction tool, and on the libfmr library.\nCreator\n\n\ncache\nLightweight caching for data science workflows.\nCreator\n\n\nassert\nLightweight assertions for data science workflows.\nCreator\n\n\ndgaFast/dga\nMultiple Systems Estimation Using Decomposable Graphical Models.\nMaintainer\n\n\nTessTools\nTools for the use of Tesseract OCR in R and for the analysis of historical newspaper archives.\nCreator"
  },
  {
    "objectID": "pages/software.html#javascript-apps",
    "href": "pages/software.html#javascript-apps",
    "title": "Software",
    "section": "Javascript apps",
    "text": "Javascript apps\n\nFractals\nHigh resolution visualization for the Mandelbrot set. A Java version with more features is also available.\n\n\nEarthquakes\nVisualize earthquakes on the globe."
  },
  {
    "objectID": "pages/software.html#other-software",
    "href": "pages/software.html#other-software",
    "title": "Open-Source Software",
    "section": "Other Software",
    "text": "Other Software\n\n\n\nProject\nDescription\nRole\n\n\n\n\nFractals\nA Javascript Mendelbrot set visualization app, built using early browser multithreading technologies (2014).\nCreator\n\n\nWelcome to the Moon App\nCompanion app to the wonderful Welcome to the Moon board game.\nCreator\n\n\nEarthquakes\nVisualize earthquakes on the globe.\nCreator\n\n\nlipsample\nSampling from arbitrary Lipschitz continuous densities on the interval in Matlab\nCreator"
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "",
    "text": "Figure from https://gpsg.duke.edu/resources-for-students/community-pantry/"
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#introduction",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#introduction",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "Introduction",
    "text": "Introduction\nDuke‚Äôs Graduate and Professional Student Government (GPSG) has been operating a community food pantry for about five years. The pantry provides nonperishable food and basic need items to graduate and professional students on campus. There is a weekly bag program, where students order customized bags of food to be picked up on Saturdays, as well as an in-person shopping program open on Thursdays and Saturdays.\n\nFigure 1: Weekly number of customers at the Pantry. The black line is a moving average of weekly visits.\n\n\nThe weekly bag program, which began in May 2018 and is still the most popular pantry offering, provides quite a bit of data regarding pantry customers and their habits. Some customers have ordered more than 80 times in the past 2 years, while others only ordered once or twice. For every bag order, we have the customer‚Äôs first name and last initial, an email address (which became mandatory around mid 2018), a phone number in a few cases, an address in some cases (for delivery), we have demographic information some cases, and we have the food order information. Available quasi-identifying information is shown in Table 1 below.\n\nTable 1: Quasi-identifying information provided on Qualtrics bag order forms. Note that phone number and address were only required while delivery was offered. Furthermore, most customers stop answering demographic questions after a few orders.\n\n\n\n\n\n\n\n\nQuestion no.\nQuestion\nAnswer form\nMandatory?\n\n\n\n\n-\nIP address\n-\nYes\n\n\n2\nFirst name and last initial\nFree form\nYes\n\n\n3\nDuke email\nFree form\nYes\n\n\n4\nPhone number\nFree form\nNo\n\n\n6\nAddress\nFree form\nNo\n\n\n8\nFood allergies\nFree form\nNo\n\n\n9\nNumber of members in household\n1-2 or 3+\nYes\n\n\n10\nWant baby bag?\nYes or no\nYes\n\n\n30\nDegree\nMultiple choices or Other\nNo\n\n\n31\nSchool\nMultiple choices or Other\nNo\n\n\n32\nYear in graduate school\nMultiple choices\nNo\n\n\n33\nNumber of adults in household\nMultiple choices\nNo\n\n\n34\nNumber of children in household\nMultiple choices\nNo\n\n\n\nGaining the most insight from this data requires linking order records from the same customer. Identifying individual customers and associating them with an order history allows us to investigate shopping recurrence patterns and identify potential issues with the pantry‚Äôs offering. For instance, we can know who stopped ordering from the pantry after the home delivery program ended. These are people who, most likely, do not have a car to get to the pantry but might benefit from new programs, such as a ride-share program or a gift card program.\nThis blog post describes the way in which records are linked at the Community Pantry. As we will see, the record linkage problem is not particularly difficult. It is not trivial either, however, and it does require care to ensure that it runs reliably and efficiently, and that it is intelligible and properly validated. This post goes in detail into these two aspects of the problem.\nRegarding efficiency and reliability of the software system, I describe the development of a Python module, called GroupByRule, for record linkage at the pantry. This Python module is maintainable, documented and tested, ensuring reliability of the system and the potential for its continued use throughout the years, even as technical volunteers change at the pantry. Regarding validation of the record linkage system, I describe simple steps that can be taken to evaluate model performance.\nBefore jumping into the technical part, let‚Äôs take a step back to discuss the issue of food insecurity on campus.\n\nFood Insecurity on Campus\nIt is often surprising to people that some Duke students might struggle having access to food. After all, Duke is one of the richest campuses in the US with its 12 billion endowment, high tuition and substantial research grants. Prior to the covid-19 pandemic, this wealth could be seen on campus and benefit many. Every weekday, there were several conferences and events with free food. Me and many other graduate students would participate in these events, earning 3-4 free lunches every week. Free food on campus is now a thing of the past, for the most part.\nHowever, free lunch or not, it‚Äôs important to realize the many financial challenges which students can face. International students on F-1 and J-1 visas have limited employment opportunities in the US. Many graduate students are married, have children or have other dependents which may not be eligible to work in the US either. Even if they are lucky enough to be paid a 9 or 12-month stipend, this stipend doesn‚Äôt go very far. For other students, going to Duke means living on a mixture of loans, financial aid, financial support from parents, and side jobs. Any imbalance in this rigid system can leave students having to compromise between their education and their health.\nA 2019 study from the World Food Policy Center reported that about 19% of graduate and professional students at Duke experienced food insecurity in the past year. This means they were unable to afford a balanced and sufficient diet, they were afraid of not having enough money for food, or they skipped meals and went hungry due to lack of money. The GPSG Community Pantry has been leading efforts to expand food insecurity monitoring on campus ‚Äì we are hoping to have more data in 2022 and in following years."
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#the-record-linkage-approach",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#the-record-linkage-approach",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "The Record Linkage Approach",
    "text": "The Record Linkage Approach\nThe bag order form contains email addresses which are highly reliable for linkage. If two records have the same email, we know for certain that they are from the same customer. However, customers do not always enter the same email address when submitting orders. Despite the request to use a Duke email address, some customers use personal emails. Furthermore, Duke email addresses have two forms. For instance, my duke email is both ob37@duke.edu and olivier.binette@duke.edu. Emails are therefore not sufficient for linkage. Phone numbers can be used as well, but these are only available for the period when home delivery was available.\nFirst name and last initial can be used to supplement emails and phone numbers. Again, agreement on first name and last initial provides strong evidence for match. On the other hand, people do not always enter their names in the same way.\nCombining the use of emails, phone numbers, and names, we may therefore link records which agree on any one of these attributes. This is a simple deterministic record linkage approach which should be reliable enough for the data analysis use of the pantry.\n\nDeterministic Record Linkage Rule\nTo be more precise, record linkage proceeds as follows:\n\nRecords are processed to clean and standardize the email, phone and name attributes. That is, leading and trailing whitespace are removed, capitalization is standardized, phone numbers are validated and standardized, and punctuation is removed from names.\nRecords which agree on any of their email, phone or name attributes are linked together.\nConnected components of the resulting graph are computed in order to obtain record clusters.\n\nThis record linkage procedure is extremely simple. It relies the fact that all three attributes are reliable indicators of a match and that, for two matching records, it is likely that at least one of these three attributes will be in agreement.\nAlso, the simplicity of the approach allows the use of available additional information (such as IP address and additional questions) for model validation. If the use of this additional information does not highlight any flaws with the simple deterministic approach, then this means that the deterministic approach is already good enough. We will come back to this when discussing model validation techniques.\n\n\nImplementation\nOur deterministic record linkage system is implemented in Python with some generality. The goal is for the system to be able to adapt to changes in data or processes.\nThe fundamental component of the system is a LinkageRule class. LinkageRule objects can be fitted to data, providing either a clustering or a linkage graph. For instance, a LinkageRule might be a rule to link all records which agree on the email attribute. Another LinkageRule might summarize a set of other rules, such as taking the union or intersection of their links.\nThe interface is as follows:\nfrom abc import ABC, abstractmethod\n\n\nclass LinkageRule(ABC):\n    \"\"\"\n    Interface for a linkage rule which can be fitted to data.\n\n    This abstract class specifies three methods. The `fit()` method fits the \n    linkage rule to a pandas DataFrame. The `graph` property can be used after \n    `fit()` to obtain a graph representing the linkage fitted to data.  The \n    `groups` property can be used after `fit()` to obtain a membership vector \n    representing the clustering fitted to data.\n    \"\"\"\n    @abstractmethod\n    def fit(self, df):\n        pass\n\n    @property\n    @abstractmethod\n    def graph(self):\n        pass\n\n    @property\n    @abstractmethod\n    def groups(self):\n        pass\nNote that group membership vectors, our representation for cluster groups, are meant to be a numpy integer array with entries indicating what group (cluster) a given record belongs to. Such a ‚Äúgroups‚Äù vector should not contain NA values; rather it should contain distinct integers for records that are not in the same cluster.\nWe will now define two other classes, Match and Any, which allow us to implement deterministic record linkage. The Match class implements an exact matching rule, while Any is the logical disjunction of a given set of rules. Our deterministic record linkage rule for the pantry will therefore be defined as follows:\nrule = Any(Match(\"name\"), Match(\"email\"), Match(\"phone\"))\nFollowing the LinkageRule interface, this rule will then be fitted to the data and used as follows:\nrule.fit(data)\ndata.groupby(rule.groups).last() # Get last visit data for all customers.\nThe benefit of this general interface is that it is extendable. By default, the Any class will return connected components when requesting group clusters. However, other clustering approaches could be used. Exact matching rules could also be relaxed to fuzzy matching rules based on string distance metrics or probabilistic record linkage. All of this can be implemented as additional LinkageRule subclasses in a way which is compatible with the above.\nLet‚Äôs now work on the Match class. For efficiency, we‚Äôll want Match to operate at the groups level. That is, if Match is called on a set of rules, then we‚Äôll first compute groups for these rules, before computing the intersection of these groups. This core functionality is implemented in the function _groups_from_rules() below. The function _groups() is a simple wrapper to interpret strings as a matching rule on the corresponding column.\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom igraph import Graph\n\ndef _groups(rule, df):\n    \"\"\"\n    Fit linkage rule to dataframe and return membership vector.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule to be fitted to the data. If `rule` is a string, then this \n        is interpreted as an exact matching rule for the corresponding column.\n    df: DataFrame\n        pandas Dataframe to which the rule is fitted.\n\n    Returns\n    -------\n    Membership vector (i.e. integer vector) u such that u[i] indicates the \n    cluster to which dataframe row i belongs. \n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Groups specified by distinct first names:\n    &gt;&gt;&gt; _groups(\"fname\", df)\n    array([2, 1, 0], dtype=int8)\n\n    Groups specified by same last names:\n    &gt;&gt;&gt; _groups(\"lname\", df)\n    array([0, 0, 3], dtype=int8)\n\n    Groups specified by a given linkage rule:\n    &gt;&gt;&gt; rule = Match(\"fname\")\n    &gt;&gt;&gt; _groups(rule, df)\n    array([2, 1, 0])\n    \"\"\"\n    if (isinstance(rule, str)):\n        arr = np.array(pd.Categorical(df[rule]).codes, dtype=np.int32) # Specifying dtype avoids overflow issues\n        I = (arr == -1)  # NA value indicators\n        arr[I] = np.arange(len(arr), len(arr)+sum(I))\n        return arr\n    elif isinstance(rule, LinkageRule):\n        return rule.fit(df).groups\n    else:\n        raise NotImplementedError()\n\n\ndef _groups_from_rules(rules, df):\n    \"\"\"\n    Fit linkage rules to data and return groups corresponding to their logical \n    conjunction.\n\n    This function computes the logical conjunction of a set of rules, operating \n    at the groups level. That is, rules are fitted to the data, membership \n    vector are obtained, and then the groups specified by these membership \n    vectors are intersected.\n\n    Parameters\n    ----------\n    rules: list[LinkageRule]\n        List of strings or Linkage rule objects to be fitted to the data. \n        Strings are interpreted as exact matching rules on the corresponding \n        columns.\n\n    df: DataFrame\n        pandas DataFrame to which the rules are fitted.\n\n    Returns\n    -------\n    Membership vector representing the cluster to which each dataframe row \n    belongs.\n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n    &gt;&gt;&gt; _groups_from_rules([\"fname\", \"lname\"], df)\n    array([2, 1, 0])\n    \"\"\"\n\n    arr = np.array([_groups(rule, df) for rule in rules]).T\n    groups = np.unique(arr, axis=0, return_inverse=True)[1]\n    return groups\nWe can now implement Match as follows. Note that the Graph representation of the clustering is only computed if and when needed.\nclass Match(LinkageRule):\n    \"\"\"\n    Class representing an exact matching rule over a given set of columns.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n    \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Link records which agree on both the \"fname\" and \"lname\" fields.\n    &gt;&gt;&gt; rule = Match(\"fname\", \"lname\")\n\n    Fit linkage rule to the data.\n    &gt;&gt;&gt; _ = rule.fit(df)\n\n    Construct deduplicated dataframe, retaining only the first record in each cluster.\n    &gt;&gt;&gt; _ = df.groupby(rule.groups).first()\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Match` object represents the logical conjunction of the set of \n            rules given in the `args` parameter. \n        \"\"\"\n        self.rules = args\n        self._update_graph = False\n        self.n = None\n\n    def fit(self, df):\n        self._groups = _groups_from_rules(self.rules, df)\n        self._update_graph = True\n        self.n = df.shape[0]\n\n        return self\n\n    @property\n    def groups(self):\n        return self._groups\nOne more method is needed to complete the implementation of a LinkageRule, namely the graph property. This property returns a Graph object corresponding to the matching rule. The graph is built as follows. First, we construct an inverted index for the clustering. That is, we construct a dictionary associating to each cluster the nodes which it contains. Then, an edge list is obtained by linking all pairs of nodes which belong to the same cluster. Note that the pure Python implementation below if not efficient for large clusters. This is not a problem for now since we will generally avoid computing this graph.\n# Part of the definition of the `Match` class:\n    @property\n    def graph(self) -&gt; Graph:\n        if self._update_graph:\n            # Inverted index\n            clust = pd.DataFrame({\"groups\": self.groups}\n                                 ).groupby(\"groups\").indices\n            self._graph = Graph(n=self.n)\n            self._graph.add_edges(itertools.chain.from_iterable(\n                itertools.combinations(c, 2) for c in clust.values()))\n            self._update_graph = False\n        return self._graph\nFinally, let‚Äôs implement the Any class. It‚Äôs purpose is to take the union (i.e.¬†logical disjunction) of a set of rules. Just like for Match, we can choose to operate at the groups or graph level. Here we‚Äôll work at the groups level for efficiency. That is, given a set of rules, Any will first compute their corresponding clusters before merging overlapping clusters.\nThere are quite a few different ways to efficiently merge clusters. Here we‚Äôll merge clusters by computing a ‚Äúpath graph‚Äù representation, taking the union of these graphs, and then computing connected components. For a given clustering, say containing records a, b, and c, the ‚Äúpath graph‚Äù links records as a path a‚Äìb‚Äìc.\nFirst, we define the functions needed to compute path graphs:\ndef pairwise(iterable):\n    \"\"\"\n    Iterate over consecutive pairs:\n        s -&gt; (s[0], s[1]), (s[1], s[2]), (s[2], s[3]), ...\n\n    Note\n    ----\n    Current implementation is from itertools' recipes list available at \n    https://docs.python.org/3/library/itertools.html\n    \"\"\"\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\ndef _path_graph(rule, df):\n    \"\"\"\n    Compute path graph corresponding to the rule's clustering: cluster elements \n    are connected as a path.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule for which to compute the corresponding path graph \n        (strings are interpreted as exact matching rules for the corresponding column).\n    df: DataFrame\n        Data to which the linkage rule is fitted.\n\n    Returns\n    -------\n    Graph object such that nodes in the same cluster (according to the fitted \n    linkage rule) are connected as graph paths.\n    \"\"\"\n    gr = _groups(rule, df)\n    \n    # Inverted index\n    clust = pd.DataFrame({\"groups\": gr}\n                         ).groupby(\"groups\").indices\n    graph = Graph(n=df.shape[0])\n    graph.add_edges(itertools.chain.from_iterable(\n        pairwise(c) for c in clust.values()))\n\n    return graph\nWe can now implement the Any class:\nclass Any(LinkageRule):\n    \"\"\"\n    Class representing the logical disjunction of linkage rules.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Any` object represents the logical disjunction of the set of \n            rules given by `args`. \n        \"\"\"\n        self.rules = args\n        self._graph = None\n        self._groups = None\n        self._update_groups = False\n\n    def fit(self, df):\n        self._update_groups = True\n        graphs_vect = [_path_graph(rule, df) for rule in self.rules]\n        self._graph = igraph.union(graphs_vect)\n        return self\n\n    @property\n    def groups(self):\n        if self._update_groups:\n            self._update_groups = False\n            self._groups = np.array(\n                self._graph.clusters().membership)\n        return self._groups\n\n    @property\n    def graph(self) -&gt; Graph:\n        return self._graph\nThe complete Python module (still under development) implementing this approach can be found on Github at OlivierBinette/GroupByRule.\n\n\nLimitations\nThere are quite a few limitations with this simple deterministic approach. We‚Äôll see in the model evaluation section that these do not affect performance to a large degree. However, for a system used with more data or over a longer timeframe, these should be carefully considered.\nFirst, the deterministic linkage does not allow the consideration of contradictory evidence. For instance, if long-form Duke email addresses are provided on two records and do not agree (e.g.¬†‚Äúolivier.binette@duke.edu‚Äù and ‚Äúolivier.bonhomme@duke.edu‚Äù are provided), then we know for sure that the records do not correspond to the same individual, even if the same name was provided (here Olivier B.). The consideration of such evidence could rely on probabilistic record linkage, where each record pair is associated a match probability.\nSecond, the use of connected components to resolve transitivity can be problematic, as a single spurious link could connect two large clusters by mistake. More sophisticated graph clustering techniques, in combination with probabilistic record linkage, would be required to mitigate the issue."
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#model-evaluation",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#model-evaluation",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nI cannot share any of the data which we have at the Pantry. However, I can describe general steps to be taken to evaluate model performance in practice.\n\nPairwise Precision and Recall\nHere we will evaluate linkage performance using pairwise precision \\(P\\) and recall \\(R\\). The precision \\(P\\) is defined as the proportion of predicted links which are true matches, whereas \\(R\\) is the proportion of true matches which are correctly predicted. That is, if \\(TP\\) is the number of true positive links, \\(P\\) the number of predicted links, and \\(T\\) the number of true matches, then we have \\[\nP = TP/P, \\quad R = TP/T.\n\\]\n\nEstimating Precision\nIt is helpful to express precision and recall in cluster form, where cluster elements are all interlinked. Let \\(C\\) be the set of true clusters and let \\(\\hat C\\) be the set of predicted clusters. For a given cluster \\(\\hat c \\in \\hat C\\), let \\(C \\cap \\hat c\\) be the restriction of the clustering \\(C\\) to \\(\\hat c\\). Then we have \\[\n  P = \\frac{\\sum_{\\hat c \\in \\hat C} \\sum_{e \\in C \\cap \\hat c} {\\lvert e\\rvert \\choose 2} }{ \\sum_{\\hat c \\in \\hat C} {\\lvert \\hat c \\rvert \\choose 2}}.\n\\]\nThe denominator can be computed exactly, while the numerator can be estimated by randomly sampling clusters \\(\\hat c \\in \\hat C\\), breaking them up into true clusters \\(e \\in C \\cap \\hat c\\), and then computing the sum of the combinations \\({\\lvert e\\rvert \\choose 2}\\). Importance sampling could be used to reduce the variance of the estimator, but it does not seem necessary for the scale of the data which we have at the pantry, where each predicted cluster can be examined quite quickly.\nIn practice, the precision estimation process can be carried out as follows:\n\nSample predicted clusters at random (in the case of the pantry, we can take all predicted clusters).\nMake a spreadsheet with all the records corresponding to the sampled clusters.\nSort the spreadsheet by predicted cluster ID.\nAdd a new empty column to the spreadsheet, called ‚ÄútrueSubClusters‚Äù.\nSeparately look at each predicted cluster. If the cluster should be broken up in multiple parts, use the ‚ÄútrueSubClusters‚Äù column to provide identifiers for true cluster membership. Note that these identifiers do not need to match across predicted clusters.\n\nThe spreadsheet can then be read-in and processed in a straightforward way to obtain an estimated precision value.\n\n\nEstimating Recall\nEstimating recall is a bit trickier than estimating precision, but we can make one assumption to simplify the process. Assume that precision is exactly 1, or very close to 1, so that all predicted clusters can roughly be taken at face value. Estimating recall then boils to the problem of identifying which predicted clusters should be merged together.\nIndeed, using the same notations as above, we can write \\[\nR = \\frac{\\sum_{ c \\in  C} \\sum_{e \\in \\hat C \\cap  c} {\\lvert e\\rvert \\choose 2} }{ \\sum_{ c \\in  C} {\\lvert  c \\rvert \\choose 2}}.\n\\] If precision is 1, then the denominator can be computed from the sizes of predicted clusters which are identified to be merged. On the other hand, the nominator simplifies to \\(\\sum_{e \\in \\hat C}{\\lvert e \\rvert \\choose 2}\\) which can be computed exactly from the sizes of predicted clusters. In the case of the Pantry, wrongly separated clusters are likely to be due to small differences in names and emails. Our procedure to identify clusters which should have been merged together is as follows:\n\nMake a spreadsheet containing canonical customer records (one representative record for each predicted individual customer).\nCreate a new empty column named ‚ÄútrueClustersA‚Äù.\nSort the spreadsheet by name.\nGo through the spreadsheet from top to bottom, looking at whether or not consecutive predicted clusters should be merged together. If so, write a corresponding cluster membership ID in the ‚ÄútrueClustersA‚Äù column.\nCreate a new empty column named ‚ÄútrueClustersB‚Äù.\nSort the spreadsheet by email\nGo through the spreadsheet from top to bottom, looking at whether or not consecutive predicted clusters should be merged together. If so, write a corresponding cluster membership ID in the ‚ÄútrueClustersB‚Äù column.\n\nThis process might not catch all wrongly separated clusters, but it is likely to find many of the errors due to different ways of writing names and different email addresses. The resulting spreadsheet can then easily be processed to obtain an estimated recall. If we were working with a larger dataset, we‚Äôd have to use further blocking to restrict our consideration to a more manageable subset of the data.\n\n\n\nResults\nI used the above procedures to estimate precision and recall of our simple deterministic approach to deduplicate the Pantry‚Äôs data. There was a total of 3281 bag order records for 689 estimated customers. The results are below.\nEstimated Precision: 92%\nPrecision is somewhat low due to about 3 relatively large clusters (around 30-50 records each) which should have been broken up in a few parts. 2% precision was lost due to a couple that shared a phone number, where each had about 20 order records. The vast majority of spurious links were tied to bag orders for which only the first name was provided (e.g.¬†‚ÄúSam‚Äù). The use of negative evidence to distinguish between individuals would help resolve these cases.\nEstimated Recall: 99.6%\nThis is certainly an overestimate, but it does show that missing links are not obviously showing up. Given the structure of the Pantry data, it is likely that recall is indeed quite high."
  },
  {
    "objectID": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#final-thoughts",
    "href": "pages/posts/2022-01-01-record-linkage-at-the-gpsg-community-pantry/record-linkage-at-the-gpsg-community-pantry.html#final-thoughts",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "section": "Final thoughts",
    "text": "Final thoughts\nThere are many ways in which the record linkage approach could be improved. As previously discussed, probabilistic record linkage would allow the consideration of negative evidence and the use of additional quasi-identifying information (such as IP addresses and other responses on the bag order forms). I‚Äôm looking forward to building on the GroupByRule Python module to provide a user-friendly and unified interface to more flexible methodology.\nHowever, it is important to ensure that any record linkage approach is intelligible and rooted in a good understanding of the underlying data. In this context, the use of a well-thought deterministic approach can provide good performance, at least as a first step or baseline for comparison. Furthermore, it is important to spend sufficient time investigating the results of the linkage to evaluate performance. I have highlighted simple steps which can be taken to estimate precision and make a good effort at identifying missing links. This is highly informative for model validation, improvement, and for the interpretation of any following results."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html",
    "title": "Test-Driven Development is Free",
    "section": "",
    "text": "Test-driven development (TDD) is the practice of writing tests before starting to write functional code.\nIt‚Äôs sounds a bit formal, but it‚Äôs very close to what we do when developing interactively in a Python notebook: starting with a working example before refactoring code in a general-purpose function, and iterating on the process of creating examples, testing, and developing. The practice started in the early days of programming, which is why some of the guides on the topic can seem complicated. But, in short:\nTDD was interactive development, before interactive development was a thing!\nNow there are advantages to formalizing TDD, without needing to move away from interactive development. I won‚Äôt list all of them here, but I will point out the ones that support my argument that TDD is free."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#why-tdd-is-free",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#why-tdd-is-free",
    "title": "Test-Driven Development is Free",
    "section": "Why TDD Is Free",
    "text": "Why TDD Is Free\nHere‚Äôs a key assumption I‚Äôm making: doing things right the first time is free. If you‚Äôre not doing it right the first time, you‚Äôll have to come back to it later anyway. And not doing it right the first time is likely to create many unnecessary costs along the way.\nSo, how do you do something right the first time? There are 2 parts to this:\n\nYou need to know what‚Äôs the ‚Äúright‚Äù thing you want to do.\nYou need to check that you actually did it right.\n\nPoint (2) is testing. You‚Äôll have to test, whether it is at the beginning, throughout, or at the end.\nPoint (1) is having clear requirements. Sure, you can write down requirements specification in detail and work off of that. But you know what else is a clear requirement? A test case.\nYou can save time by combining points (1) and (2) together in test cases. Just keep in mind that you‚Äôll have to write tests first in order to satisfy point (1).\nSo, TDD is free: it‚Äôs not doing anything that you wouldn‚Äôt have to do anyway, and it‚Äôs saving you from extra work now and in the future.\nNote that there is a learning curve to TDD. You need to find a TDD workflow that works for you. That takes a bit of time. But afterwards, you are saving time."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#this-isnt-a-new-idea",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#this-isnt-a-new-idea",
    "title": "Test-Driven Development is Free",
    "section": "This Isn‚Äôt a New Idea",
    "text": "This Isn‚Äôt a New Idea\nYou‚Äôre already doing TDD:\n\nIn agile development, we use ‚ÄúUser Stories‚Äù to describe specifications. These are high-level test case descriptions: ‚Äúgiven starting point X, I want to do Y to achieve Z.‚Äù User stories don‚Äôt tell you how to code things - that‚Äôs the functional implementation. It‚Äôs something you figure out afterwards, once you know what the input looks like, what the function is meant to do, and what the result should look like.\nAs mentioned earlier, interactive development is informal TDD. How can you formalize TDD in interactive development, without losing the benefits of interactive development? Simply bring the tests to your interactive development workflow. It can be done by staying organized, or you can use tools like the ‚Äúipytest‚Äù library for unit testing in Python notebooks."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#next-steps",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#next-steps",
    "title": "Test-Driven Development is Free",
    "section": "Next Steps",
    "text": "Next Steps\nYou‚Äôre already doing TDD, but maybe you‚Äôre not doing it in the most effective way. If you answer yes to some of the questions below, then it might be worth it to improve your TDD practices:\n\nCould you save time by catching bugs earlier?\nCould you save time by writing examples/tests, instead of long-form documentation?\nCould you save time by keeping track of the experiments, tests, and examples you use in a notebook as you develop?\nCould you save time by clicking a single button to run all tests in your notebook, instead of backtracking to execute notebook cells one by one?\nDo you often have to go back to fix bugs in your code or other people‚Äôs code?\n\nThere are lots of guides online about TDD. But remember: you need to create a workflow that works for you. TDD is not about formality, complicated testing, or full-coverage testing. TDD is about speeding up your development and building things right the first time."
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#tdd-myths",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#tdd-myths",
    "title": "Test-Driven Development is Free",
    "section": "TDD Myths",
    "text": "TDD Myths\nBe careful not to fall into the following traps:\n\n‚ÄúAll tests need to be written upfront.‚Äù No.¬†Your TDD tests only need to cover what you want to code up in the next 5-30 minutes. They‚Äôre meant to help you develop, not give you analysis paralysis.\n‚ÄúTests can‚Äôt change.‚Äù No.¬†TDD tests are there to help you develop. Change them as much as you like.\n‚ÄúI can‚Äôt add more test after I‚Äôm done implementing.‚Äù No.¬†TDD is an iterative process. Create a test, make sure it runs (and generally fails), develop, create more tests, check what fails, develop, and keep going until you are satisfied.\n‚ÄúI don‚Äôt need QA if I do TDD.‚Äù No.¬†TDD is all about development. It helps develop faster and better. It‚Äôs about you, as a developer, building what you want to build right the first time. But, as often happens, it‚Äôs not because something is built right that it is the right thing for your customer!"
  },
  {
    "objectID": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#practical-example",
    "href": "pages/posts/2024-08-24-test-driven-development-is-free/2024-08-24-test-driven-development-is-free.html#practical-example",
    "title": "Test-Driven Development is Free",
    "section": "Practical Example",
    "text": "Practical Example\nHere‚Äôs what TDD looks like in practice.\nSay I want to code a function ‚Äúfibonacci‚Äù that computes the first n numbers of the standard Fibonacci sequence.\n\nStep 1: A first simple example and test\nFirst, I‚Äôll write an example or what I want to do. This defines requirements for my function and lets me check it. The first tests should be simple and useful for development. If I don‚Äôt know in advance what the output should be, that‚Äôs OK: I can do a smoke test instead (just check that the function runs without error and show its output).\n# Input\ninput_n = 5\n\n# Output\nexpected_output = [1, 2, 3, 5, 8]\nThen I keep track of this as a test case, so it‚Äôs easy to execute.\ndef test_fibonacci():\n  assert fibonnaci(input_n) == expected_output\nNotice that this first step is very simple and directly related to my current development task: develop a function that gets the logic right. I don‚Äôt want to worry about edge cases and every detail right now, so I don‚Äôt write tests/examples for that.\n\n\nStep 2: Implement and check\nNow I code the function and test it.\ndef fibonacci(n):\n  result = [1, 2]\n  while len(result) &lt; n:\n    result.append(result[-1], result[-2])\n  \n  return result\n\ntest_fibonacci()\nIf it doesn‚Äôt pass, make changes until it does. When it passes, great! We have the right logic. Now we can think about edge cases and iterate.\n\n\nStep 3: Iterate\nFirst, create examples/test cases. Again, this specifies what we want to achieve, and makes it easy for us to check it.\ndef test_fibonacci_edge_cases():\n  assert fibonacci(0) = []\n  assert fibonacci(1) = [1]\n  # etc \nThen, make changes to your function and run the tests:\ndef fibonacci(n):\n  ...\n\ntest_fibonacci() # Make sure I didn't break anything\ntest_fibonacci_edge_cases() # New tests\nA large number of tests can quickly become unwieldy. This is where testing frameworks like pytest become handy. They keep track of test suites and let you run all tests in a single click."
  },
  {
    "objectID": "pages/posts/2024-08-15-personal-knowledge-management/2024-08-15-personal-knowledge-management.html",
    "href": "pages/posts/2024-08-15-personal-knowledge-management/2024-08-15-personal-knowledge-management.html",
    "title": "Personal Knowledge Management",
    "section": "",
    "text": "Essentially all of my work involves reading and writing. I write papers and proposals, code, documentation, emails, and I jot down thoughts in problem-solving sessions. And all of that is in relation to the writings and ideas of an incredibly large number of people.\nKeeping up with all this information requires knowledge management systems. They are often integrated into our online experiences - we have bookmarks, searchable email inboxes, online code repositories, etc.\nBut some effort is needed to use these systems effectively, without being overwhelmed by all of these disparate systems. That‚Äôs where personal knowledge management comes in.\nIt‚Äôs not a new idea. For millennia, beginning at least with Aristotle, writers have been using ‚Äúcommonplace‚Äú books to organize their notes, quotes, and ideas. Stephen Johnson, in the book Where Good Ideas Come From, relates Darwin‚Äôs notebooks to this tradition:\n\nDarwin‚Äôs notebooks lie at the tail end of a long and fruitful tradition that peaked in Enlightenment-era Europe, particularly in England: the practice of maintaining a ‚Äòcommonplace‚Äô book. Scholars, amateur scientists, aspiring men of letters - just about anyone with intellectual ambition in the seventeenth and eighteenth centuries was likely to keep a commonplace book. The great minds of the period - Milton, Bacon, Locke - were zealous believers in the memory-enhancing powers of the commonplace book.\n\nSomething as simple as the ‚Äúnotes‚Äù app on your phone, or sending yourself emails, can work well enough for note-taking. But we can get much more out of our notes by using technology to help index notes, create connections between them, and help summarize and extract relevant information when needed.\nTechnology can also help us overcome the challenges of determining how to organize notes. Personally, I cannot keep any file tree well organized. There is an alternative: instead of a hierarchical tree, we can organize notes in a graph using tags and links. This is how Wikipedia is structured. You don‚Äôt find a wiki page by going down a file tree. Rather, you do keyword searches and follow links between pages.\nMy favorite tool for this is Obsidian (at work I use Confluence). Previously I used Notion, and before that I only used paper. Obsidian is free, easy-to-use, private (it‚Äôs a desktop app!), and responsive. I use it to keep track of everything that isn‚Äôt my paper notepad, emails, or LaTeX/Word documents.\n\nThere are lots of other tools available:\n\n\n\nhypothes.is for web annotation\nRoam\nNotion\n\n\n\nLogseq\nDendron\nDatabyss\n\n\n\nIn short, it‚Äôs easy to take modern digital features like hypertext or search for granted. But it‚Äôs really amazing how far we‚Äôve come to get here, and I think we can do even more amazing things if we can use these features to their full extent or push them even further.\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html",
    "title": "Measurement and Management",
    "section": "",
    "text": "W. Edwards Deming pioneered the use of measurement and statistics in manufacturing industries, using data to improve processes. Some even credit part of the success of the post-WWII Japanese auto industry (e.g.¬†Toyota) to Deming‚Äôs japanese career, where he taught and popularized the use of Statistical Process Control (SPC) [1].\nUnfortunately, Deming‚Äôs work and ideas are widely misunderstood. And Deming was aware of this. Much of his later writings emphasize how a naive understanding of quality management is counterproductive. 1\n\n\n\n\nW. Edwards Deming"
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#dont-manage-by-numbers.",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#dont-manage-by-numbers.",
    "title": "Measurement and Management",
    "section": "Don‚Äôt manage by numbers.",
    "text": "Don‚Äôt manage by numbers.\nIt‚Äôs a bit confusing: Deming encouraged the use of measurement, metrics, data, and statistics, as a key tool for process improvement and quality control. And yet he also painstakingly tried to drive in points like this:\n\n‚ÄúIt is wrong to suppose that if you can‚Äôt measure it, you can‚Äôt manage it ‚Äì a costly myth.‚Äù\n‚ÄúEliminate management by numbers and numerical goals.‚Äù\n\nHow can this be? How can he simultaneously be pro-measurement, pro-data, and against data-driven management?"
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#how-can-we-resolve-this-false-paradox",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#how-can-we-resolve-this-false-paradox",
    "title": "Measurement and Management",
    "section": "How can we resolve this false paradox?",
    "text": "How can we resolve this false paradox?\nAs a statistician, Deming was aware how important what you can‚Äôt measure is to making valid inferences. Statistics is not about data. It‚Äôs about combining data and context to make valid inferences. Data on its own has no meaning. Missing data - including both the data you wish you had and the data you don‚Äôt even know you‚Äôre missing - is more important than the data you have. A statistician‚Äôs work is to help learn about such unknowns. It‚Äôs a fallacy to make decisions based only on available data - the McNamara fallacy.\n\n‚ÄúBut when the McNamara discipline is applied too literally, the first step is to measure whatever can be easily measured. The second step is to disregard that which can‚Äôt easily be measured or given a quantitative value. The third step is to presume that what can‚Äôt be measured easily really isn‚Äôt important. The fourth step is to say that what can‚Äôt be easily measured really doesn‚Äôt exist. This is suicide.‚Äù ‚Äî Daniel Yankelovich, ‚ÄúInterpreting the New Life Styles‚Äù, Sales Management (1971)\n\nThe problem isn‚Äôt data or measurement. In fact, you should aim to measure as much as you can, as often as you can. You should build measurement and observability as core components of your systems and infrastructures. You should work to continually improve your approach to measurement of what matters. And you should have statisticians or data scientists make sense of these numbers through their context, given specific goals.\nBut here‚Äôs the thing: measurement is not management.\nAs a manager, your job is to create and maintain structures that drive customer value and continuous improvement. To achieve this, you need to think about knowns (i.e., data, metrics) and unknowns. Statisticians or data scientists can help you contextualize data and shed light on unknowns, athough it‚Äôs not always an easy process."
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#in-short",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#in-short",
    "title": "Measurement and Management",
    "section": "In Short",
    "text": "In Short\nThere are many misconceptions surrounding data and its use in management. It is important for all to understand both the importance of data and its limitations. We can do so by learning from resources such as the Deming Institute‚Äôs website:\n\n\nDeming advocated for structures that removed fear in workers, fostered continuous improvement, and enabled taking pride in one‚Äôs work."
  },
  {
    "objectID": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#footnotes",
    "href": "pages/posts/2024-08-15-measurement-and-management/2024-08-15-measurement-and-management.html#footnotes",
    "title": "Measurement and Management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDeming started working in Japan in 1947, bringing knowledge of the theory of Statistical Process Control (SPC) that was pioneered by Walter A. Shewhart at Bell Laboratories a few decades earlier. During post-war reconstruction, the Union of Japanese Scientists and Engineers (JUSE) invited Deming to teach SPC to engineers and managers. He went on to work with private enterprises and received multiple awards for his contributions.‚Ü©Ô∏é"
  },
  {
    "objectID": "pages/posts/2017-03-18-comment-on-sample-size-for-importance-sampling/2017-03-18-comment-on-sample-size-for-importance-sampling.html",
    "href": "pages/posts/2017-03-18-comment-on-sample-size-for-importance-sampling/2017-03-18-comment-on-sample-size-for-importance-sampling.html",
    "title": "Comment on The Sample Size Required in Importance Sampling",
    "section": "",
    "text": "The problem is to evaluate\n\n\\(I = I(f) = \\int f d\\mu,\\)\n\nwhere $$ is a probability measure on a space \\(\\mathbb{M}\\) and where \\(f: \\mathbb{M} \\rightarrow \\mathbb{R}\\) is measurable. The Monte-Carlo estimate of \\(I\\) is\n\n\\(\\frac{1}{n}\\sum_{i=1}^n f(x_i), \\qquad x_i \\sim \\mu.\\)\n\nWhen it is too difficult to sample \\(\\mu\\), for instance, other estimates can be obtained. Suppose that \\(\\mu\\) is absolutely continuous with respect to another probability measure \\(\\lambda\\), and that the density of \\(\\mu\\) with respect to \\(\\lambda\\) is given by \\(\\rho\\). Another unbiaised estimate of \\(I\\) is then\n\n\\(I_n(f) = \\frac{1}{n}\\sum_{i=1}^n f(y_i)\\rho(y_i),\\qquad y_i \\sim \\lambda.\\)\n\nThis is the general framework of importance sampling, with the Monte-Carlo estimate recovered by taking \\(\\lambda = \\mu\\). An important question is the following.\n\nHow large should \\(n\\) be for \\(I_n(f)\\) to be close to \\(I(f)\\)?\n\nAn answer is given, under certain conditions, by Chatterjee and Diaconis (2015). Their main result can be interpreted as follows. If \\(X \\sim \\mu\\) and if \\(\\log \\rho(X)\\) is concentrated around its expected value \\(L=\\text{E}[\\log \\rho(X)]\\), then a sample size of approximately \\(n = e^{L}\\) is both necessary and sufficient for \\(I_n\\) to be close to \\(I\\). The exact sample size needed depends on \\(\\|f\\|_{L^2(\\mu)}\\) and on the tail behavior of \\(\\log\\rho(X)\\). I state below their theorem with a small modification.\nTheorem 1. (Chatterjee and Diaconis, 2015) As above, let \\(X \\sim \\mu\\). For any \\(a \\gt; 0\\) and \\(n \\in \\mathbb{N}\\),\n\n\\(\\mathbb{E} |I_n(f) - I(f)| \\le \\|f\\|_{L^2(\\mu)}\\left( \\sqrt{a/n} + 2\\sqrt{\\mathbb{P} (\\rho(X) \\gt; a)} \\right).\\)\n\nConversely, for any \\(\\delta \\in (0,1)\\) and \\(b \\gt; 0\\),\n\n\\(\\mathbb{P}(1 - I_n(1) \\le \\delta) \\le \\frac{n}{b} + \\frac{\\mathbb{P}(\\rho(X) \\le b)}{1-\\delta}.\\)\n\nRemark 1. Suppose \\(\\|f\\|_{L^2(\\mu)} \\le 1\\) and that \\(\\log\\rho(X)\\) is concentrated around \\(L = \\mathbb{E} \\log\\rho(X)\\), meaning that for some \\(t \\gt; 0\\) we have that \\(\\mathbb{P}(\\log \\rho(X) \\gt; L+t/2)\\) and \\(\\mathbb{P}(\\log\\rho(X) \\lt; L-t/2)\\) are both less than an arbitrary \\(\\varepsilon \\gt; 0\\). Then, taking \\(n \\geq e^{L+t}\\) we find\n\n$ |I_n(f) - I| e^{-t/4} + 2.$\n\nHowever, if $n e^{L-t} $, we obtain\n\n$ (1 - I_n(1) ) e^{-t/2} + 2 .$\n\nmeaning that there can be a high probability that \\(I(1)\\) and \\(I_n(1)\\) are not close.\nRemark 2. Let \\(\\lambda = \\mu\\), so that \\(\\rho = 1\\). In that case, \\(\\log\\rho(X)\\) only takes its expected value \\(0\\). The theorem yields\n\n\\(\\mathbb{E} |I_n(f) - I(f)| \\le \\frac{\\|f\\|_{L^2(\\mu)}}{\\sqrt{n}}\\)\n\nand no useful bound on \\(\\mathbb{P}(1-I_n(1) \\le \\delta)\\).\nComment. For the theorem to yield a sharp cutoff, it is necessary that \\(L = \\mathbb{E} \\log\\rho(X)\\) be relatively large and that \\(\\log\\rho(X)\\) be highly concentrated around \\(L\\). The first condition is not aimed at in the practice of importance sampling. This difficulty contrasts with the broad claim that ‚Äúa sample of size approximately \\(e^{L}\\) is necessary and sufficient for accurate estimation by importance sampling‚Äù. The result in conceptually interesting, but I‚Äôm not convinced that a sharp cutoff is common.\n\nExample\n\nI consider their example 1.4. Here \\(\\lambda\\) is the exponential distribution of mean \\(1\\), \\(\\mu\\) is the exponential distribution of mean 2, \\(\\rho(x) = e^{x/2}/2\\) and \\(f(x) = x\\). Thus \\(I(f) = 2\\). We have \\(L = \\mathbb{E}\\log\\rho(X) = 1-\\log(2) \\approxeq 0.3\\), meaning that the theorem yields no useful cutoff. Furthermore, \\({}\\mathbb{P}(\\rho(X) \\gt; a) = \\tfrac{1}{2a}\\) and \\(\\|f\\|_{L^2(\\mu)} = 2\\). Optimizing the bound given by the theorem yields\n\n\\(\\mathbb{E}|I_n(f)-2| \\le \\frac{4\\sqrt{2}}{(2n)^{1/4}}.\\)\n\nThe figure below shows \\(100\\) trajectories of \\(I_k(f)\\). The shaded area bounds the expected error.\n\nThis next figure shows \\(100\\) trajectories for the Monte-Carlo estimate of \\(2 = \\int x d\\mu\\), taking \\(\\lambda = \\mu\\) and \\(\\rho = 1\\). Here the theorem yields\n\n\\(\\mathbb{E}|I_n(f)-2| \\le \\frac{2}{\\sqrt{n}}.\\)\n\n\nReferences.\nChatterjee, S. and Diaconis, P. The Sample Size Required in Importance Sampling. https://arxiv.org/abs/1511.01437v2\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2017-04-29-critical-points-in-invariant-domains/2017-04-29-critical-points-in-invariant-domains.html",
    "href": "pages/posts/2017-04-29-critical-points-in-invariant-domains/2017-04-29-critical-points-in-invariant-domains.html",
    "title": "Short Proof: Critical Points in Invariant Domains",
    "section": "",
    "text": "Let \\(f : \\mathbb{R}^k \\rightarrow \\mathbb{R}^k\\) be a \\({}\\mathcal{C}^1\\) vector field and denote by \\(\\phi(x): t \\mapsto \\phi_t(x)\\) its stream. That is, \\(\\phi_0(x) = x\\) and \\(\\frac{d}{dt}\\phi_t(x) = f(\\phi_t(x))\\). A domain \\(D \\subset \\mathbb{R}^k\\) is said to be invariant (under the stream of \\(f\\)) if \\(\\phi_t(x) \\in D\\) for all \\(x \\in D\\) and \\(t \\geq 0\\). The curve \\(\\{¬† \\phi_t(x) \\,|\\, t \\in \\mathbb{R} \\}\\) is said to be a closed orbit of \\(f\\) if there exists \\(T \\gt; 0\\) such that \\(\\phi_0(x) = \\phi_T(x)\\).\n\n\nTheorem.If \\(D \\subset \\mathbb{R}^k\\) is invariant and diffeomorphic to a closed ball of \\(\\mathbb{R}^k\\), then \\(f\\) has a zero in \\(D\\).\n\n\nCorollary.If \\(k=2\\), then any closed orbit of \\(f\\) encloses a zero of \\(f\\).\n\n\nProof of the theorem.Suppose that \\(\\|f(x)\\| \\gt; \\alpha \\gt; 0\\) for all \\(x \\in D\\) and let \\(M = \\sup_{x \\in D} \\|f(x)\\|\\). Since \\(f\\) is uniformly continuous on \\(D\\), there exists \\(\\delta \\gt; 0\\) such that \\(\\|x-y\\| \\lt; \\delta\\) implies \\(\\|f(x) - f(y)\\| \\lt; \\alpha\\). Also, by Brouwer‚Äôs fixed point theorem, there exists \\(x_0 \\in D\\) such that \\(\\phi_{\\delta / M}(x_0) = x_0\\). This yields a closed orbit \\(\\Gamma = \\{\\phi_t(x_0) \\,|\\, t \\geq 0\\}\\) such that any two points on \\(\\Gamma\\) are at distance at most \\(\\delta\\) from each other. Since \\(\\Gamma\\) is closed, there must exist \\(a,b \\in \\Gamma\\) such that \\(\\langle f(a), f(b) \\rangle \\leq 0\\). Hence we find that \\(\\|f(a) - f(b)\\| \\gt; \\|f(a)\\| \\gt; \\alpha\\), even though \\(\\|a-b\\| \\lt; \\delta\\). This is impossible. Thus \\(\\|f\\|\\) is not bounded away from zero and \\(f\\) must have a zero in the compact \\(D\\). \\(\\Box\\)\n\n\nProof of the corollary.When \\(k=2\\), the Jordan-Brouwer theorem implies that closed orbits separate the plane in two connected components, one of which is bounded. Schoenflies‚Äô theorem, strengthening the above, ensures that the union of bounded component with the closed orbit is diffeomorphic to the closed disk. Invariance follows from the unicity of the solution to initial value problems when \\(f\\) is \\(\\mathcal{C}^1\\). \\(\\Box\\)\n\n\nThis can be generalized as follows. For the sake of mixing things up, we state the result in topological terms.\n\n\nTheorem¬†(Particular case of the Poincar√©-Hopf theorem).Let \\(M\\) be a compact submanifold of \\(\\mathbb{R}^k\\) with non-zero Euler characteristic \\(\\chi(M)\\), and let \\(\\phi : [0,1] \\times M \\rightarrow M : (t,x) \\mapsto \\phi_t(x)\\) be a smooth¬† isotopy. Then for all \\(t \\in [0,1]\\), there exists distinct points \\(x_1, x_2, \\dots x_{|\\chi(M)|}\\) such that \\(\\frac{d}{dt}\\phi_t(x_i) = 0.\\)\n\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2024-11-15-build-or-buy/2024-11-15-build-or-buy.html",
    "href": "pages/posts/2024-11-15-build-or-buy/2024-11-15-build-or-buy.html",
    "title": "Buy or Build?",
    "section": "",
    "text": "Choosing where to get what you need - whether that‚Äôs software, hardware, or people - is called strategic sourcing. It‚Äôs about minimizing the total cost of ownership, including the costs associated with using, not using, or maintaining the thing you need.\nThe problem is particularly tricky for software developers. They‚Äôre paid to build, after all, and they know or want to know how to build. So why should they buy a solution when they can make it themselves?\n\n\n\n\n\n\n\n\n\nBefore you build, make sure you understand the real costs to succeed over the long term, and only embark on those code-writing efforts you‚Äôre sure your business is capable of. - Robert Sher, HBR\n\nAnswering this question requires having clear requirements, understanding the extent to which suppliers can meet these requirements, and understanding the total costs associated with each alternative.\nBut there‚Äôs a rule of thumb that covers many situations. If:\n\nyou can buy what you need,\nfrom a reasonably mature competitive market,\nthat benefits from economies of scale,\n\nthen you should buy and not build.\nWhy? You‚Äôre unlikely to beat a competitive market with economies of scale, so buy if you can.\nThere are exceptions to this, such as if this is an area of core capability where you‚Äôre trying to compete. But it‚Äôs a good rule of thumb for the rest.\n\nOther tips from PipDecks‚Äô Strategy Tactics:\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://pipdecks.com/products/strategy-tactics\n\n\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/teaching.html",
    "href": "pages/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Date\nEvent\nTopic\n\n\n\n\nFeb 2025\nDuke Responsible AI Symposium\nPoster on ‚ÄúImproving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework‚Äù\n\n\nOct 2024\nPrinceton Symposium on Advances in Record Linkage\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2024\nJoint Statistical Meetings\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2023\nJoint Statistical Meetings\nOn the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery"
  },
  {
    "objectID": "pages/posts/2024-09-01-pareto-principle-and-project-failures/2024-09-01-pareto-principle-and-project-failures.html",
    "href": "pages/posts/2024-09-01-pareto-principle-and-project-failures/2024-09-01-pareto-principle-and-project-failures.html",
    "title": "The Pareto Principle and Project Failures",
    "section": "",
    "text": "The Pareto principle, or the 80/20 rule, states that 80% of consequences come from 20% of the causes.\nSurprisingly enough, this principle has general statistical underpinnings and does actually occur in a broad range of situations. The numbers 80/20 could be something else, but there is often an imbalance of this sort. It‚Äôs related to selection bias and size bias. Let me explain in the context of software development.\nSay you‚Äôre building a piece of software for some use case. There‚Äôs a lot that goes into building and deploying the software: the UI, the logic, the backend, the deployment infrastructure, the iterative changes, etc. Each part contributes more or less to the functionality a user can see.\nIn this plot, UI+logic+backend is 80% of the functionality* the user can see, but only 40% of the required effort to complete the project.\nIf functionality and effort are uncorrelated or negatively correlated, then building the most functionality first will lead to decreasing return of efforts on functionality over the project‚Äôs life. The smallest set of components that contribute to 80% functionality is a biased selection that isn‚Äôt representative of the overall effort distribution.\nThis doesn‚Äôt mean that the 80% seen functionality is more important than the other 20%. In fact, your software is going to be useless if you can‚Äôt build the infrastructure it needs for deployment. All components are equally important in this example. This mismatch between true value and apparent functionality can be dangerously misleading."
  },
  {
    "objectID": "pages/posts/2024-09-01-pareto-principle-and-project-failures/2024-09-01-pareto-principle-and-project-failures.html#why-software-projects-fail",
    "href": "pages/posts/2024-09-01-pareto-principle-and-project-failures/2024-09-01-pareto-principle-and-project-failures.html#why-software-projects-fail",
    "title": "The Pareto Principle and Project Failures",
    "section": "Why Software Projects Fail",
    "text": "Why Software Projects Fail\nThe Pareto principle plays into the common failure (or cost overrun, scope creep, technical debt) of software projects.\nOften, development teams prioritize building a minimal viable product (MVP), or delivering the most apparent functionality for a given effort level. The fast achievement of 80% functionality can lead to poor expectations of what‚Äôs needed to reach a product that has actual value, i.e.¬†something maintainable and deployable. Clients, project managers, and developers can misunderstand the scope of project if they rank tasks in functionality-first order, without considering the full value chain.\n\nA Better Approach - Managing Risks And the Full Value Chain\nAs part of good project management, you want to:\n\nMap risks and uncertainties, and address the most important ones first.\nDeliver self-contained value to the client throughout the project, if possible.\n\nE.g. for (1), if you don‚Äôt know what a client wants, that‚Äôs a big risk. Getting an MVP in front of them might help reduce uncertainties and mitigate that risk. A cost overrun is also a big risk. If you don‚Äôt know how long it will take to build the infrastructure to deploy your system, then you might want to address that first.\nFor (2), note that value is not always the same as functionality. Undeployed functionality has no value to a client. An MVP, unless it is truly viable on its own, typically has little value to a client. A product that doesn‚Äôt meet quality requirements does not have any value. If clients hire you for software development, value is something they can use without any further software development."
  },
  {
    "objectID": "pages/posts/2024-09-01-pareto-principle-and-project-failures/2024-09-01-pareto-principle-and-project-failures.html#in-short",
    "href": "pages/posts/2024-09-01-pareto-principle-and-project-failures/2024-09-01-pareto-principle-and-project-failures.html#in-short",
    "title": "The Pareto Principle and Project Failures",
    "section": "In Short",
    "text": "In Short\nThe Pareto principle is both about the big impact you can have from a few actions (e.g., achieve 80% in 20% of the time), and how easily misled you can be about scope and impact (e.g., forgetting about a necessary 20% that takes 80% of the time).\n\n\n\n\nInfographic from Sheraz Ishak"
  },
  {
    "objectID": "pages/posts/2024-08-29-NABCs-of-innovation/2024-08-29-NABCs-of-innovation.html",
    "href": "pages/posts/2024-08-29-NABCs-of-innovation/2024-08-29-NABCs-of-innovation.html",
    "title": "The NABCs of Innovation",
    "section": "",
    "text": "Innovation is creating and delivering new value to customers.\nIt happens at different levels. R&D projects are often expected to fail, but have potential for breakthroughs. Bringing existing technology to new markets is also a form of innovation, possibly with a higher success rate. Incremental optimizations and process improvements also involve innovation and are essential to an efficient business.\nInnovation begins with someone having an idea they think could be valuable. Developing that idea and bringing it to customers requires time an energy.\nA value proposition is what explains why this time and energy should be expended.\nCurtis R. Carlson, ex-President of SRI International, developed a framework for value propositions. It has four main components (the ‚ÄúNABCs‚Äù) that aim to answer essential business questions:\nAdditionally, there should be a driving force behind the proposition, i.e.¬†motivated people willing and able to push this forward. The value proposition should also be aligned with the organization, both to support its development and enable capturing resulting value.\nBuilding a good value proposition is an iterative process. The customer need is what matters and the approach might change - don‚Äôt fall in love with an idea. Focus on customer needs and the reasons underlying what they say they want. Try to quantify the value proposition, even if some of it may be guesswork. Address the most major risks and uncertainties first, before trying to build everything. Maintain and adjust the value proposition throughout the project."
  },
  {
    "objectID": "pages/posts/2024-08-29-NABCs-of-innovation/2024-08-29-NABCs-of-innovation.html#exceptional-innovations",
    "href": "pages/posts/2024-08-29-NABCs-of-innovation/2024-08-29-NABCs-of-innovation.html#exceptional-innovations",
    "title": "The NABCs of Innovation",
    "section": "Exceptional Innovations",
    "text": "Exceptional Innovations\nThe best innovations don‚Äôt just provide new value.\nThey fit within or enable compounding processes, where past innovations keep on providing more and more value as they are built upon. Relatedly, they create more than one opportunity to capture value, i.e.¬†they help expose the business to new opportunities, such as by entering new markets.\nThey align with the business‚Äô strategic vision (its plan for growth) and reinforces its strategic positioning (how it distinguishes itself from competitors and provides compelling value, despite constraints.)"
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html",
    "title": "Strategic Project Management Made Simple",
    "section": "",
    "text": "Everything that follows is a quote from Terry‚Äôs book, with minimal adaptations for flow in some places. It‚Äôs an excellent book. Get it here.\nThe most potent opportunities seldom show up labeled as ‚Äúprojects,‚Äù but arrive disguised as problems, issues, or murky messes. Tackling so called Big, Hairy, Audacious Goals, as Jim Collins describes them in Built to Last, involves juggling a full spectrum of slippery Objectives that can be difficult to define, let alone manage.\nIn the pages ahead, I‚Äôll walk you through a flexible thinking process, and show you how to sort through the fog of fuzzy ideas and develop sound strategies and executable plans. You‚Äôll see how these tools scale up and down to handle issues of any size and flex to fit multiple situations you may face. But first, let‚Äôs review why most project plans are inadequate. See how many of these resonate with your personal experience:"
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#the-four-critical-questions",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#the-four-critical-questions",
    "title": "Strategic Project Management Made Simple",
    "section": "The Four Critical Questions",
    "text": "The Four Critical Questions\nAll great solutions begin by asking the right questions. They seem like simple questions - that‚Äôs exactly the point. They are indeed simple, but not simplistic. The four following carefully crafted questions work wonders in virtually any situation. The first three are usually glossed over in the rush to answer the fourth.\n\nWhat are we trying to accomplish and why?\nThe question of what the project should accomplish - and more importantly - why it needs to be done, deserves fine-tuned attention because those answers drive everything else. In the rush to decide on the how, who, and when of a project, people often gloss over the why.\nHow will we measure success?\nThis question is significant because Measures flesh out and anchor what the Objectives really mean. Until you define how success will be measured, even the most sincere visions are no more than highfalutin‚Äô fluff.\nWhat other conditions must exist?\nThis third question puts your project, issue, or initiative into a larger strategic context. Asking this expands the analysis to include some of the outside factors which may disrupt your carefully crafted plans.\nHow do we get there?\nThe majority of project teams I have worked with tend to delve deep into the details much too soon, or get sidelined by premature technical arguments. They gloss over the first three questions in a rush to get moving. The value of the fourth question comes from consciously placing it in its only, truly functional place in the planning sequence: Last."
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#logframes",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#logframes",
    "title": "Strategic Project Management Made Simple",
    "section": "LogFrames",
    "text": "LogFrames\nWhile the LogFrame matrix may initially seem intimidating, the ideas it captures are basic. The four strategic questions offer a user friendly way to learn and apply this tool. These questions are inherently embedded in the matrix and answering them helps you design your project in a way that connect all the dots.\n\n\n\n\n\n\n\nAlternative LogFrame Diagram\n\n\n\n\n\n\n\n\n\n\nWhat Are We Trying To Accomplish And Why? (Objectives)\nThe first column describes Objectives and the If-Then logic linking them together. The LogFrame makes important distinctions among various ‚Äúlevels‚Äù of Objectives: Strategic intention (Goal), project impact (Purpose), project deliverables (Outcomes), and the key action steps (Inputs).\nHow Will We Measure Success? (Measures and Verifications)\n\nThe second column identifies the Measures of sucess for Objectives at each level. here wew select appropriate Measures and choose quantity, quality, and time indicators to clarify what each Objective means.\nThe third column summarizes how we will verify the status of the Measures at eaech level. Think of the Verification column as the project‚Äôs management information and feedback system.\n\nWhat Other Conditions Must Exist? (Assumptions)\nThe fourth column captures Assumptions; those ever-present, but often neglected risk factors outside of the project, on which project success depends. Defining and testing Assumptions lets you spot potential problems and deal with them in advance.\nHow do we get There? (Inputs)\nThe bottom row captures the project action plan: Who does what, when, and with what resources. Conventional project management like Work Breakdown Structures (WBS) and Gantt chart schedules fit here.\n\n\n\n\n\n\n\nLogFrame Tips\n\n\n\n\n\nLogFrame Tips\n\nTreat the matrix as a summary. Keep it clear and concise; supplement with other documents.\nMake sure everyone on the team has working understanding of the LogFrame (at a minimum, knowing the four critical questions).\nMake sure the right peopole are involved. Invite key stakeholders to participate in project planning.\nStress the importance of the process of planning as much as the plan that comes out of the planning process. Supplement liberally with other supporting tools.\nIterate to make it great. Consider the first Logframe to be a rough draft that will require revision and reworking, perhaps through many cycles.\nBuild in specific milestones on the calendar at which you refine and revise the matrix in the light of new information.\nMonitor and manage changing Assumptions over time.\n\n\n\n\n\n\n\n\n\n\nTurning a Problem Into a Set of Objectives\n\n\n\n\n\nTurning a Problem Into a Set of Objectives\nA problem is simply a project in disguise. Projects masquerading as problems must first be converted into Objectives before advancing to solutions. Spend some time carefully diagnosing the problem because the way you define it shapes the range of solution options. Don‚Äôt get sucked in by an over-simplified definition, catch phrase, or symptom. Get at the root causes. Find the right problem to solve.\nStakeholder collaboration during problem analysis builds shared understanding, generates better solution approaches, and greases the skids for smoother execution.\n\nAsk Your Stakeholders\n\nWhat do you see as the problem?\nWhy is this a problem and for whom?\nWhat causes the problem?\nWhat are the consequences if we ignore the problem?\nHow will you know when the problem is gone?\nWhat benefits will a solution bring?\nWhat might an ideal solution look like?\n\n\n\n\n\n\n\n\n\n\n\nExploring Distinctions Among LogFrame Levels\n\n\n\n\n\nExploring Distinctions Among LogFrame Levels\n\nGoal: The Big Picture Impact\nThe Goal is the big picture context ‚Äî the overarching corporate or strategic Objective to which your project, and usually other projects, contribute.\nSome typical Goal examples:\n\nDelight our customers\nBecome the top provider in the market\nIncrease corporate profits\nEnsure reliability of the nuclear stockpile\nFoster a climate of innovation\nBe the global leader in safety education\n\nThese secondary trigger questions can help you get to the priamary Goal of a project:\n\nWhat is the higher corporate or strategic Objective to which this project contributes?\nWhy is the project‚Äôs impact important?\nWhat should happen after we achieve the Purpose?\nWhat is the big picture reason for doing this project?\n\n\n\nPurpose: The Project Sweet Spot\nPurpose is the vital, often missing focus that expresses the desired result or the impact we expect the project deliverables to produce. It describes expected change in system behavior, whether the system of interest is a core process, a new organization unit, or target customers. Purpose floats a level above that which we can directly control ‚Äî the Outcomes. It‚Äôs a subtle concept, often hard to grasp because we are so conditioned to thinking of activities and Outcomes.\nConsider these examples:\n\n\n\nOutcomes Statement\nCorresponding Purposes\n\n\n\n\nSystem built or delivered\nCustomers use our system\n\n\nProcess improved\nImproved process used\n\n\nSystem developed\nSystem successfully implemented\n\n\nStaff trained in safe procedures\nStaff operates machinery safely\n\n\n\nHere are some trigger questions you can ask to articulate the Purpose:\n\nWhy are we really doing this project?\nWhat would the clients or users like to see happen because of this project?\nIf this project were a success, how would we know?\nWhat impact are we trying to achieve?\n\n\n\nOutcomes: What the Project Will Deliver\nProject Outcomes describe what the team can, must, and commits to make happen to achieve Purpose. They can be functioning systems or processes (i.e., recruiting process operating) as well as completed end products (i.e., prototype built) and delivered services (i.e., people trained). They describe the specifi c end-results (or deliverables) expected from implementing a series of activities or tasks.\nUse these questions to help solidify required Outcomes:\n\nWhat are our main project deliverables?\nWhat do we need to make happen in order to achieve the project Purpose?\nWhat are the end results for which the project team can be held accountable?\nWhat processes do we need to put in place to achieve Purpose?\n\n\n\n\nInputs (Activities)\nOutcomes\n\n\n\n\nTrain users\nUsers trained\n\n\nImprove skills\nSkills improvevd\n\n\nDetermine best methods\nBest methods determined\n\n\nBuild new office\nNew office built\n\n\n\n\n\n\n\n\n\n\n\n\n\nFour Tips for Meaningful Measures\n\n\n\n\n\nFour Tips for Meaningful Measures\nDon‚Äôt fall into the trap of measuring only that which is easy to measure. Measuring Inputs and Outcomes is most straightforward, but progress towards Purpose and Goal is what really counts. The best Measures meet these criteria:\n\nValid ‚Äî They accurately measure the Objective. Changes in the status of Measures accurately reflect changes in the status of the Objective.\nVerifiable ‚Äî Clear, non-subjective evidence exists or can be obtained. This third LogFrame column\nidentifies processes and mechanisms for determining the status of Measures in column two.\nTargeted ‚Äî Quality, quantity, and time targets are pinned down. Choose targets that are sufficient to achieve impact at the next higher level. Sometimes, rather than locking in a single number, it‚Äôs appropriate to state a rough range.\nIndependent ‚Äî Each level in the hierarchy has separate Measures.\n\nGoal Measures tend to be broad macro-Measures that include the long-term impact of one project or multiple projects aimed at the same Goal.\nPurpose Measures describe those conditions we expect will exist when we are willing to call the project a success.\nOutcome Measures describe specific tangible results that the project team can make happen and commits to doing so. Describe them as completed results (using the past tense verb form, such as ‚ÄúSystem developed‚Äùor ‚ÄúTraining completed‚Äù).\nInput Measures deal with activity, budget, and schedule.\n\n\nPurpose Measures are the most important in the hierarchy. Why? Because that‚Äôs your primary aiming point, the what-should-occur result you expect after you deliver what you can.\n\n\n\n\n\n\n\n\n\nThree Steps for Managing Assumptions\n\n\n\n\n\nThree Steps for Managing Assumptions\n\nStep 1. Identify Key Assumptions\nBrainstorm all the conditions you believe are necessary to go from one LogFrame level to the next.\n\n\n\nStep 2. Analyze and Test Them\nTry to assess the degree of risk you can expect from these critical Assumptions by using a simple rating system or probability percentages. Decide which Assumptions to highlight in the LogFrame matrix.\n\nHow important is this Assumption to project success or failure?\nHow valid or probable is this Assumption? What are the odds that it is valid (or not)? Can we express it as a percentage? How do we know?\nIf the Assumptions fail, what is the effect on the project? Does a failed Assumption diminish accomplishment? Delay it? Destroy it?\nWhat could cause this Assumption not to be valid? ‚Äù(Note: This one raises specific risk factors.)\n\n\n\nStep 3. Act on Them\nPut each key Assumption under your mental microscope and consider the following:\n\nIs this a reasonable risk to take?\nTo what extent is it amenable to control? Can we manage it? Influence and nudge it? Or only monitor it\nWhat are some ways we can influence the Assumption?\nWhat contingency plans might we put in place just in case the Assumption proves wrong?\nHow can we design the project to minimize the impact of, or work around, the Assumption?\nIs this Assumption under someone else‚Äôs control?\nHow could we design the project to make this Assumption moot or irrelevant?"
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#exploring-distinctions-among-logframe-levels",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#exploring-distinctions-among-logframe-levels",
    "title": "Strategic Project Management Made Simple",
    "section": "Exploring Distinctions Among LogFrame Levels",
    "text": "Exploring Distinctions Among LogFrame Levels\n\nGoal: The Big Picture Impact\nThe Goal is the big picture context ‚Äî the overarching corporate or strategic Objective to which your project, and usually other projects, contribute.\nSome typical Goal examples:\n\nDelight our customers\nBecome the top provider in the market\nIncrease corporate profits\nEnsure reliability of the nuclear stockpile\nFoster a climate of innovation\nBe the global leader in safety education\n\nThese secondary trigger questions can help you get to the priamary Goal of a project:\n\nWhat is the higher corporate or strategic Objective to which this project contributes?\nWhy is the project‚Äôs impact important?\nWhat should happen after we achieve the Purpose?\nWhat is the big picture reason for doing this project?\n\n\n\nPurpose: The Project Sweet Spot\nPurpose is the vital, often missing focus that expresses the desired result or the impact we expect the project deliverables to produce. It describes expected change in system behavior, whether the system of interest is a core process, a new organization unit, or target customers. Purpose floats a level above that which we can directly control ‚Äî the Outcomes. It‚Äôs a subtle concept, often hard to grasp because we are so conditioned to thinking of activities and Outcomes.\nConsider these examples:\n\n\n\nOutcomes Statement\nCorresponding Purposes\n\n\n\n\nSystem built or delivered\nCustomers use our system\n\n\nProcess improved\nImproved process used\n\n\nSystem developed\nSystem successfully implemented\n\n\nStaff trained in safe procedures\nStaff operates machinery safely\n\n\n\nHere are some trigger questions you can ask to articulate the Purpose:\n\nWhy are we really doing this project?\nWhat would the clients or users like to see happen because of this project?\nIf this project were a success, how would we know?\nWhat impact are we trying to achieve?\n\n\n\nOutcomes: What the Project Will Deliver\nProject Outcomes describe what the team can, must, and commits to make happen to achieve Purpose. They can be functioning systems or processes (i.e., recruiting process operating) as well as completed end products (i.e., prototype built) and delivered services (i.e., people trained). They describe the specifi c end-results (or deliverables) expected from implementing a series of activities or tasks.\nUse these questions to help solidify required Outcomes:\n\nWhat are our main project deliverables?\nWhat do we need to make happen in order to achieve the project Purpose?\nWhat are the end results for which the project team can be held accountable?\nWhat processes do we need to put in place to achieve Purpose?\n\n\n\n\nInputs (Activities)\nOutcomes\n\n\n\n\nTrain users\nUsers trained\n\n\nImprove skills\nSkills improvevd\n\n\nDetermine best methods\nBest methods determined\n\n\nBuild new office\nNew office built"
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#four-tips-for-meaningful-measures",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#four-tips-for-meaningful-measures",
    "title": "Strategic Project Management Made Simple",
    "section": "Four Tips for Meaningful Measures",
    "text": "Four Tips for Meaningful Measures\nDon‚Äôt fall into the trap of measuring only that which is easy to measure. Measuring Inputs and Outcomes is most straightforward, but progress towards Purpose and Goal is what really counts. The best Measures meet these criteria:\n\nValid ‚Äî They accurately measure the Objective. Changes in the status of Measures accurately reflect changes in the status of the Objective.\nVerifiable ‚Äî Clear, non-subjective evidence exists or can be obtained. This third LogFrame column\nidentifies processes and mechanisms for determining the status of Measures in column two.\nTargeted ‚Äî Quality, quantity, and time targets are pinned down. Choose targets that are sufficient to achieve impact at the next higher level. Sometimes, rather than locking in a single number, it‚Äôs appropriate to state a rough range.\nIndependent ‚Äî Each level in the hierarchy has separate Measures.\n\nGoal Measures tend to be broad macro-Measures that include the long-term impact of one project or multiple projects aimed at the same Goal.\nPurpose Measures describe those conditions we expect will exist when we are willing to call the project a success.\nOutcome Measures describe specific tangible results that the project team can make happen and commits to doing so. Describe them as completed results (using the past tense verb form, such as ‚ÄúSystem developed‚Äùor ‚ÄúTraining completed‚Äù).\nInput Measures deal with activity, budget, and schedule.\n\n\nPurpose Measures are the most important in the hierarchy. Why? Because that‚Äôs your primary aiming point, the what-should-occur result you expect after you deliver what you can."
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#three-steps-for-managing-assumptions",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#three-steps-for-managing-assumptions",
    "title": "Strategic Project Management Made Simple",
    "section": "Three Steps for Managing Assumptions",
    "text": "Three Steps for Managing Assumptions\n\nStep 1. Identify Key Assumptions\nBrainstorm all the conditions you believe are necessary to go from one LogFrame level to the next.\n\n\n\nStep 2. Analyze and Test Them\nTry to assess the degree of risk you can expect from these critical Assumptions by using a simple rating system or probability percentages. Decide which Assumptions to highlight in the LogFrame matrix.\n\nHow important is this Assumption to project success or failure?\nHow valid or probable is this Assumption? What are the odds that it is valid (or not)? Can we express it as a percentage? How do we know?\nIf the Assumptions fail, what is the effect on the project? Does a failed Assumption diminish accomplishment? Delay it? Destroy it?\nWhat could cause this Assumption not to be valid? ‚Äù(Note: This one raises specific risk factors.)\n\n\n\nStep 3. Act on Them\nPut each key Assumption under your mental microscope and consider the following:\n\nIs this a reasonable risk to take?\nTo what extent is it amenable to control? Can we manage it? Influence and nudge it? Or only monitor it\nWhat are some ways we can influence the Assumption?\nWhat contingency plans might we put in place just in case the Assumption proves wrong?\nHow can we design the project to minimize the impact of, or work around, the Assumption?\nIs this Assumption under someone else‚Äôs control?\nHow could we design the project to make this Assumption moot or irrelevant?"
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#aligning-projects-with-strategic-intent",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#aligning-projects-with-strategic-intent",
    "title": "Strategic Project Management Made Simple",
    "section": "Aligning Projects With Strategic Intent",
    "text": "Aligning Projects With Strategic Intent\nThe LogFrame can be the cornerstone of any unit-level management system. However, this presumes that there is a sound, overarching strategy to begin with.\nStrategy is the particular means chosen to get from where you are to where you want to go, selected from multiple possibilities and reflecting your vision, mission, and values. An overall Strategy (big ‚ÄúS‚Äù) usually consists of multiple strategic initiatives (small ‚Äús‚Äù), which are executed through programs, projects, and tasks.\nStrategic planning steps:\n\nClarify the Planning Context and Issues - Be clear about your expected planning Outcomes and identify issues to include.\nInvolve Key Players - Decide who to involve in your process to build buy-in and stay-ini.\nScan Your Environment - Identify what‚Äôs changing in your environment; and analyze divvision and department plans to extract Goals your group shares or owns.\nRevisit Your Vision/Mission/Values - Turn these ‚Äúfluff‚Äú statements into high-performance tools that energize staff and build shared commitment.\nSharpen Your Goals and Measures - Develop a meaningful performance scorecard that identifies how you deliver customer value.\nDevelop Core Strategies - Turn Goals into strategies, and test those strategies for impact against Measures to ensure smart choices.\nTurn Strategies into Executable Plans - Using the Logical Framework. Let the responsible players flesh out implementation plans.\nFollow Up and Continue the Process - Build momentum by revieweing and updating the plans while strenghtening the planning process itself."
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#the-strategic-action-cycle",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#the-strategic-action-cycle",
    "title": "Strategic Project Management Made Simple",
    "section": "The Strategic Action Cycle",
    "text": "The Strategic Action Cycle\n\n\nThe cycle begins with ‚ÄúThink,‚Äù the big picture strategic/program focus which follows the process from Chapter 4, or an equivalent strategic planning process.\nResults of strategic thinking identify projects to be managed with the Plan-Act-Assess cycle.\nProject plans created with LogFrames provide a solid foundation for action (execution/implementation) and Assessment.\nThe Assess block can complete the loop in three ways. If assessment shows that success has been achieved - as defined by project Purpose - the project can be considered complete.\n\nProject Monitoring is an ongoing process of tracking budget and schedule against deliverables and making tactical adjustments. It presumes the Logical Framework is the best design and focuses team attention on translating Inputs into Outcomes.\nProject Review is an occasional process that asks managers to step back from the day-to-day work and reassess their approach. It challenges the project design and invites changes in the LogFrame, with emphasis on the Outcome to Purpose link.\nProject Evaluation examines impact and cost effectiveness. Project evaluations are often timed as the end of one phase nears and another is about to begin, or after the project is over. Evaluation examines Purpose to Goal linkages."
  },
  {
    "objectID": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#other",
    "href": "pages/posts/2024-09-04-strategic-project-management-made-simple/2024-09-04-strategic-project-management-made-simple.html#other",
    "title": "Strategic Project Management Made Simple",
    "section": "Other",
    "text": "Other\n\n\n\n\n\n\nTips\n\n\n\n\n\n\nThe process of planning is more crucial than the planning documents that emerge at the other end. The collaborative use of the LogFrame helps you simultaneously build and shape a strong team while they work together to create an actionable plan.\nMake sure that everyone speaks the same language by agreeing on what your key terms mean and using them in a consistent way.\nThe LogFrame matrix usually shows four levels, but Objectives above the Goal can be included to illustrate a higher level of impact. The higher up the hierarchy we climb, the more long-term, general, and ‚Äúvision-sounding‚Äù these Objectives become.\nDon‚Äôt ask ‚ÄúHows it going on this task?‚Äú Instead, ask:\n\nAre you having difficulties that would keep you from meeting targets?\nAre you getting the support you need from others?\nIs there anything else I should know about this?\nWhat do you need from me?\n\nProject monitoring asks ‚ÄúAre we on track?‚Äú; project reviews ask ‚ÄúAre we on the right track?‚Äú Use the LogFrame to challenge your strategy by posing questions such as:\n\nIs our Purpose still valid? What‚Äôs our progress toward Purpose?\nIs our Purpose likely to be achieved with this plan? Will this Purpose get us to the Goal?\nWhat is the status of Assumptions?\nAre these the right Outcomes? Are we producing them effectively?\nShould new Outcomes or Assumptions be added? Existing ones dropped?\nHow should we rervise our key strategic hypotheses (Outcome to Purpose to Goal) to produce better results?\n\nBecause the LogFrame‚Äôs systems thinking underpinnings are generic and flexible, so is the grid format itself. Be innovative and customize the LogFrame to your needs and add your own categories.\nAt times you‚Äôll need to zoom in on a project component for more visibility. Some tasks are large enough to justify their own LogFrame.\nMake responsibilities clear to all\nClarify Resource Requirements\nAnalyze stakeholder interests\nManage with emotional intelligence"
  },
  {
    "objectID": "pages/posts/2020-11-15-validating-arguments-in-r/2020-11-15-validating-arguments.html",
    "href": "pages/posts/2020-11-15-validating-arguments-in-r/2020-11-15-validating-arguments.html",
    "title": "Validating function arguments in R",
    "section": "",
    "text": "Update: The assert package is now available on CRAN:\nI was programming a Gibbs sampler the other day and all hell broke loose: small errors were hard to trace back to the source of the problem and debugging was a pain.\nThe bugs could have been caught much more early if I had properly validated the input arguments of my various helper functions. So I decided it was time for me to learn how to do this properly."
  },
  {
    "objectID": "pages/posts/2020-11-15-validating-arguments-in-r/2020-11-15-validating-arguments.html#validating-function-input-arguments-in-r",
    "href": "pages/posts/2020-11-15-validating-arguments-in-r/2020-11-15-validating-arguments.html#validating-function-input-arguments-in-r",
    "title": "Validating function arguments in R",
    "section": "Validating function input arguments in R",
    "text": "Validating function input arguments in R\nThe easiest way is to manually incorporate checks.\nmySum &lt;- function(a, b) {\n  if (!is.numeric(a) | !is.numeric(b)) {\n    stop(\"Arguments should be numeric.\")\n  }\n  if (length(a) != length(b)) {\n    stop(\"Arguments should be of the same length.\")\n  } \n  \n  return(a+b)\n}\nThis works well enough, but it takes up a lot of space and you have to manually write up the description of the errors.\n\nA first solution\nLet‚Äôs use the assertthat package.\nmySum &lt;- function(a, b) {\n  assert_that(is.numeric(a), is.numeric(b))\n    assert_that(length(a) == length(b))\n  \n  return(a+b)\n}\nThis is neater, but the error messages are not very descriptive.\n&gt; mySum(1, \"1\")\n        Error: b is not a numeric or integer vector \nWhat is b here? What arguments in the function call caused the error? It‚Äôs a bit hard to tell, especially if the call to this function is hidden in some large Gibbs sampler.\n\n\nThe assert function\nMy solution is the assert function which you can find on my Github Gist.\nsource(\"assert.R\")\nUsage is similar to what we did above:\nmySum &lt;- function(a, b) {\n  assert(is.numeric(a), is.numeric(b))\n    assert(length(a) == length(b))\n  \n  return(a+b)\n}\nBut now we have much more descriptive error messages.\n&gt; mySum(1, \"1\")\n        Error: in mySum(a = 1, b = \"1\")\n        Failed checks: \n            is.numeric(b)"
  },
  {
    "objectID": "pages/posts/2019-05-19-global-bounds-for-jensen-functional/2019-05-19-global-bounds-jensen-functional.html",
    "href": "pages/posts/2019-05-19-global-bounds-for-jensen-functional/2019-05-19-global-bounds-jensen-functional.html",
    "title": "Global Bounds for the Jensen Functional",
    "section": "",
    "text": "Given a convex function \\(\\varphi : \\mathbb{R} \\rightarrow \\mathbb{R}\\) and \\(X\\) a random variable on \\(\\mathbb{R}\\), the Jensen functional of \\(\\varphi\\) and \\(X\\) is defined as\n\\[\n\\mathcal{J}(\\varphi, X) = \\mathbb{E}[\\varphi(X)] -\\varphi(\\mathbb{E}[X]).\\tag{1}\n\\]\nThe well-known Jensen inequality states that \\(\\mathcal{J}(\\varphi, X) \\geq 0\\). For instance, if \\(\\varphi(x) = x^2\\), then \\(\\mathcal{J}(\\varphi, X) = \\text{Var}(X) \\geq 0\\). If \\(\\mu\\) and \\(\\nu\\) are two probability measures, \\(X \\sim \\nu\\) and \\(\\varphi\\) is convex with \\(\\varphi(1) = 0\\), then \\(\\mathcal{J}(\\varphi, \\tfrac{d\\mu}{d\\nu}(X)) =: D_\\varphi(\\mu, \\nu)\\) is a so-called \\(f\\)-divergence between probability measures such as the total variation distance, the Kullback-Leibler divergence, the \\(\\chi^2\\) divergence, etc.\nIf \\(X\\) is bounded, then a converse to the Jensen inequality can be easily obtained as follows: let \\(m\\) and \\(M\\) be the infimum and maximum of \\(X\\), and write \\(X = \\alpha m + (1-\\alpha)M\\) for some random variable \\(\\alpha\\) taking values in \\([0,1]\\). Then \\(\\mathbb{E}[\\alpha] = (M - \\mu)/(M-m)\\) and consequently with \\(\\mu:= \\mathbb{E}[X]\\),\n\\[\n\\mathcal{J}(\\varphi, X) \\leq \\mathbb{E}[\\alpha\\phi(m) + (1-\\alpha)\\phi(M)] - \\varphi(\\mu)\n= \\frac{(M-\\mu)\\varphi(m) + (\\mu-m)\\varphi(M)}{M-m}- \\varphi(\\mu).\\tag{2}\n\\]\nWhen \\(\\mu\\) is unknown in practice, then maximizing the above over all possibilities is the bound \\[\n\\mathcal{J}(\\varphi, X) \\leq \\max_{p \\in [0,1]} \\left\\{p\\varphi(m) + (1-p)\\varphi(M) - \\varphi(pm + (1-p) M)\\right\\}\\tag{3}\n\\]\nwhich is Theorem C in Simic (2011)."
  },
  {
    "objectID": "pages/posts/2019-05-19-global-bounds-for-jensen-functional/2019-05-19-global-bounds-jensen-functional.html#some-examples",
    "href": "pages/posts/2019-05-19-global-bounds-for-jensen-functional/2019-05-19-global-bounds-jensen-functional.html#some-examples",
    "title": "Global Bounds for the Jensen Functional",
    "section": "Some examples",
    "text": "Some examples\nVariance bound. Consider for example the case where \\(\\varphi(x) = x^2\\), so that \\(\\mathcal{J}(\\varphi, X) = \\text{Var}(X)\\). Then for \\(X\\) taking values in say \\([0,1]\\), the above bounds read as\n\\[\n\\text{Var}(X) \\leq \\mu(1-\\mu) \\leq 1/4\n\\]\nwhich is a well-known elementary result.\n\\(f\\)-divergence bounds. In (Binette, 2019), I show how we can use similar ideas to get best-possible reverse Pinsker inequalities: upper bounds on \\(f\\)-divergences in terms of the total variation distance and likelihood ratio extremums. In particular, with \\(D(\\mu\\|\\nu) = \\int \\log\\left(\\frac{d\\mu}{d\\nu}\\right) d\\mu\\) the Kullback-Leibler divergence between the probability measures \\(\\mu\\) and \\(\\nu\\), we find that if \\(a = \\inf \\frac{d\\nu}{d\\mu}\\) and \\(b = \\sup \\frac{d\\nu}{d\\mu}\\), then \\[\nD(\\mu|\\nu) \\leq \\sup_A|\\mu(A) - \\nu(A)| \\left(\\frac{\\log(a)}{a-1} +\\frac{\\log(b)}{1-b}\\right).\n\\]\nApplying again the Jensen functional bound to \\(\\sup_A \\lvert \\mu(A)-\\nu(A) \\rvert = \\frac{1}{2}\\int\\left \\lvert \\frac{d\\mu}{d\\nu} - 1\\right \\rvert  d\\nu\\), we obtain\n\\[\n\\sup_A|\\mu(A) - \\nu(A)| \\leq \\frac{(M-1)(1-m)}{M-m}\n\\]\nand this implies the range of values theorem\n\\[\nD(\\mu|\\nu) \\leq \\frac{(a-1)\\log(b) + (1-b)\\log(a)}{b-a}.\n\\]"
  },
  {
    "objectID": "pages/posts/2019-05-19-global-bounds-for-jensen-functional/2019-05-19-global-bounds-jensen-functional.html#variations",
    "href": "pages/posts/2019-05-19-global-bounds-for-jensen-functional/2019-05-19-global-bounds-jensen-functional.html#variations",
    "title": "Global Bounds for the Jensen Functional",
    "section": "Variations",
    "text": "Variations\nIn cases where \\(\\mu\\) is unknown and optimizing over all possibilities is not quite feasible, we can use the following trick.\nLet \\(f(x) = x\\varphi(m) + (1-x)\\varphi(M) - \\varphi(x m +(1-x)M)\\) be the term involved in the maximization step of \\((3)\\). Then \\(f\\) is concave with \\(f(0) = f(1) = 0\\), and hence for any \\(p \\in (0,1)\\) we have that\n\\[\n\\max_{x \\in [0,1]} f(x) \\leq (\\min\\{p, 1-p\\})^{-1}f\\left(pm +(1-p)M\\right).\n\\]\nIn particular, taking \\(p = 1/ 2\\), we obtain the result of Simic (2008) stating that\n\\[\n\\mathcal{J}(\\varphi, X) \\leq \\varphi(m) + \\varphi(M) - 2\\varphi\\left(\\frac{m+M}{2}\\right).\n\\]\nWhen \\(\\varphi\\) is differentiable (this assumption is not strictly necessary but it facilitate the statements), then we can use the concavity of \\(f\\) (using the fact that \\(f(0) = f(1) = 0\\)) to very easily obtain\n\\[\n\\mathcal{J}(\\varphi, X) \\leq \\frac{f'(1)f'(0)}{f'(1)-f'(0)} \\leq \\frac{1}{4}(f'(0)-f'(1)) = \\frac{1}{4}(M-m)(\\varphi'(M) - \\varphi'(m))\n\\]\nwhich is an inequality attributed to S.S. Dragomir (1999), although I haven‚Äôt managed to find the original paper yet."
  },
  {
    "objectID": "pages/posts/2019-01-06-3d-data-visualization-with-webglthreejs/2019-01-06-3d-data-visualization-with-webglthreejs.html",
    "href": "pages/posts/2019-01-06-3d-data-visualization-with-webglthreejs/2019-01-06-3d-data-visualization-with-webglthreejs.html",
    "title": "3D data visualization with WebGL/Three.js",
    "section": "",
    "text": "Javascript app to visualize the positions and depths of earthquakes of magnitude greater than 6 from January 1st 2014 up to January 1st 2019. Data is from the US Geological Survey (usgs.gov). Code is on GitHub.\n\n\n\n\n\nThe original motivation was to make a web tool for high-dimensional data exploration through spherical multidimensional scaling (S-MDS). The basic idea of S-MDS is to map a possibly high-dimensional dataset on the sphere while approximately preserving a matrix of pairwise distances (or divergences). An interactive visualization tool could help explore the mapped dataset and translate observations back to the original data domain. To be continued‚Ä¶\n\n\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2019-09-11-credibility-confidence-intervals/2019-09-11-credibility-confidence-intervals.html",
    "href": "pages/posts/2019-09-11-credibility-confidence-intervals/2019-09-11-credibility-confidence-intervals.html",
    "title": "The Credibility of confidence intervals",
    "section": "",
    "text": "Andrew Gelman and Sander Greenman went ‚Äúhead to head‚Äù in a discussion on the interpretation of confidence intervals in The BMJ. Greenman stated the following, which doesn‚Äôt seem quite right to me.\n\nThe label ‚Äú95% confidence interval‚Äù evokes the idea that we should invest the interval with 95/5 (19:1) betting odds that the observed interval contains the true value (which would make the confidence interval a 95% bayesian posterior interval\\(^{11}\\)). This view may be harmless in a perfect randomized experiment with no background information to inform the bet (the original setting for the ‚Äúconfidence‚Äù concept); more often, however [‚Ä¶]\n\nIt‚Äôs not true that ‚Äúthis view may is harmless in perfect randomized experiments‚Äù, and I‚Äôm not sure where this ‚Äúoriginal setting of the confidence concept‚Äù is coming from. In fact, even in the simplest possible cases, the posterior probability of a \\(95\\%\\) confidence interval can be pretty much anything.\nImagine a ‚Äúperfect randomized experiment‚Äù, where we use a test of the hypothesis \\(H_0: \\mu = 0\\) for which, for some reason, has zero power. If \\(p &lt; 0.05\\), meaning that the associated confidence interval excludes \\(0\\), then we are certain that \\(H_0\\) holds and the posterior probability of the confidence interval is zero.\nLet this sink in. For some (albeit trivial) statistical tests, observing \\(p &lt; 0.05\\) brings evidence in favor of the null.\nThe power of the test carries information, and the posterior probability of a confidence interval (or of an hypothesis), depends on this power among other things, even in perfect randomized experiments.\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2019-04-10-significance-adjusted-r2/2019-04-10-significance-adjusted-r2.html",
    "href": "pages/posts/2019-04-10-significance-adjusted-r2/2019-04-10-significance-adjusted-r2.html",
    "title": "The Significance of the adjusted R squared coefficient",
    "section": "",
    "text": "My friend Anthony Coache and I have been curious about uses and misuses of the adjusted \\(R^2\\) coefficient which comes up in linear regression for model comparison and as a measure of ‚Äúgoodness of fit‚Äù. We were underwhelmed by the depth of the literature arguing for its use, and wanted to show exactly how it behaves under certain sets of assumptions. Investigating the issue brought us to re-interpret the adjusted \\(R^2\\) and to highlight a new distribution-free perspective on nested model comparison which is equivalent, under Gaussian assumptions, to Fisher‚Äôs classical \\(F\\)-test. This generalizes to nested GLMs comparison and provides exact comparison tests that are not based on asymptotic approximations. We still have many questions to answer, but here‚Äôs some of what we‚Äôve done.\nSo, in the context of least squares linear regression, the model for relating a vector of \\(n\\) observed responses \\(Y\\) to \\(p-1\\) independent covariates is \\(Y = X \\beta + \\varepsilon\\), where \\(X\\) is the design matrix and \\(\\varepsilon\\) is the vector of random errors. One of many summary statistics arising from data analyses based on this model is the adjusted \\(R^2\\) coefficient, defined as\n\\[R^2_a(Y, X) = 1 - \\frac{\\|\\hat \\varepsilon\\|^2}{\\left\\|Y - \\bar Y \\right\\|^2}\\frac{n-1}{n-p},\\]\nwhere \\(\\hat \\varepsilon\\) is the vector of residual errors and \\(\\bar Y\\) is the mean of \\(Y\\) (Cramer, 1987; Ohtani, 2004). The \\(R^2\\) coefficient and its adjusted counterpart are widely used as measures of goodness of fit, as model selection criteria and as estimators of the squared multiple correlation coefficient \\(\\rho^2\\) of the parent population. While their properties have been thoroughly studied in these contexts (Olkin, 1958; Helland, 1987; Cramer, 1987; Meepagala, 1992; Ohtani, 2004), the literature is scarce in explanations as to what, exactly, \\(R^2_a\\) adjusts for in non-trivial cases. It is not an unbiased estimator of \\(\\rho^2\\) and the degrees of freedom adjustment heuristic (Theil, 1971) is of limited depth.\nHere we show in what sense the adjusted \\(R^2\\) coefficient may be considered ‚Äúunbiased‚Äù. For nested models comparison, we also suggest how to test the significance of a \\(R^2_a\\) difference between two nested models which is equivalent to Fisher‚Äôs \\(F\\)-test under Gaussian assymptions. The \\(R^2_a\\) test is however done from a largely distribution-free perspective which is conditional on the observation of \\(Y\\). The results are then reinterpreted under classical Gaussian assumptions, which emphasize the dual perspectives between those two tests."
  },
  {
    "objectID": "pages/posts/2019-04-10-significance-adjusted-r2/2019-04-10-significance-adjusted-r2.html#testing-for-an-increase-of-r2_a",
    "href": "pages/posts/2019-04-10-significance-adjusted-r2/2019-04-10-significance-adjusted-r2.html#testing-for-an-increase-of-r2_a",
    "title": "The Significance of the adjusted R squared coefficient",
    "section": "Testing for an increase of \\(R^2_a\\)",
    "text": "Testing for an increase of \\(R^2_a\\)\nSuppose we have two design matrices \\(X\\) and \\(\\tilde X\\), where \\(\\text{Span}(X) \\subset \\text{Span}(\\tilde X)\\). Let \\(p=\\text{rank}(X)\\) and \\(\\tilde p = \\text{rank}(\\tilde X) = p+k\\). Given the vector of observations \\(Y\\), we observe two values \\(R^2 _ a (Y, X)\\) and \\(R^2_ a (Y, \\tilde X)\\) associated to the nested models. The classical way to test for a significant increase of \\(R^2 _ a\\) is to carry out Fisher‚Äôs \\(F\\)-test based on the statistics\n\\[\nF = \\frac{\\| \\hat Y_0 - \\hat Y \\|^2}{\\| Y - \\hat Y \\|^2} \\frac{n - \\tilde p }{k},\n\\]\nwhere \\(\\hat Y_0 = P_X Y\\) and \\(\\hat Y = P_ {\\tilde X}Y\\). This is a function of both \\(R^2_a(Y, X)\\) and \\(R^2_a(Y, \\tilde X)\\), which, under the assumption\n\\[\nH_0: \\quad Y = X \\beta + \\varepsilon\n\\]\nfor \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), has an \\(F\\)-distribution.\nThis is, however, a rather convoluted way of going about comparing the two numbers \\(R^2_a(Y, X)\\) and \\(R^2_a(Y, \\tilde X)\\). Can we do simpler, and can we drop the Gaussian assumption? The answer is yes, although we‚Äôll have to change a bit our point of view on the problem.\n\nA Dual perspective on nested model comparison\nThe whole point of nested model comparison is to see if the new covariates in \\(\\tilde X\\), i.e.¬†those that are not part of \\(\\text{Span}(X)\\), bring new information about \\(Y\\). In the context of an exploratory analysis where the observations and predictors are all observed, we propose to change our perspective to the following testing procedure:\n\ncondition on the observation of \\(Y\\) and \\(X\\) (consider them fixed, observed values);\ntests if the new covariates in \\(\\tilde X\\) are random noise.\n\nHence, rather than testing the model \\(Y = X\\beta + \\varepsilon\\) under a Gaussian noise assumption, we test for covariate randomness, our null hypothesis becomes\n\\[\nH_0':\\quad \\text{the complement of $\\text{Span}(X)$ in $\\text{Span}(\\tilde X)$ is a random subspace.}\n\\]\nThis test can be carried out using any test statistic \\(T\\), and obviously the distribution of \\(T\\) under \\(H_0'\\) (and conditionally on \\(Y\\)), will not depend on the unknown parameter \\(\\beta\\) nor on the noise structure \\(\\varepsilon\\) (which has been conditionned out of randomness). In particular, we can take \\(T = R^2_a(Y, \\tilde X)\\).\nDoes it make any sense? Well it does not change anything! The test obtained in this framework is entirely equivalent to Fisher‚Äôs \\(F\\)-test we reviewed before: for any given observation of \\(Y\\), \\(X\\) and \\(\\tilde X\\), the two tests will give the same results.\nLet me make all of this more precise.\n\n\nSome precisions\nLet \\(\\tilde X = [X \\; W] \\in \\mathbb{R}^{n \\times \\tilde p}\\) be the concatenation of \\(X\\) with a matrix \\(W = [W_1 \\, \\cdots \\, W_k]\\) of \\(k\\) new covariates. The goal is to test whether or not \\(R^2_a(Y, \\tilde X)\\) has significantly increased from \\(R^2_a(Y, X)\\). Henceforth, we shall assume that both \\(Y\\) and \\(X\\) are fixed and the null hypothesis is\n\\[\nH_0':\\; \\text{ the } W_i \\text{ are independent and of uniformly distributed directions.}\n\\]\nBy saying that \\(W_i\\) has a uniformly distributed direction, we mean that \\(W_i/\\|W_i\\|\\) is uniformly distributed on the \\(n\\)-sphere. This is satisfied, for instance, if \\(W_i \\sim N(0, \\sigma_i^2 I_n)\\) and this represents the augmentation of the covariate space through random directions. It is equivalent to saying that the complement of \\(\\text{Span}(X)\\) in \\(\\text{Span}(\\tilde X)\\) is a random subspace. The following proposition shows that the expected value of \\(R^2_a(Y, \\tilde X)\\) is invariant under the addition of such covariates and provides the distribution of \\(R^2_a(Y, \\tilde X)\\) under \\(H_0'\\).\nProposition 1. Let \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^{n \\times p}\\) be fixed and let \\(\\tilde X = [X \\; W_1 \\, \\cdots \\, W_k]\\) be the concatenation of \\(X\\) with \\(k \\leq n- p\\) independent random vectors \\(W_1, \\ldots, W_k\\) of uniformly distributed directions. Then\n\\[\n\\mathbb{E}\\left[ R^2_a(Y, \\tilde X) \\right] = R^2_a(Y, X).\n\\]\nand, more precisely, under \\(H_0\\) we have that \\(R^2_a(Y, \\tilde X)\\) is distributed as\n\\[\n1-  \\frac{(n-1)\\| \\hat \\varepsilon \\|^2}{(n-\\tilde p) \\| Y - \\bar Y \\|^2}\\text{Beta}\\left(\\tfrac{n-\\tilde p}{2}, \\tfrac{k}{2} \\right)\n\\]\nwhere \\(\\text{Beta}\\left(\\tfrac{(n-\\tilde p)}{2}, \\tfrac{k}{2} \\right)\\) is a Beta random variable of parameters \\((n-\\tilde p)/2\\) and \\(k/2\\).\nProof. Let \\(\\omega\\) be the projection of \\([W_1 \\, \\cdots \\, W_k]\\) on the orthogonal \\(V\\) of \\(\\text{Span}(X)\\) and denote by \\(P_\\omega\\) the orthogonal projection onto \\(V_\\omega =  \\text{Span}(\\omega)\\). By the Pythagorean theorem we have \\(\\|Y - P_ {\\tilde X} Y \\|^2 + \\|P_\\omega \\hat \\varepsilon\\|^2 = \\|\\hat \\varepsilon\\|^2\\) and hence we may write\n\\[\n    R^2_a(Y, \\tilde X) = 1- \\frac{(n-1)\\|\\hat \\varepsilon\\|^2}{(n-\\tilde p) \\left\\| Y - \\bar Y\\right\\|^2} \\left(1 - \\frac{\\|P_\\omega \\hat \\varepsilon\\|^2}{\\|\\hat \\varepsilon\\|^2} \\right).\n\\]\nWe now derive the distribution of \\(\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2\\). This term is the squared norm of projection of the unit vector \\(\\hat \\varepsilon / \\|\\hat \\varepsilon\\| \\in V\\) on the random subspace \\(V_\\omega \\subset V\\). Let us now introduce a random unitary matrix \\(U\\) obtained by orthonormalizing \\(\\dim(V) = n - p\\) random vectors of uniformly distributed directions, so that \\(P_\\omega \\hat \\varepsilon\\) is distributed as the first \\(k\\) components of the vector \\(U \\hat \\varepsilon\\). Since \\(U\\hat \\varepsilon / \\|\\hat \\varepsilon\\|\\) is uniformly distributed on the unit sphere of \\(V\\), it follows that the squared norm of its first \\(k\\) components has a \\(\\text{Beta}(k/2, (n-\\tilde p)/2)\\) distribution. In other words, we have shown that \\(\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2 \\sim \\text{Beta}\\left(\\tfrac{k}{2}, \\tfrac{n-\\tilde p}{2} \\right)\\).\nThe expectation of \\(R^2_a(Y, \\tilde X)\\) is obtained from this distributional expression. \\(\\Box\\)\n\n\nReinterpretation under Gaussian hypotheses\nWhile the preceding analysis was conditional on the observation of \\(Y\\), suppose now that \\(Y = X \\beta + \\varepsilon\\), where \\(\\varepsilon \\sim N(0, \\sigma^2)\\) for some \\(\\sigma^2 &gt; 0\\). The distribution of \\(R^2_a(Y, X)\\) is then intricately related to the unknown parameter \\(\\beta\\), preventing a direct analysis.\nHowever, as shown in Cramer (1987), the adjusted \\(R^2\\) coefficient can still be understood as compensating for irrelevant covariates: in a correctly specified model, its expected value is invariant under the addition of covariates. This is formalized in Proposition 2 below. We preferred a more elementary proof than found therein, avoiding the rather involved explicit expression of the expected value that depends on the unknown parameter \\(\\beta\\).\nProposition 2. Suppose \\(Y = X \\beta + \\varepsilon\\), where \\(\\beta \\in \\mathbb{R}^{p}\\) and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\) is Gaussian noise. If \\(\\tilde X\\) is another design matrix of rank \\(\\tilde p\\) such that \\(\\text{Span}(X) \\subset \\text{Span}(\\tilde X)\\), then\n\\[\n\\mathbb{E}\\left[ R^2_a(Y, \\tilde X) \\right] = \\mathbb{E}\\left[R^2_a(Y, X)\\right].\n\\]\nRemark. More precisely, we know the conditional distribution of \\(R^2_a(Y, \\tilde X)\\) given \\(R^2_a(Y, X)\\): it is the same as the distribution which appears in the context of Proposition 1. The above results then follows from a simple computation.\nProof. Let \\(\\hat \\varepsilon^{*} = Y - P_ {\\tilde X} Y\\) and write \\(\\lambda = \\left\\|\\mathbb{E}\\left[Y - \\bar Y\\right]\\right\\|^2/\\sigma^2\\). Then \\(\\frac{\\|\\hat \\varepsilon^{*}\\|^2}{\\left\\|Y - \\bar Y\\right\\|^2}\\) is distributed as \\[\n    \\frac{\\sum_ {i=1}^{n - \\tilde p} Z_i^2}{\\sum_ {i=1}^{n - \\tilde p} Z_i^2 + \\chi^2_ {\\tilde p -1} (\\lambda)}\n\\]\nfor independent \\(Z_i \\sim N(0,1)\\) and \\(\\chi^2_ {\\tilde p - 1} (\\lambda)\\) a noncentral \\(\\chi^2\\) random variable of parameter \\(\\lambda\\). Hence\n\\[\n\\mathbb{E}\\left[ \\frac{\\|\\hat \\varepsilon^{}\\|^2}{\\left\\|Y - \\bar Y\\right\\|^2} \\right] = (n-\\tilde p) \\mathbb{E}\\left[\\frac{Z_1^2}{\\sum{i=1}^{n - \\tilde p} Z_i^2 + \\chi^2{\\tilde p -1} (\\lambda)}\\right]\n    = (n-\\tilde p)K,\n\\]\nwhere \\(K = \\mathbb{E}\\left[\\frac{Z_1^2}{Z_1^2 + \\chi^2_ {n - 2} (\\lambda)}\\right]\\) and \\(\\chi^2_ {n-2}(\\lambda)\\) is a new and independent noncentral \\(\\chi^2\\) random variable. It follows that\n\\[\n    \\mathbb{E} \\left[R^2_a(Y, \\tilde X) \\right] = 1 - (n-1)K\n\\]\ndepends on \\(\\tilde X\\) only through \\(X\\) and must equal \\(\\mathbb{E} \\left[R^2_a(Y, X) \\right]\\). \\(\\Box\\)\n\n\nRelationship with Fisher‚Äôs \\(F\\)-test\nIn the context of Proposition 2, suppose in particular that \\(\\tilde X = [X \\; W]\\), where \\(W = [W_1\\, \\cdots \\, W_k]\\) is a matrix of additional fixed regressors. Recall that the \\(F\\)-statistic for Fisher‚Äôs test with nested models of \\(p\\) and \\(\\tilde p = p + k\\) parameters respectively is given by\n\\[\nF = \\frac{\\| \\hat Y_0 - \\hat Y \\|^2}{\\| Y - \\hat Y \\|^2} \\frac{n - \\tilde p }{k},\n\\]\nwhere \\(\\hat Y_0 = P_X Y\\) and \\(\\hat Y = P_{\\tilde X} Y\\) are the vector of predicted values for the models corresponding to \\(X\\) and \\(\\tilde X\\). The test of significance devised in Section 2, based on \\(R^2_a(Y, \\tilde X)\\), is then equivalent to Fisher‚Äôs \\(F\\)-test of the hypothesis\n\\[\nH_0^{\\text{Gauss}}:\\; Y = X\\beta + \\varepsilon\\, \\text{ where }\\,\\varepsilon \\sim N(0, \\sigma^2 I_n).\n\\]\nTo see this, let \\(\\omega\\) be, as in the proof of Proposition 1, the projection of \\([W_1\\,\\cdots\\,W_k]\\) on the orthogonal of \\(\\text{Span}(X)\\) and denote by \\(P_\\omega\\) the projection on \\(\\text{Span}(\\omega)\\). Then the \\(F\\)-statistic can be written as\n\\[\n    F = \\frac{\\|P_\\omega \\hat \\varepsilon\\|^2}{\\|\\hat \\varepsilon - P_\\omega \\hat \\varepsilon \\|^2} \\frac{n-\\tilde p}{k} = \\frac{\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2}{1 - \\|P_\\omega \\hat \\varepsilon \\|^2/\\|\\hat \\varepsilon\\|^2} \\frac{n-\\tilde p}{k}.\n\\]\nThis is a monotonous invertible transform of \\(\\|P_\\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2\\) which, under \\(H_0^{\\text{Gauss}}\\), follows a Beta distribution of parameters \\(k/2\\) and \\((n-\\tilde p)/2\\). Yet in the framework of Section 2 and under \\(H_0\\), where now \\(\\omega\\) is random and \\(\\hat \\varepsilon\\) fixed, the test statistic \\(R^2_ a(Y, \\tilde X)\\) is also a monotonous invertible function of \\(\\|P_ \\omega \\hat \\varepsilon\\|^2/\\|\\hat \\varepsilon\\|^2 \\sim \\text{Beta}(k/2, (n-\\tilde p)/2)\\). This shows that the two unilateral tests are equivalent: the same observations yield the same \\(p\\)-values."
  },
  {
    "objectID": "pages/posts/2019-04-10-significance-adjusted-r2/2019-04-10-significance-adjusted-r2.html#discussion",
    "href": "pages/posts/2019-04-10-significance-adjusted-r2/2019-04-10-significance-adjusted-r2.html#discussion",
    "title": "The Significance of the adjusted R squared coefficient",
    "section": "Discussion",
    "text": "Discussion\nWe have highlighted dual perspectives on nested models comparison. An increase of \\(R^2\\) may be due to random noise that correlates with fixed regressors, or to random regressors that correlate with fixed observations. Fisher‚Äôs test of the first hypothesis is equivalent to the \\(R^2_a\\) test of the second. Furthermore, we showed that \\(R^2_a\\) compensates properly, on the average, for both types of inflation of \\(R^2\\). We suggest this provides a clear explanation of what \\(R^2_a\\) exactly adjusts for and how it can properly be used for models comparison.\nFurthermore, the fact that random covariate tests, conditional on the observations, can be carried out exactly using any measure of goodness of fit (e.g.¬†the likelihood or the AIC) suggests that our approach may be helpful in devising nested model comparison tests for GLMs. Testing at a chosen confidence level also provides more flexibility than using a rule-based procedure such as the AIC.\nReferences\n\nCramer, J. S. (1987). Mean and variance of r2 in small and moderate samples. Journal of Econometrics 35(2), 253 ‚Äì 266.\nHelland, I. S. (1987). On the interpretation and use of r2 in regression analysis. Biometrics 43(1), 61‚Äì69.\nMeepagala, G. (1992). The small sample properties of r2 in a misspecified regression model with stochastic regressors. Economics Letters 40(1), 1 ‚Äì 6.\nOhtani, K. and H. Tanizaki (2004). Exact distributions of r2 and adjusted r2 ina linear regression model with multivariate t error terms. Journal of the Japan Statistical Society 34(1), 101‚Äì109.\nOlkin, I. and J. W. Pratt (1958). Unbiased estimation of certain correlation coefficients. The Annals of Mathematical Statistics 29(1), 201‚Äì211.\nTheil, H. (1971). Principles of econometrics (1 ed.). New York: J. Wiley."
  },
  {
    "objectID": "pages/posts/2019-04-15-two-sampleing-algorithms/2019-04-15-two-sampling-algorithms.html",
    "href": "pages/posts/2019-04-15-two-sampleing-algorithms/2019-04-15-two-sampling-algorithms.html",
    "title": "Two sampling algorithms for trigonometric densities",
    "section": "",
    "text": "Trigonometric densities (or non-negative trigonometric sums) are probability density functions of circular random variables (i.e.¬†\\(2\\pi\\)-periodic densities) which take the form \\[\nf(u) = a_0 + \\sum_{k=1}^n(a_k \\sin(k u) + b_k\\cos(ku)) \\tag{1}\n\\]\nfor some real coefficients \\(a_k, b_k \\in \\mathbb{R}\\) which are such that \\(f(u) \\geq 0\\) and \\(a_0 = \\frac{1}{2\\pi} \\int f(u)\\,du = (2\\pi)^{-1}\\). These provide flexible models of circular distributions. Circular density modelling comes up in studies about the mechanisms of animal orientation and also come up in bio-informatics in relationship to the protein structure prediction problem (the secondary structure of a protein - the way its backbone folds - is determined by a sequence of angles).\nHere I am discussing two simple sampling algorithms for such trigonometric densities. The first is the rejection sampling algorithm proposed in Fern√°ndez-Dur√°n et al.¬†(2014) and the second uses negative mixture sampling."
  },
  {
    "objectID": "pages/posts/2019-04-15-two-sampleing-algorithms/2019-04-15-two-sampling-algorithms.html#algorithm-1-naive-rejection-sampling",
    "href": "pages/posts/2019-04-15-two-sampleing-algorithms/2019-04-15-two-sampling-algorithms.html#algorithm-1-naive-rejection-sampling",
    "title": "Two sampling algorithms for trigonometric densities",
    "section": "Algorithm 1: Naive rejection sampling",
    "text": "Algorithm 1: Naive rejection sampling\nGiven an uniform upper bound \\(C\\) on the family \\(\\mathcal{V}_n\\) of trigonometric densities, we can sample from a given \\(f\\in \\mathcal{V}_n\\) using simple rejection sampling as follows:\n\nLet \\((x, y)\\) be uniformly distributed over \\([0, 2\\pi) \\times [0, C]\\);\nIf \\(y \\leq f(x)\\), then return \\(x\\); otherwise return to step 1.\n\nNow the problem is to figure out a good upper bound \\(C\\). The most basic idea is to do as in Fernandez-Duran et al.¬†(2014) and to apply the Cauchy-Schwarz inequality\n\\[\nf(u) = \\left\\| \\sum_{k=0}^n c_k e^{i k u} \\right\\|^2 \\leq \\|c\\|^2 \\sum_{k=0}^n|e^{iku}| = \\frac{n+1}{2\\pi}.\n\\]\nCan we find a better bound? I think that \\(C = \\sqrt{n}/\\pi\\) would work, but I have no clue how to prove it‚Ä¶.\nLet‚Äôs implement this in R.\n\nImplementation\nFirst we need a trigonometric density model.\ntrig_function &lt;- function(c_real, complex=NULL) {\n  # Returns the trigonometric function defined as either:\n  #     f(u) = 1/(2\\pi) + \\sum_{k=1}^{n} c_real[2*k-1] \\sin(k u) + c_real[2*k] \\cos(ku),\n  # or\n  #   f(u) = \\| \\sum_{k=0}^n complex e^{i k u} \\|^2,\n  # where n is the degree of the polynomial.\n  #\n  # Args\n  #   c_real: Vector of 2*n real numbers, where n is the degree of \n  #           the trigonometric polynomial.\n  #   complex: Vector of (n+1) complex numbers.\n  \n  if (!is.null(complex)) {\n    lambd &lt;- function(u) {\n      n = length(complex) - 1\n      k = 0:n\n      return(abs(sum(complex * exp(u * k * 1i)))**2)\n    }\n  }\n  else {\n    lambd &lt;- function(u) {\n      n = length(c_real)/2\n      k = 1:n\n      return(1/(2*pi) + sum(c_real[2*k - 1] * cos(k*u)) + sum(c_real[2*k] * cos(k*u)))\n    }\n  }\n  return(Vectorize(lambd));\n}\nWe can also generate random trigonometric densities of a fixed degree as follows.\nrtrig &lt;- function(n) {\n  u = rnorm(n);\n  v = rnorm(n);\n  c_comp = u + v*1i;\n  c_comp = c_comp / (sqrt(2*pi*sum(abs(c_comp)**2)));\n  return(trig_function(complex=c_comp))\n}\nUsage is like this:\nu = seq(0, 2*pi, 0.005)\nplot(u, rtrig(10)(u), type=\"l\")\n\nAnd finally we can implement the naive rejection sampling algorithm.\nnaive_rejection_sampling &lt;- function(f, n) {\n  # Returns a random variate following the trigonometric density f of degree n.\n  drawn = FALSE\n  while(!drawn) {\n    x = runif(1)*2*pi\n    y = runif(1)*(n+1) / (2*pi)\n    if (y &lt; f(x)) {\n      drawn = TRUE\n    }\n  }\n  return(x);\n}"
  },
  {
    "objectID": "pages/posts/2019-04-15-two-sampleing-algorithms/2019-04-15-two-sampling-algorithms.html#algorithm-2-negative-mixture-sampling",
    "href": "pages/posts/2019-04-15-two-sampleing-algorithms/2019-04-15-two-sampling-algorithms.html#algorithm-2-negative-mixture-sampling",
    "title": "Two sampling algorithms for trigonometric densities",
    "section": "Algorithm 2: Negative Mixture Sampling",
    "text": "Algorithm 2: Negative Mixture Sampling\nAnother approach to simulate from trigonometric densities relies on the De la Vall√©e Poussin mixture representation. That is, any \\(f\\in \\mathcal{V}_n\\) can be written as\n\\[\nf = \\alpha f_a - (\\alpha - 1) f_b,\\qquad f_a = \\sum_{j=0}^{2n} a_j C_{j_n}, \\quad f_b = \\sum_{j=0}^{2n} b_j C_{j,n},\n\\]\nwhere $ $, \\(a_j, b_j \\geq 0\\) and \\(\\sum_ j a_j = \\sum_ j b_ j = 1\\). We can assume that \\(a_j b_j = 0\\) for every $j $; i.e.¬†there is no redundancy in the components of \\(f_a\\) and \\(f_b\\). The density \\(f_b\\) accounts for negative weights in the mixture representation of \\(f\\) using the De la Vall√©e Poussin densities \\((3)\\).\nWe can now sample from \\(f\\) using samples from \\(f_a\\) and a simple rejection method.\nAlgorithm 2.\n\nLet \\(x \\sim f_a\\).\nReturn \\(x\\) with probability \\(\\frac{f(x)}{\\alpha f_a(x)}\\); otherwise return to step 1.\n\n\nImplementation\nDe la Vall√©e Poussin densities and its random variate generator.\ndvallee &lt;- function(u, j, n) {\n  # De la Vall√©e Poussin density $C_{j,n}(u)$\n  \n  return(2^n * (1+cos(u - (2*pi*j)/(2*n+1)))^n / (2*pi*choose(2*n, n)))\n}\nrvallee &lt;- function(j, n, m) {\n  # Returns m random variates following the De la Vall√©e Poussin density $C_{j,n}$.\n  \n  V = runif(m) &gt; 0.5\n  W = rbeta(m, 1/2, 1/2 + n)\n  return((1-2*V)*acos(1-2*W) + (2*pi*j)/(2*n + 1))\n}\nUsage:\ns = rvallee(2, 5, 10000)\nu = seq(-pi, pi, 0.05)\nhist(s, prob=TRUE, xlim=c(-pi,pi))\nlines(u, dvallee(u, 2, 5), col=2)\n\nDe la Vall√©e Poussin mixtures.\ndValleeMixture &lt;- function(coeffs) {\n  # De la Vall√©e Poussin mixture densities\n  \n  n = (length(coeffs) - 1)/2;\n  \n  lambd &lt;- function(u) {\n    j = 0:(2*n)\n    return(sum(dvallee(u, j, n) * coeffs))\n  }\n  \n  return(Vectorize(lambd))\n}\nrValleeMixture &lt;- function(coeffs) {\n  # Random sample from a De la Vall√©e Poussin mixture density. The mixture weights are allowed to take negative values.\n  \n  f = dValleeMixture(coeffs)\n  n = (length(coeffs) - 1)/2\n\n  a = coeffs * (coeffs &gt; 0)\n  b = coeffs * (coeffs &lt; 0)\n  \n  alpha = sum(a)\n  a = a / alpha\n  b = b / (1-alpha)\n  fa = dValleeMixture(a)\n  \n  drawn = FALSE\n  while(!drawn) {\n    # Sample from f_a\n    i = sample(0:(2*n), 1, prob = a)\n    x = rvallee(i, n, 1)\n    if ( runif(1) &lt;  f(x)/(alpha*fa(x))) {\n      drawn = TRUE\n    }\n  }\n  \n  return(x %% (2*pi))\n}\nExample:\ncoeffs = c(0.55, -0.15, 0.55, 0, 0, 0,0.05)\nf = dValleeMixture(coeffs)\nu = seq(0, 2*pi, 0.05)\ns = replicate(50000, rValleeMixture(coeffs))\nhist(s, prob=T, ylim=c(0, 0.6))\nlines(u, f(u), col=2)\n\n\nOther things we could do:\n\nThe black box Lipschitz sampling algorithm can also be used to sample from trigonometric densities. This requires to compute good upper bounds on the Lipchitz constant on the density, which should be doable using the De la Vall√©e Poussin mixture representation."
  },
  {
    "objectID": "pages/posts/2017-11-05-sampling-lipschitz-continuous-densities.html",
    "href": "pages/posts/2017-11-05-sampling-lipschitz-continuous-densities.html",
    "title": "2017-11-05-sampling-lipschitz-continuous-densities",
    "section": "",
    "text": "ReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2017-11-05-sampling-lipschitz-continuous-densities/2017-11-05-sampling-lipschitz-continuous-densities.html",
    "href": "pages/posts/2017-11-05-sampling-lipschitz-continuous-densities/2017-11-05-sampling-lipschitz-continuous-densities.html",
    "title": "Sampling Lipschitz Continuous Densities",
    "section": "",
    "text": "Full code: https://github.com/OlivierBinette/LipSample\n\nfunction [sample, x, y] = lipsample(f, L, limits, m, varargin)\n% Random variates from a Lipschitz continuous probability density function on [a,b].\n%\n%   s = lipsample(@f, L, [a b], m)\n%       Draws _m_ random variates from the probability density _f_ on [_a_, _b_] \n%       which is Lipchitz continuous of order _L_. If _f_ is continuously \n%       differentiable, then the best choice of _L_ is the maximum value \n%       of its derivative.\n%\n%   s = lipsample(..., 'N', n)\n%       ... Uses _n_ mixtures components in the spline envelope of _f_. \n%       The default choice is n = ceil(2*_L_), although increasing _n_ may\n%       improve performance in some cases.\n%\n%       \n%   [s, x, y] = lipsample(@f, L, [a b], m)\n%       ... Returns the spline envelope constructed by the algorithm: the\n%       envelope linearly interpolates the points (x,y).\n%\n%   Dependencies\n%   ------------\n%     - Function discretesample.m\n%\n%   Examples\n%   --------\n%   % In file myfunc.m\n%       function y = myfunc(x)\n%           y = 1 + cos(2*pi*x)\n%       end\n%\n%   % A few exact samples\n%       sample = lipsample(@myfunc, 2*pi, [0 1], 10000);\n%\n%   % Plot 10 million variates.\n%       sample = lipsample(@myfunc, 2*pi, [0 1], 10000000);\n%       hold on\n%       pretty_hist(sample, [0 1]);\n%       plot(linspace(0,1), myfunc(linspace(0,1)));\n%       hold off\n%\n%   % Plot the envelope constructed by the algorithm\n%       [sample, x, y] = lipsample(@myfunc, 4*pi, [0 1], 10000);\n%       u = linspace(0, 1, 200);\n%       hold on\n%       pretty_hist(sample, [0 1]);\n%       plot(u, myfunc(u));\n%       plot(u, interp1(x,y,u));\n%       hold off\n%       \n%\n%   Implementation details\n%   ----------------------\n%     - Acceptance-rejection sampling. A first degree spline envelope of _f_\n%       is constructed. The number of components is a function of _L_, chosen\n%       as to maximize expected efficiency.\n%\n%   Warnings\n%   --------\n%       _L_ must be greater or equal to the best Lipschitz continuity constant\n%       of _f_. Otherwise the algorithm may fail to yield exact samples.\n%\n%     - Efficiency bottleneck is the evaluation of _f_ at O(m) points. \n%\n%   CC-BY O.B. sept. 15 2017\n\n    % Parse input arguments.\n    a = limits(1);\n    b = limits(2);\n\n    p = inputParser;\n    addOptional(p, 'N', ceil(200*L) + 200);\n    \n    parse(p, varargin{:});\n    n = p.Results.N;\n        \n    % Construct the spline envelope.\n    s = (b-a) * L / (2*n);\n    x = linspace(0,1,n+1);\n    y = arrayfun(f, x*(b-a) + a);\n    ylow = arrayfun(f, x*(b-a) + a);\n    \n    % Use the Lipschitz constant to locally adjust the spline.\n    alpha = atan(L);\n    d = diff(y);\n    beta = abs(atan(n*d/(b-a)));\n    r = 0.5*sqrt(((b-a)/n )^2 + d.^2).*sin(pi-alpha-beta)./sin(alpha);\n    h = r.*(L - abs(n*d/(b-a)));\n    y(1) = y(1) + h(1); ylow(1) = ylow(1) - h(1);\n    y(n+1) = y(n+1) + h(n); ylow(n+1) = ylow(n+1) - h(n);\n    for i = 2:n\n        y(i) = y(i) + max(h(i-1), h(i));\n        ylow(i) = ylow(i) - max(h(i-1), h(i));\n    end\n            \n    % Generate random variates following the envelope.\n    nProp = ceil((1+s)*m);\n    U1 = rand(1, nProp);\n    U2 = rand(1, nProp);\n\n    y(1) = y(1)/2;\n    y(end) = y(end)/2;\n    I = discretesample(y, nProp);\n    y(1) = 2*y(1);\n    y(end) = 2*y(end);\n\n    U = abs((U1 + U2 + I - 2)/n);\n    U(U &gt; 1) = 2 - U(U &gt; 1); % The sample.\n\n    % Generate from  f\n    V = rand(1, nProp);\n    B = interp1(x, ylow, U);\n    passlow = lt(V .* interp1(x,y,U), B);\n    sample1 = U(passlow);\n    U = U(~passlow); V = V(~passlow);\n    sample2 = U(lt(V.*interp1(x,y,U), arrayfun(f, U*(b-a)+a)));\n    sample = (b-a)*cat(2, sample1, sample2) + a;\n    \n    if numel(sample) &lt; m\n        sample = cat(2, sample, lipsample(f, L, [a b], m - numel(sample)));\n    else\n        sample = sample(1:m);\n    end\n    \n    x = x *(b-a) + a;\nend\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2023-12-12-reality-ideality-gap-entity-resolution/2023-12-12-reality-ideality-gap-entity-resolution.html",
    "href": "pages/posts/2023-12-12-reality-ideality-gap-entity-resolution/2023-12-12-reality-ideality-gap-entity-resolution.html",
    "title": "What is the Reality-Ideality-Gap in Entity Resolution?",
    "section": "",
    "text": "Wang et al (2022) describe the frustration when real-world performance does not match expectations obtained from benchmark datasets. This difference is the ‚Äúreality-ideality‚Äù gap which is all too common in real-world applications of entity resolution.\n\nWhy does it happen? They posit that three main issues limit the generalizability of current benchmarks, specifically in the context of deep learning approaches to entity resolution:\n\n1. ùêìùê°ùêûùê´ùêû ùê¢ùê¨ ùê•ùêûùêöùê§ùêöùê†ùêû ùêüùê´ùê®ùê¶ ùê≠ùê°ùêû ùê≠ùê´ùêöùê¢ùêßùê¢ùêßùê† ùê¨ùêûùê≠ ùê¢ùêßùê≠ùê® ùê≠ùê°ùêû ùê≠ùêûùê¨ùê≠ ùê¨ùêûùê≠. In typical benchmark constructions, record pairs are randomly sampled, leading to the same cluster being represented in both the train and test dataset. This biases results, especially in deep learning approaches which rely on learning record embeddings.\n\n2. ùêëùêûùêöùê•-ùê∞ùê®ùê´ùê•ùêù ùêùùêöùê≠ùêö ùê¢ùê¨ ùê¶ùêÆùêúùê° ùê¶ùê®ùê´ùêû ùê¢ùê¶ùêõùêöùê•ùêöùêßùêúùêûùêù ùê≠ùê°ùêöùêß ùêõùêûùêßùêúùê°ùê¶ùêöùê´ùê§ ùêùùêöùê≠ùêöùê¨ùêûùê≠ùê¨ in terms of matching vs non-matching record pairs. In other words, there is much more opportunity for error in real data than in a benchmark dataset.\n\n3. Partly as a consequence of the two above issues, ùê≠ùê≤ùê©ùê¢ùêúùêöùê• ùêõùêûùêßùêúùê°ùê¶ùêöùê´ùê§ùê¨ ùêÆùêßùêùùêûùê´ùêûùê¨ùê≠ùê¢ùê¶ùêöùê≠ùêû ùê≠ùê°ùêû ùê¢ùê¶ùê©ùê®ùê´ùê≠ùêöùêßùêúùêû ùê®ùêü ùêöùêùùêùùê¢ùê≠ùê¢ùê®ùêßùêöùê• ùêüùêûùêöùê≠ùêÆùê´ùêûùê¨ ùêöùêßùêù ùê¶ùêÆùê•ùê≠ùê¢ùê¶ùê®ùêùùêöùê• ùê¢ùêßùêüùê®ùê´ùê¶ùêöùê≠ùê¢ùê®ùêß. This leads to under-specified systems which do not perform as well as they could.\n\nThe paper goes on to define clear tasks for entity resolution systems and detail issues with current benchmarks:\n\n\n‚ÄúOur findings reveal that previous benchmarks biased the evaluation of the progress of current entity matching approaches, and there is still a long way to go to build effective entity matchers.‚Äù\n\n\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/teaching.html#presentations",
    "href": "pages/teaching.html#presentations",
    "title": "Teaching",
    "section": "",
    "text": "Date\nEvent\nTopic\n\n\n\n\nFeb 2025\nDuke Responsible AI Symposium\nPoster on ‚Äô‚ÄôImproving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework‚Äù\n\n\nOct 2024\nPrinceton Symposium on Advances in Record Linkage\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2024\nJoint Statistical Meetings\nEnd-to-End Evaluation of Entity Resolution Systems"
  },
  {
    "objectID": "pages/teaching.html#classes",
    "href": "pages/teaching.html#classes",
    "title": "Teaching",
    "section": "Classes",
    "text": "Classes\n\nInstructional TA at Duke University\n\nSTA 323 - Statistical Computing (Spring 2024)\nSTA 440 - Case Studies in Statistical Science (Fall 2023)\nSTA 723 - Case Studies in Statistical Science (Spring 2023)\nSTA 344 - Spatio-Temporal Models (Fall 2022)\nSTA 199 - Introduction to Data Science (Fall 2021)\nSTA 490/690 - Entity Resolution (Spring 2021)\nSTA 360 - Bayesian and Modern Statistics (Fall 2020) [teaching evaluation]\n\n\n\nInstructional TA at Universit√© du Qu√©bec √† Montr√©al\n\nSTT 1000 - Statistics I (Winter 2019)\nMAT 1130 - Analysis I (Winter 2019)\nMAT 2710 - Probability II (Fall 2018)\nMAT 2160 - Complex Analysis (Winter 2018)\nMAT 1190 - Analysis and algebra for the actuarial sciences (Winter 2018)\nMAT 2150 - Analysis II (Fall 2017 & Fall 2018)\n\n\n\nOther teaching activities\n\nCo-produced an activity for children at the Eureka! Science Festival (Montreal Science Center, June 2018)"
  },
  {
    "objectID": "pages/teaching.html#presentationstalks",
    "href": "pages/teaching.html#presentationstalks",
    "title": "Teaching",
    "section": "",
    "text": "Date\nEvent\nTopic\n\n\n\n\nFeb 2025\nDuke Responsible AI Symposium\nPoster on ‚ÄúImproving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework‚Äù\n\n\nOct 2024\nPrinceton Symposium on Advances in Record Linkage\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2024\nJoint Statistical Meetings\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2023\nJoint Statistical Meetings\nOn the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery"
  },
  {
    "objectID": "pages/teaching.html#talks",
    "href": "pages/teaching.html#talks",
    "title": "Teaching",
    "section": "",
    "text": "Date\nEvent\nTopic\n\n\n\n\nFeb 2025\nDuke Responsible AI Symposium\nPoster on ‚ÄúImproving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework‚Äù\n\n\nOct 2024\nPrinceton Symposium on Advances in Record Linkage\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2024\nJoint Statistical Meetings\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2023\nJoint Statistical Meetings\nOn the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery"
  },
  {
    "objectID": "pages/teaching.html#recent-talks",
    "href": "pages/teaching.html#recent-talks",
    "title": "Teaching",
    "section": "",
    "text": "Date\nEvent\nTopic\n\n\n\n\nFeb 2025\nDuke Responsible AI Symposium\nPoster on ‚ÄúImproving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework‚Äù\n\n\nOct 2024\nPrinceton Symposium on Advances in Record Linkage\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2024\nJoint Statistical Meetings\nEnd-to-End Evaluation of Entity Resolution Systems\n\n\nSummer 2023\nJoint Statistical Meetings\nOn the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery"
  },
  {
    "objectID": "pages/posts/2020-11-15-analysis-problem/analysis-problem.html",
    "href": "pages/posts/2020-11-15-analysis-problem/analysis-problem.html",
    "title": "An analysis problem",
    "section": "",
    "text": "Problem from F√©lix Locas:\nLet \\(r(n) = \\lfloor \\log_2 \\frac{n}{\\log_2 n} \\rfloor\\). Show that \\[\\lim_{n \\rightarrow \\infty} \\left( \\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} \\right)^n = 1.\\]\nMy solution:\nThe series \\(\\sum_{k=1}^{\\infty} \\frac{1}{k(k+1) 2^k}\\) is easy to calculate. It is, for instance, the difference between the integrals of geometric series: \\[\\sum_{k=1}^\\infty \\frac{1}{k(k+1) 2^k} = \\sum_{k=1}^\\infty \\frac{1}{k 2^k} - \\sum_{k=1}^\\infty \\frac{1}{(k+1) 2^k} = 1-\\log 2.\\]\nFurthermore, abbreviating \\(r = r(n)\\), \\[r^{3/2} 2^{r} \\sum_{k=r+1}^\\infty \\frac{1}{k(k+1) 2^{k}} \\le \\sum_{k=0}^\\infty \\frac{1}{\\sqrt{r} 2^k} \\xrightarrow{r \\rightarrow \\infty} 0\\]\nimplies that for \\(n\\) sufficiently large we have \\(\\sum_{k=r+1}^\\infty \\frac{1}{k(k+1) 2^{k}} &lt; r^{-3/2} 2^{-r}\\) and \\[\\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} = 1-\\sum_{k=r+1}^{\\infty} \\frac{1}{k(k+1) 2^k} \\geq 1 - r^{-3/2} 2^{-r}. \\qquad (*)\\]\nFinally, since \\(r = \\log_2 \\frac{n}{\\log_2 n} - \\varepsilon_n\\) for some \\(0 \\le\\varepsilon_n &lt; 1\\), we have \\[n r^{-3/2} 2^{-r} = \\frac{2^{\\varepsilon_n}\\log_2 n}{(\\log_2 n - \\log_2 \\log_2 n - \\varepsilon_n)^{3/2}} \\rightarrow 0\\]\nwhich implies that \\[\\left( 1 - r^{-3/2} 2^{-r} \\right)^n \\rightarrow 1.\\]\nSince also \\(\\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} \\le 1\\) comparing this with \\((*)\\) yields \\[\\lim_{n \\rightarrow \\infty} \\left( \\log 2+\\sum_{k=1}^{r(n)} \\frac{1}{k(k+1) 2^k} \\right)^n = 1.\\]\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2020-11-15-bayesian-numerical-analysis/bayesian-numerical-analysis.html",
    "href": "pages/posts/2020-11-15-bayesian-numerical-analysis/bayesian-numerical-analysis.html",
    "title": "Bayesian numerical analysis",
    "section": "",
    "text": "Distributing points \\(\\{x_i\\}_{i=1}^n\\) on the sphere as to minimize the mean square error\n\\[\\mathbb{E}\\left[\\left(q_n(f) - \\int_{\\mathbb{S}^2}f(s)\\,ds\\right)^2\\right]\\]\nof the quadrature formula \\(q_n(f) =\\frac{1}{n}\\sum_{i=1}^n f(x_i)\\), where \\(f\\) is a centered Gaussian process with covariance function \\(C(x,y) = \\exp(\\langle x, y \\rangle)\\). Shown is \\(n=6, 12, 23\\).\n\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html",
    "href": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html",
    "title": "Bayesian Optimalities",
    "section": "",
    "text": "This first section is not directly about properties of the posterior distribution, but it is rather concerned with some summaries of the posterior which have nice statistical properties in different contexts.\n\n\nSuppose \\(\\pi\\) is a prior on an euclidean parameter space \\(\\Theta \\subset \\mathbb{R}^d\\) with norm \\(\\|\\theta\\|^2 = \\theta^T \\theta\\) defined through the dot product. Given a likelihood \\(p _ \\theta(X)\\) for data \\(X\\), the posterior distribution is defined as\n\\[\n\\pi(A \\mid X) \\propto \\int _ A p _ \\theta(X) \\pi(d\\theta)\n\\]\nand the mean of the posterior distribution is\n\\[\n\\hat \\theta _ {\\pi} = \\int \\theta \\,\\pi(d\\theta \\mid X) = \\mathbb{E} _ {\\theta \\sim \\pi}[\\theta \\mid X].\n\\]\nIf we define the risk of an estimator \\(\\hat \\theta\\) for the estimation of a parameter \\(\\theta _ 0\\) as\n\\[\nR(\\hat \\theta; \\theta _ 0) = \\mathbb{E} _ {X \\sim p _ \\theta}[\\|\\theta _ 0 - \\hat \\theta(X)\\|^2],\n\\]\nand if\n\\[\nB _ \\pi(\\hat \\theta) = \\mathbb{E} _ {\\theta _ 0 \\sim \\pi}[R(\\hat \\theta; \\theta _ 0)]\n\\]\nis the expected risk of \\(\\hat \\theta\\) with respect to the prior \\(\\pi\\) (also called the Bayes risk), then we have that the posterior mean estimate \\(\\hat \\theta _ {\\pi}\\) satisfies\n\\[\nB _ \\pi(\\hat \\theta) \\geq B _ \\pi(\\hat \\theta _ \\pi)\n\\]\nfor any estimator \\(\\hat \\theta\\). That is, the posterior mean estimate minimizes the expected risk.\nThe proof follows from the fact that\n\\[\n\\| \\theta _ 0 - \\hat \\theta(X) \\|^2 \\geq \\|\\theta _ 0 - \\hat \\theta _ \\pi(X) \\|^2 + \\langle \\theta _ 0 - \\hat \\theta _ \\pi(X), \\hat \\theta _ \\pi(X) - \\hat \\theta(X)\\rangle.\n\\]\nWriting the expected risk as an expected posterior loss, i.e.¬†using the fact that\n\\[\n\\mathbb{E} _ {\\theta _ 0 \\sim \\pi}\\left[\\mathbb{E} _ {X \\sim p _ {\\theta _ 0}}[\\,\\cdot\\,]\\right] = \\mathbb{E} _ {X \\sim m}\\left[\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\,\\cdot\\,]\\right]\n\\]\nwhere \\(m\\) has density \\(m(x) = \\int p _ \\theta(x) \\pi(\\theta)\\,d\\theta\\), and since\n\\[\n\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}\\left[\\langle \\theta _ 0 - \\hat \\theta _ \\pi(X), \\hat \\theta _ \\pi(X) - \\hat \\theta(X)\\rangle\\right] = 0,\n\\]\nwe obtain the result.\nA few remarks:\n\nThe expected risk has stability properties. If \\(\\tilde \\pi\\) and \\(\\pi\\) are two priors that are absolutely continuous with respect to each other, and if \\(\\|\\log \\frac{d\\tilde \\pi}{d\\pi}\\| _ \\infty \\leq C\\), then\n\n\\[\n   e^{-C}B _ \\pi(\\hat\\theta) \\leq B _ {\\tilde \\pi}(\\hat \\theta) \\leq e^C B _ {\\pi}(\\hat \\theta).\n\\]\nIf the risk \\(R(\\hat \\theta; \\theta _ 0)\\) is uniformly bounded by some constant \\(M\\) over \\(\\theta _ 0\\in \\Theta\\), then\n\\[\n   B _ {\\tilde \\pi}(\\hat \\theta) \\leq \\sqrt{M B _ {\\pi}(\\hat \\theta)} \\left\\|d\\tilde\\pi/d\\pi\\right\\| _ {L^2(\\pi)}.\n\\]\nThis shows how small chances in the prior does not result in a dramatic change in the expected loss of an estimator, as long as the priors have ‚Äúcompatible tails‚Äù (i.e.¬†a manageable likelihood ratio).\n\nIt is sometimes advocated to choose the prior \\(\\pi\\) so that the risk \\(R(\\hat \\theta _ \\pi; \\theta _ 0)\\) is constant over \\(\\theta _ 0\\): the resulting estimator \\(\\hat \\theta _ \\pi\\) is then agnostic, from a risk point of view, to \\(\\theta _ 0\\). This may result in a sample-size dependent prior (which is arguably not in the Bayesian spirit), but the fun thing is that it makes the expected risk maximal and the Bayes estimator \\(\\hat \\theta _ \\pi\\) minimax: \\(\\hat \\theta  _  \\pi \\in \\arg\\min  _   {\\hat\\theta} \\sup  _   {\\theta  _  0}R(\\hat \\theta;\\theta  _  0)\\). Indeed, in that case we have for any estimator \\(\\hat \\theta\\) that \\(\\sup   _   {\\theta  _  0} R(\\hat \\theta; \\theta  _  0) \\geq B  _  \\pi(\\hat \\theta) \\geq B  _   \\pi(\\theta  _  \\pi) = \\sup  _   {\\theta  _   0}R(\\hat \\theta  _  \\pi;\\theta  _  0)\\), from which it follows that \\(\\hat \\theta  _  \\pi\\) is minimax.\nThe idea of minimizing expected risk is not quite Bayesian, since it required us to first average over all data possibilities when computing the risk. One of the main advantage of the Bayesian framework is that it allows us to condition over the observed data, rather than pre-emptively considering all possibilities, and we can try to make use of that. Define the posterior expected loss (or posterior risk) or an estimator \\(\\hat \\theta\\), conditionally on \\(X\\), as\n\n\\[\n   R _ \\pi(\\hat \\theta\\mid X) = \\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}\\left[(\\hat \\theta(X) - \\theta _ 0)^2\\right].\n\\]\n\n\nIt is clear from the previous computations that the posterior mean estimate minimizes the posterior risk, and hence the two approaches are equivalent. It turns out that, whatever the loss function we consider (under some regularity condition ensuring that stuff is finite and minimizers exist), minimizing the posterior risk is equivalent to minimizing the Bayes risk. In other words, we have that for any loss function (again under some regularity conditions ensuring finiteness and existence of stuff), we have\n\n\n\\[\n   \\arg\\min _ {\\hat \\theta}\\mathbb{E} _ {X \\sim m}\\left[\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\ell(\\hat \\theta(X), \\theta _ 0)]\\right] = \\arg\\min _ {\\hat\\theta}\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\ell(\\hat \\theta(X), \\theta _ 0)].\n\\]\n\n\nThis is roughly self-evident if we think about it. An interesting consequence is that any estimator minimizing a Bayes risk is a function of the posterior distribution."
  },
  {
    "objectID": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html#point-estimation-and-minimal-expected-risk",
    "href": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html#point-estimation-and-minimal-expected-risk",
    "title": "Bayesian Optimalities",
    "section": "",
    "text": "This first section is not directly about properties of the posterior distribution, but it is rather concerned with some summaries of the posterior which have nice statistical properties in different contexts.\n\n\nSuppose \\(\\pi\\) is a prior on an euclidean parameter space \\(\\Theta \\subset \\mathbb{R}^d\\) with norm \\(\\|\\theta\\|^2 = \\theta^T \\theta\\) defined through the dot product. Given a likelihood \\(p _ \\theta(X)\\) for data \\(X\\), the posterior distribution is defined as\n\\[\n\\pi(A \\mid X) \\propto \\int _ A p _ \\theta(X) \\pi(d\\theta)\n\\]\nand the mean of the posterior distribution is\n\\[\n\\hat \\theta _ {\\pi} = \\int \\theta \\,\\pi(d\\theta \\mid X) = \\mathbb{E} _ {\\theta \\sim \\pi}[\\theta \\mid X].\n\\]\nIf we define the risk of an estimator \\(\\hat \\theta\\) for the estimation of a parameter \\(\\theta _ 0\\) as\n\\[\nR(\\hat \\theta; \\theta _ 0) = \\mathbb{E} _ {X \\sim p _ \\theta}[\\|\\theta _ 0 - \\hat \\theta(X)\\|^2],\n\\]\nand if\n\\[\nB _ \\pi(\\hat \\theta) = \\mathbb{E} _ {\\theta _ 0 \\sim \\pi}[R(\\hat \\theta; \\theta _ 0)]\n\\]\nis the expected risk of \\(\\hat \\theta\\) with respect to the prior \\(\\pi\\) (also called the Bayes risk), then we have that the posterior mean estimate \\(\\hat \\theta _ {\\pi}\\) satisfies\n\\[\nB _ \\pi(\\hat \\theta) \\geq B _ \\pi(\\hat \\theta _ \\pi)\n\\]\nfor any estimator \\(\\hat \\theta\\). That is, the posterior mean estimate minimizes the expected risk.\nThe proof follows from the fact that\n\\[\n\\| \\theta _ 0 - \\hat \\theta(X) \\|^2 \\geq \\|\\theta _ 0 - \\hat \\theta _ \\pi(X) \\|^2 + \\langle \\theta _ 0 - \\hat \\theta _ \\pi(X), \\hat \\theta _ \\pi(X) - \\hat \\theta(X)\\rangle.\n\\]\nWriting the expected risk as an expected posterior loss, i.e.¬†using the fact that\n\\[\n\\mathbb{E} _ {\\theta _ 0 \\sim \\pi}\\left[\\mathbb{E} _ {X \\sim p _ {\\theta _ 0}}[\\,\\cdot\\,]\\right] = \\mathbb{E} _ {X \\sim m}\\left[\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\,\\cdot\\,]\\right]\n\\]\nwhere \\(m\\) has density \\(m(x) = \\int p _ \\theta(x) \\pi(\\theta)\\,d\\theta\\), and since\n\\[\n\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}\\left[\\langle \\theta _ 0 - \\hat \\theta _ \\pi(X), \\hat \\theta _ \\pi(X) - \\hat \\theta(X)\\rangle\\right] = 0,\n\\]\nwe obtain the result.\nA few remarks:\n\nThe expected risk has stability properties. If \\(\\tilde \\pi\\) and \\(\\pi\\) are two priors that are absolutely continuous with respect to each other, and if \\(\\|\\log \\frac{d\\tilde \\pi}{d\\pi}\\| _ \\infty \\leq C\\), then\n\n\\[\n   e^{-C}B _ \\pi(\\hat\\theta) \\leq B _ {\\tilde \\pi}(\\hat \\theta) \\leq e^C B _ {\\pi}(\\hat \\theta).\n\\]\nIf the risk \\(R(\\hat \\theta; \\theta _ 0)\\) is uniformly bounded by some constant \\(M\\) over \\(\\theta _ 0\\in \\Theta\\), then\n\\[\n   B _ {\\tilde \\pi}(\\hat \\theta) \\leq \\sqrt{M B _ {\\pi}(\\hat \\theta)} \\left\\|d\\tilde\\pi/d\\pi\\right\\| _ {L^2(\\pi)}.\n\\]\nThis shows how small chances in the prior does not result in a dramatic change in the expected loss of an estimator, as long as the priors have ‚Äúcompatible tails‚Äù (i.e.¬†a manageable likelihood ratio).\n\nIt is sometimes advocated to choose the prior \\(\\pi\\) so that the risk \\(R(\\hat \\theta _ \\pi; \\theta _ 0)\\) is constant over \\(\\theta _ 0\\): the resulting estimator \\(\\hat \\theta _ \\pi\\) is then agnostic, from a risk point of view, to \\(\\theta _ 0\\). This may result in a sample-size dependent prior (which is arguably not in the Bayesian spirit), but the fun thing is that it makes the expected risk maximal and the Bayes estimator \\(\\hat \\theta _ \\pi\\) minimax: \\(\\hat \\theta  _  \\pi \\in \\arg\\min  _   {\\hat\\theta} \\sup  _   {\\theta  _  0}R(\\hat \\theta;\\theta  _  0)\\). Indeed, in that case we have for any estimator \\(\\hat \\theta\\) that \\(\\sup   _   {\\theta  _  0} R(\\hat \\theta; \\theta  _  0) \\geq B  _  \\pi(\\hat \\theta) \\geq B  _   \\pi(\\theta  _  \\pi) = \\sup  _   {\\theta  _   0}R(\\hat \\theta  _  \\pi;\\theta  _  0)\\), from which it follows that \\(\\hat \\theta  _  \\pi\\) is minimax.\nThe idea of minimizing expected risk is not quite Bayesian, since it required us to first average over all data possibilities when computing the risk. One of the main advantage of the Bayesian framework is that it allows us to condition over the observed data, rather than pre-emptively considering all possibilities, and we can try to make use of that. Define the posterior expected loss (or posterior risk) or an estimator \\(\\hat \\theta\\), conditionally on \\(X\\), as\n\n\\[\n   R _ \\pi(\\hat \\theta\\mid X) = \\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}\\left[(\\hat \\theta(X) - \\theta _ 0)^2\\right].\n\\]\n\n\nIt is clear from the previous computations that the posterior mean estimate minimizes the posterior risk, and hence the two approaches are equivalent. It turns out that, whatever the loss function we consider (under some regularity condition ensuring that stuff is finite and minimizers exist), minimizing the posterior risk is equivalent to minimizing the Bayes risk. In other words, we have that for any loss function (again under some regularity conditions ensuring finiteness and existence of stuff), we have\n\n\n\\[\n   \\arg\\min _ {\\hat \\theta}\\mathbb{E} _ {X \\sim m}\\left[\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\ell(\\hat \\theta(X), \\theta _ 0)]\\right] = \\arg\\min _ {\\hat\\theta}\\mathbb{E} _ {\\theta _ 0 \\sim \\pi(\\cdot \\mid X)}[\\ell(\\hat \\theta(X), \\theta _ 0)].\n\\]\n\n\nThis is roughly self-evident if we think about it. An interesting consequence is that any estimator minimizing a Bayes risk is a function of the posterior distribution."
  },
  {
    "objectID": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html#randomized-estimation-and-information-risk-minimization",
    "href": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html#randomized-estimation-and-information-risk-minimization",
    "title": "Bayesian Optimalities",
    "section": "2. Randomized estimation and information risk minimization",
    "text": "2. Randomized estimation and information risk minimization\nLet \\(\\Theta\\) be a model, let \\(X \\sim Q\\) be some data and let \\(\\ell _ \\theta(X)\\) be a loss associated with using \\(\\theta\\) to fitting the data \\(X\\). For instance, we could have \\(\\Theta = \\{f:\\mathcal{X} \\rightarrow \\mathbb{R}\\}\\) a set of functions, \\(X =\\{(U _ i, Y _ i)\\} _ {i=1}^n \\subset \\mathcal{X}\\times \\mathbb{R}\\) a set of features with associated responses, and \\(\\ell _ \\theta(X) = \\sum _ {i}(Y _ i -\\theta(U _ i))^2\\) the sum of squared loss.\nThere may be a parameter \\(\\theta _ 0\\in\\Theta\\) minimizing the risk \\(R(\\theta) = \\mathbb{E} _ {X\\sim Q}[\\ell _ \\theta(X)]\\), which will then be our learning target. Now we consider randomized estimators taking the form \\(\\theta\\sim \\hat \\pi _ X\\), where \\(\\hat\\pi _ X\\) is a data-dependent distribution, and the performance of this estimation method can then be evaluated by the empirical risk \\(R _ X (\\hat\\pi _ X) = \\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\ell _ \\theta(X)]\\).\nHere we should be raising an eyebrow. There is typically no point in having the estimator \\(\\theta\\) being random, i.e.¬†we typically will prefer to take \\(\\hat \\pi _ X\\) a point mass rather than anything else. But bear with me for a sec.¬†The cool thing is that if we choose\n\\[\n\\hat \\pi _ X = \\arg\\min _ {\\hat \\pi _ X} \\left\\{R(\\hat \\pi _ X) + D(\\hat \\pi _ X \\| \\pi)\\right\\}, \\tag{$*$}\n\\]\nwhere \\(D(\\hat \\pi _ X\\| \\pi) = \\int \\log \\frac{d\\hat \\pi _ X}{d\\pi} \\,d\\hat \\pi _ X\\) is the Kullback-Leibler divergence, then this distribution will satisfy\n\\[\nd\\hat \\pi _ X(\\theta) \\propto e^{-\\ell _ \\theta(X)}d\\pi(\\theta).\n\\]\nThat is, Bayesian-type posteriors arise by minimizing the empirical risk of a randomized estimation scheme penalized by the Kullback-Leibler divergence form prior to posterior (Zhang, 2006).\nFor the proof, write\n\\[\nR _ X(\\hat \\pi _ X) + D(\\hat \\pi _ X \\| \\pi) = \\int \\left(\\ell _ \\theta(X) + \\log\\frac{d\\hat \\pi _ X(\\theta)}{d\\pi(\\theta)}\\right) d\\hat \\pi _ X (\\theta)=\\int\\left(\\log\\frac{d\\hat\\pi _ X(\\theta)}{e^{-\\ell _ \\theta(X)}d\\pi(\\theta)}\\right)d\\hat \\pi _ X(\\theta)\n\\]\nwhich is also equal to \\(D(d\\hat \\pi _ X \\| e^{-\\ell _ \\theta(X)} d\\pi)\\) and, by properties of the Kullback-Leibler divergence, obviously minimized at \\(d\\hat \\pi _ X \\propto e^{\\ell _ \\theta(X)}d\\pi(\\theta)\\).\nIs this practically useful and insightful? Possibly. But at least this approach is suited to a general theory, as shown in Zhang (2006) and as I reproduce below.\nLet us introduce a R√©nyi-type generalization error defined, for \\(\\alpha \\in (0,1)\\), by\n\\[\nd _ \\alpha(\\theta; Q) = -\\alpha^{-1}\\log\\mathbb{E} _ {X' \\sim Q}[e^{-\\alpha \\ell _ \\theta(X')}].\n\\]\nThis is a measure of loss associated with the use of a parameter \\(\\theta\\) to fit new data \\(X' \\sim Q\\). We also write\n\\[\nd _ \\alpha(\\hat \\pi _ X; Q) = -\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}\\left[ \\alpha^{-1}\\log\\mathbb{E} _ {X' \\sim Q}[e^{-\\alpha \\ell _ \\theta(X')}] \\right]\n\\]\nfor the expected R√©nyi generalization error when using the randomization scheme \\(\\theta \\sim \\hat \\pi _ X\\).\nIn order to get interesting bounds on this generalization error, we can follow the approach of Zhang (2006).\n\nChange of measure inequality\nWe‚Äôll need the change of measure inequality, which states that for any function \\(f\\) and distributions \\(\\pi\\), \\(\\hat \\pi\\) on \\(\\Theta,\\)\n\\[\n\\mathbb{E} _ {\\theta \\sim \\hat\\pi}[f(\\theta)] \\leq D(\\hat \\pi \\| \\pi) + \\log \\mathbb{E} _ {\\theta \\sim \\pi}\\left[e^{f(\\theta)}\\right].\n\\]\nIndeed, with some sloppyness and Jensen‚Äôs inequality we can compute\n\\[\n\\log \\int e^{f(\\theta)}\\pi(d\\theta)\\geq \\int f(\\theta)\\log(d\\pi/d\\hat\\pi(\\theta))d\\hat \\pi = \\mathbb{E} _ {\\theta \\sim \\hat \\pi}[f(\\theta)] - D(\\hat \\pi\\|\\pi).\n\\]\n\n\nGeneralization error bound\nWe can now attempt bounding \\(d _ \\alpha(\\hat \\pi _ X;Q)\\). Consider the difference \\(\\Delta _ X (\\theta) = d _ \\alpha(\\theta;Q) - \\ell _ \\theta(X)\\) between the generalization error and the empirical loss corresponding to the use of a fixed parameter \\(\\theta\\). Then by the change of measure inequality,\n\\[\n\\exp\\{\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\Delta _ X(\\theta)] - D(\\hat \\pi _ X\\|\\pi)\\} \\leq \\mathbb{E} _ {\\theta \\sim \\pi}\\left[e^{\\Delta _ X(\\theta)}\\right]\n\\]\nand hence for any \\(\\pi\\),\n\\[\n\\mathbb{E} _ {X \\sim Q}\\left[\\exp\\left\\{\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\Delta _ X(\\theta)] - D(\\hat \\pi _ X\\|\\pi)\\right\\}\\right] \\leq \\mathbb{E} _ {X \\sim Q}\\left[\\mathbb{E} _ {\\theta \\sim \\pi}\\left[e^{\\Delta _ X(\\theta)}\\right]\\right] = 1\n\\]\nBy Markov‚Äôs inequality, this implies that \\(\\forall t &gt; 0\\),\n\\[\n\\mathbb{P}\\left(\\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\Delta _ X(\\theta)] - D(\\hat \\pi _ X\\|\\pi) \\geq t\\right) \\leq e^{-t}.\n\\]\nRewriting yields\n\\[\nd _ \\alpha(\\hat \\pi _ X;Q) \\leq R _ X(\\hat \\pi _ X) + D(\\hat \\pi _ X\\|\\pi) + t\n\\]\nwith probability at least \\(1-e^{-t}\\). To recap: the term \\(d _ \\alpha(\\hat \\pi _ X;Q)\\) is understood as a generalization error, on the right hand side \\(R _ X(\\hat \\pi _ X) = \\mathbb{E} _ {\\theta \\sim \\hat \\pi _ X}[\\ell _ \\theta(X)]\\) is the empirical risk, the Kullback-Leibler divergence \\(D(\\hat \\pi _ X\\|\\pi)\\) penalizes the complexity of \\(\\hat\\pi _ X\\) seen as a divergence from a ‚Äúprior‚Äù \\(\\pi\\), and \\(t\\) is a tuning parameter."
  },
  {
    "objectID": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html#online-learning-regret-and-kullback-leibler-divergence",
    "href": "pages/posts/2020-11-15-bayesian-optimalities/bayesian-optimalities.html#online-learning-regret-and-kullback-leibler-divergence",
    "title": "Bayesian Optimalities",
    "section": "3. Online learning, regret and Kullback-Leibler divergence",
    "text": "3. Online learning, regret and Kullback-Leibler divergence\nFollowing Barron (1998), suppose we sequentially observe data points \\(X _ 1, X _ 2, X _ 3, \\dots\\) which are say i.i.d. with common distribution \\(Q\\) with density \\(q\\). At each time step \\(n\\), the goal is to predict \\(X _ {n+1}\\) using the data \\(X^n = (X _ 1, \\dots, X _ n)\\). Our prediction is not a point estimate of \\(X _ {n+1}\\), but somewhat similarly as in the randomized estimation scenario we output a density estimate \\(\\hat p _ n = p(\\cdot \\mid X^n)\\), the goal being that \\(p(X _ {n+1}\\mid X^n)\\) be as large as possible. A bit more precisely, we individually score a density estimate \\(\\hat p _ n\\) through the risk \\(\\ell _ q(\\hat p _ n) = \\mathbb{E} _ {X _ {n+1}\\sim q}[\\log(q(X _ {n+1})/\\hat p _ n(X _ {n+1} ))] = D(q\\| \\hat p _ n)\\) which is the Kullback-Leibler divergence between \\(\\hat p _ n\\) and \\(q\\). The regret over times \\(n=1, 2,\\dots, N\\) is the sum of the risk over the whole process, i.e.\n\\[\n\\text{regret} = \\sum _ {n=1}^N D(q\\| \\hat p _ n).\n\\]\nFormally, this process is equivalent to estimating the distribution of \\(X^N\\) all at once: our density estimate \\(\\hat p^N\\) of \\(X^N\\) would simply be\n\\[\n\\hat p^N(X^N) = \\prod _ {n=1}^N \\hat p _ n(X _ n)\n\\]\nand the regret is, by the chain rule, simply \\(D(q^N \\| \\hat p^N)\\), where \\(q^N\\) is the \\(N\\)th independent product of \\(q\\).\nGiven a prior \\(\\pi\\) over a space of distributions for \\(q\\), our problem then to minimize the Bayes risk\n\\[\nB _ \\pi(\\hat p^N) = \\mathbb{E} _ {q\\sim \\pi} D(q^N\\|\\hat p^N).\n\\]\nThis is achieved by choosing \\(\\hat p^N(x) = \\hat p _ \\pi^N(x) = \\int q^N(x) \\pi(dq)\\) the prior predictive density. This is equivalent to using, at each time step \\(n\\), the poterior predictive density \\(\\hat p _ {n, \\pi}(x) = \\int q(x) \\,\\pi(dq\\mid \\{X  _  i\\}  _ {i=1}^n)\\).\nTo see this minimizing property of the Bayes average, it suffices to write\n\\[\nB _ \\pi(\\hat p^N) = \\mathbb{E} _ {q \\sim \\pi} \\left[D(q^N\\| \\hat p _ \\pi^N)\\right] + D(\\hat p _ \\pi^N \\| \\hat p^N).\n\\]\nNote that an consequence of this analysis is also that the posterior predictive distribution \\(\\hat p _ {n, \\pi}\\) will minimize the expected posterior risk:\n\\[\n\\hat p _ {n, \\pi} \\in \\arg\\min _ {\\hat p _ {n}} \\mathbb{E} _ {q \\sim \\pi(\\cdot\\mid X^n)}\\left[D(q\\|\\hat p _ n)\\right].\n\\]\nFollowing section 1, this furthermore means that the posterior predictive distribution minimizes the Bayes risk associated with the Kullback-Leibler loss."
  },
  {
    "objectID": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html",
    "href": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html",
    "title": "Posterior Concentration in terms of the Separation Alpha-Entropy",
    "section": "",
    "text": "This post continues the series on posterior concentration under misspecification. Here I introduce an unifying point of view on the subject through the introduction of the separation \\(\\alpha\\)-entropy. We use this notion of prior entropy to bridge the gap between Bayesian fractional posteriors and regular posterior distributions: in the case where this entropy is finite, direct analogues to some of the concentration results for fractional posteriors (Bhattacharya et al., 2019) are recovered.\nThis post is going to be quite abstract, just like last week. I‚Äôll talk in a future post about how this separation \\(\\alpha\\)-entropy generalizes generalizes the covering numbers for testing under misspecification of Kleijn et al.¬†(2006) as well as the prior summability conditions of De Blasi et al.¬†(2013).\nQuick word of warning: this is not the definitive version of the results I‚Äôm working on, but I still had to get them out somewhere.\nAnother word of warning: Wordpress has gotten significantly worse at dealing with math recently. I will find a new platform, but for now expect to find typos and some rendering issues."
  },
  {
    "objectID": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#the-framework",
    "href": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#the-framework",
    "title": "Posterior Concentration in terms of the Separation Alpha-Entropy",
    "section": "The framework",
    "text": "The framework\nWe continue in the same theoretical framework as before: \\(\\mathbb{F}\\) is a set of densities on a complete and separable metric space \\(\\mathcal{X}\\) with respect to a \\(\\sigma\\)-finite measure \\(\\mu\\) defined on the Borel \\(\\sigma\\)-algebra of \\(\\mathcal{X}\\), \\(H\\) is the Hellinger distance defined by \\[ H(f, g) = \\left(\\int \\left(\\sqrt{f} - \\sqrt{g}\\right)^2 \\, d\\mu\\right)^{1/2}\\] and we make use of the R√©nyi divergences defined by \\[ d_\\alpha(f, g) = -\\alpha^{-1}\\log A_\\alpha(f, g),\\quad A_\\alpha(f, g) = \\int f^{\\alpha}g^{1-\\alpha} \\,d\\mu .\\] Here we assume that data is generated following a distribution \\(f_0 \\in \\mathbb{F}\\) having a density in our model (this assumption could be weakened), and therefore defined the off-centered R√©nyi divergence \\[ d_\\alpha^{f_0}(f, f^\\star) = -\\alpha^{-1}\\log(A_\\alpha^{f_0}(f, f^\\star))\\] where \\[ A_\\alpha^{f_0}(f, f^\\star) = \\int (f/f^\\star)^\\alpha f_0\\,d\\mu\\] assuming that all this is well defined.\n\nPrior and posterior distributions\nNow let \\(\\Pi\\) be a prior on \\(\\mathbb{F}\\). Given either a single data point \\(X \\sim f_0\\) or a sequence of independent variables \\(X^{(n)} = \\{X_i\\}_{i=1}^n\\) with common probability density function \\(f_0\\), the posterior distribution of \\(\\Pi\\) given \\(X^{(n)}\\) is the random quantity \\(\\Pi(\\cdot \\mid X^{(n)})\\) defined by \\[ \\Pi\\left(A\\mid X^{(n)}\\right) = \\int_A \\prod_{i=1}^n f(X_i) \\Pi(df)\\Big/ \\int_{\\mathbb{F}} \\prod_{i=1}^n f(X_i) \\Pi(df)\\] and \\(\\Pi(\\cdot \\mid X) = \\Pi(\\cdot \\mid X^{(1)})\\). This may not always be well-defined, but I don‚Äôt want to get into technicalities for now."
  },
  {
    "objectID": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#separation-alpha-entropy",
    "href": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#separation-alpha-entropy",
    "title": "Posterior Concentration in terms of the Separation Alpha-Entropy",
    "section": "Separation \\(\\alpha\\)-entropy",
    "text": "Separation \\(\\alpha\\)-entropy\nWe state our concentration results in terms of the separation \\(\\alpha\\)-entropy. It is inspired by the Hausdorff \\(\\alpha\\)-entropy introduced in Xing et al.¬†(2009), although the separation \\(\\alpha\\)-entropy has no relationship with the Hausdorff measure and instead builds upon the concept of \\(\\delta\\)-separation of Choi et al.¬†(2008) defined below.\nGiven a set \\(A \\subset \\mathbb{F}\\), we denote by \\(\\langle A \\rangle\\) the convex hull of \\(A\\): it is the set of all densities of the form \\(\\int_A f \\,\\nu(df)\\) where \\(\\nu\\) is a probability measure on \\(A\\).\nDefinition (\\(\\delta\\)-separation). Let \\(f_0 \\in \\mathbb{F}\\) be fixed as above. A set of densities \\(A \\subset \\mathbb{F}\\) is said to be \\(\\delta\\)-separated from \\(f^\\star \\in \\mathbb{F}\\) with respect to the divergence \\(d_\\alpha^{f_0}\\) if for every \\(f \\in \\langle A \\rangle\\), \\[ d_\\alpha^{f_0}\\left(f, f^\\star\\right) \\geq \\delta.\\] A collection of sets \\(\\{A_i\\}_{i=1}^\\infty\\) is said to be \\(\\delta\\)-separated from \\(f_0\\) if every $A {A_i}_{i=1}^$ is \\(\\delta\\)-separated from \\(f_0\\).\nAn important property of \\(\\delta\\)-separation, first noted by Walker (2004) and used for the study of posterior consistency, is that it scales with product densities. The general statement of the result is stated in the following lemma.\nLemma (Separation of product densities). Let \\((\\mathcal{X}_i, \\mathcal{B}_{i}, \\mu_i)\\), \\(i \\in\\{ 1,2, \\dots, n\\}\\), be a sequence of \\(\\sigma\\)-finite measured spaces where each \\(\\mathcal{X}_i\\) is a complete and separable locally compact metric space and \\(\\mathcal{B}_i\\) is the corresponding Borel \\(\\sigma\\)-algebra. Denote by \\(\\mathbb{F}_i\\) the set of probability density functions on \\((\\mathcal{X}_i, \\mathcal{B}{i}, \\mu_i)\\), fix \\(f_{0,i} \\in \\mathbb{F}_i\\) and let \\(A_i \\subset \\mathbb{F}_i\\) be \\(\\delta_i\\)-separated from \\(f_{i}^\\star \\in \\mathbb{F}_i\\) with respect to \\(d_\\alpha^{f_{0,i}}\\) for some \\(\\delta_i \\geq 0\\). Let \\(\\prod_{i=1}^n A_i = \\left\\{\\prod_{i=1}^n f_{i} \\mid f_i \\in \\mathbb{F}_i\\right\\}\\) where \\(\\prod_{i=1}^n f_i\\) is the product density on \\(\\prod_{i=1}^n \\mathcal{X}_i\\) defined by \\((x_1, \\dots, x_n) \\mapsto \\prod_{i=1}^n f_i(x_i)\\). Then \\(\\prod_{i=1}^nA_i\\) is \\(\\left(\\sum_{i=1}^n\\delta_i\\right)\\)-separated from \\(\\prod_{i=1}^n f_{i}^\\star\\) with respect to \\(d_\\alpha^{f_0}\\) where \\(f_0 = \\prod_{i=1}^nf_{0,i}\\).\nWe can now define the separation \\(\\alpha\\)-entropy of a set \\(A\\subset \\mathbb{F}\\) with parameter \\(\\delta &gt; 0\\) as the minimal \\(\\alpha\\)-entropy of a \\(\\delta\\)-separated covering of \\(A\\). When this entropy is finite, we can study the concentration properties of the posterior distribution using simple information-theoretic techniques similar to those used in Bhattacharya (2019) for the study of Bayesian fractional posteriors.\nDefinition (Separation \\(\\alpha\\)-entropy). Fix \\(\\delta &gt; 0\\), \\(\\alpha \\in (0,1)\\) and let \\(A\\) be a subset of \\(\\mathbb{F}\\). Recall \\(\\Pi\\), \\(f_0\\) and \\(f^\\star\\) fixed as previously. The separation \\(\\alpha\\)-entropy of \\(A\\) is defined as \\[ \\mathcal{S}_\\alpha^\\star(A, \\delta) = \\mathcal{S}_\\alpha^\\star(A, \\delta; \\Pi, f_0, f^\\star) = \\inf \\,(1-\\alpha)^{-1} \\log \\sum_{i=1}^\\infty \\left(\\frac{\\Pi(A_i)}{\\Pi(A)}\\right)^\\alpha\\] where the infimum is taken over all (measurable) families \\(\\{A_i\\}_{i=1}^\\infty\\), \\(A_i \\subset \\mathbb{F}\\), satisfying \\(\\Pi(A \\backslash (\\cup_{i}A_i)) = 0\\) and which are \\(\\delta\\)-separated from \\(f_0\\) with respect to the divergence \\(d_\\alpha^{f_0}\\). When no such covering exists we let \\(\\mathcal{S}_\\alpha(A, \\delta) = \\infty\\), and when \\(\\Pi(A) = 0\\) we define \\(\\mathcal{S}_\\alpha(A, \\delta) = 0\\).\nRemark. When \\(f_0 = f^\\star\\), so that \\(d_\\alpha^{f_0}(f, f^\\star) = d_\\alpha(f, f_0)\\), we drop the indicator \\(\\star\\) and denote \\(\\mathcal{S}_\\alpha(A, \\delta) = \\mathcal{S}^\\star(A, \\delta)\\), to emphasize the fact.\nProposition (Properties of the separation \\(\\alpha\\)-entropy). The separation \\(\\alpha\\)-entropy \\(\\mathcal{S}_\\alpha^\\star(A, \\delta)\\) of a set \\(A\\subset \\mathbb{F}\\) is non-negative and \\(\\mathcal{S}_\\alpha(A, \\delta) = 0\\) if \\(A\\) is \\(\\delta\\)-separated from \\(f^\\star\\) with respect to the divergence \\(d_\\alpha^{f_0}\\). Furthermore, if \\(0 &lt; \\alpha \\leq \\beta &lt; 1\\) and \\(0 &lt; \\delta \\leq \\delta'\\), then\n\\[ {}\\mathcal{S}_\\alpha^\\star(A, \\delta) \\leq \\mathcal{S}_\\alpha^\\star(A, \\delta')\\] and if also \\(f^\\star = f_0\\), then\n\\[ {}\\mathcal{S}_\\beta(A, \\tfrac{1-\\beta}{\\beta}\\delta) \\leq \\mathcal{S}_\\alpha(A, \\tfrac{1-\\alpha}{\\alpha}\\delta).\\] For a subset \\(A \\subset B \\subset \\mathbb{F}\\) with \\(\\Pi(A) &gt; 0\\), we have\n\n\n\nimg\n\n\nand, more generally, if \\(A \\subset \\bigcup_{n=1}^\\infty B_n\\) for subsets \\(B_n \\subset \\mathbb{F}\\), then \\[ {}\\Pi(A)^{\\alpha}\\left(\\exp\\mathcal{S}_\\alpha^\\star(A, \\delta)\\right)^{1-\\alpha}         \\leq \\sum_{n=1}^\\infty \\Pi(B_n)^\\alpha \\left(\\exp\\mathcal{S}_\\alpha^\\star(B_n, \\delta)\\right)^{1-\\alpha}.\\]"
  },
  {
    "objectID": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#posterior-consistency",
    "href": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#posterior-consistency",
    "title": "Posterior Concentration in terms of the Separation Alpha-Entropy",
    "section": "Posterior consistency",
    "text": "Posterior consistency\nTheorem (Posterior consistency). Let \\(f_0, f^\\star \\in \\mathbb{F}\\) and let \\(\\{X_i\\}\\) be a sequence of independent random variables with common probability density \\(f_0\\). Suppose there exists \\(\\delta &gt; 0\\) such that \\[ \\Pi\\left({f \\in \\mathbb{F} \\mid D(f_0| f) &lt; \\delta}\\right) &gt; 0.\\] If \\(A \\subset \\mathbb{F}\\) satisfies \\(\\mathcal{S}_\\alpha^\\star\\left(A, \\delta\\right) &lt; \\infty\\) for some \\(\\alpha \\in (0,1)\\), then \\(\\Pi\\left(A\\mid \\{X_i\\}_{i=1}^n\\right) \\rightarrow 0\\) almost surely as \\(n\\rightarrow \\infty\\).\nRemark. The condition \\(\\mathcal{S}_\\alpha^\\star(A, \\delta) &lt; \\infty\\) implies in particular that \\(A \\subset \\{f\\in \\mathbb{F} \\mid d_\\alpha^{f_0}(f, f^\\star) \\geq \\delta\\}\\).\nCorollary (Well-specified consistency). Suppose that \\(f_0\\) is in the Kullback-Leibler support of \\(\\Pi\\). If \\(A \\subset \\mathbb{F}\\) satisfies \\(\\mathcal{S}_\\alpha(A, \\delta) &lt; \\infty\\) for some \\(\\alpha \\in (0,1)\\) and for some \\(\\delta &gt; 0\\), then \\(\\Pi_n(A) \\rightarrow 0\\) almost surely as \\(n \\rightarrow \\infty\\).\nCorollary (Well-specified Hellinger consistency). Suppose that \\(f_0\\) is in the Kullback-Leibler support of \\(\\Pi\\) and fix \\(\\varepsilon &gt; 0\\). If there exists a covering \\(\\{A_i\\}_{i=1}^\\infty\\) of \\(\\mathbb{F}\\) by Hellinger balls of diameter at most \\(\\delta &lt; \\varepsilon\\) satisfying \\(\\sum_{i=1}^\\infty \\Pi(A_i)^\\alpha &lt; \\infty\\) for some \\(\\alpha \\in (0,1)\\), then \\(\\Pi_n\\left(\\left\\{f \\in \\mathbb{F} \\mid H(f, f_0) \\geq \\varepsilon \\right\\}\\right) \\rightarrow 0\\) almost surely as \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#posterior-concentration",
    "href": "pages/posts/2020-11-15-posterior-concentration-in-terms-of-the-separation-alpha-entropy/posterior-concentration-in-terms-of-the-separation-alpha-entropy.html#posterior-concentration",
    "title": "Posterior Concentration in terms of the Separation Alpha-Entropy",
    "section": "Posterior concentration",
    "text": "Posterior concentration\nFollowing Kleijn et al.¬†(2006) and Bhattacharya et al.¬†(2019), we let \\[ B(\\delta, f^\\star;f_0) = \\left\\{ f\\in \\mathbb{F} \\mid \\int \\log \\left(\\frac{f}{f^\\star}\\right) f_0 \\,d\\mu \\leq \\delta,\\, \\int \\left(\\log \\left(\\frac{f}{f^\\star}\\right)\\right)^2 f_0 \\,d\\mu \\leq \\delta \\right\\}\\] be a Kullback-Leibler type neighborhood of \\(f^\\star\\) (relatively to \\(f_0\\)) where the second moment of the log likelihood ratio \\(\\log(f/f^\\star)\\) is also controlled.\nTheorem (Posterior concentration bound). Let \\(f_0, f^\\star \\in \\mathbb{F}\\) and let \\(X \\sim f_0\\). For any \\(\\delta &gt; 0\\) and \\(\\kappa &gt; 1\\) we have that \\[ \\log\\Pi(A \\mid X) \\leq \\frac{1-\\alpha}{\\alpha}\\mathcal{S}_\\alpha^\\star(A, \\kappa \\delta)- \\log\\Pi(B(\\delta, f^\\star;f_0)) - \\kappa\\delta\\] holds with probability at least \\(1-8/(\\alpha^2\\delta)\\).\nCorollary (Posterior concentration bound, i.i.d. case). Let \\(f_0, f^\\star \\in \\mathbb{F}\\) and let \\(\\{X_i\\}\\) be a sequence of independent random variables with common probability density \\(f_0\\). For any \\(\\delta &gt; 0\\) and \\(\\kappa &gt; 1\\) we have that \\[ \\log\\Pi\\left(A \\mid \\{X_i\\}_{i=1}^n\\right) \\leq \\frac{1-\\alpha}{\\alpha}\\mathcal{S}_\\alpha^\\star(A, \\kappa \\delta)- \\log\\Pi(B(\\delta, f^\\star;f_0)) - n\\kappa\\delta\\] holds with probability at least \\(1-8/(\\alpha^2 n \\delta)\\).\nReferences\n\nBhattacharya, A., D. Pati, and Y. Yang (2019).Bayesian fractional posteriors.Ann.Statist. 47(1), 39‚Äì66.\nChoi, T. and R. V. Ramamoorthi (2008).Remarks on consistency of posterior distributions,Volume Volume 3, pp.¬†170‚Äì186. Beachwood, Ohio, USA: Institute of Mathematical Statistics.\nDe Blasi, P. and S. G. Walker (2013). Bayesian asymptotics with misspecified models.StatisticaSinica, 169‚Äì187.\nGr√ºnwald, P. and T. van Ommen (2017). Inconsistency of bayesian inference for misspecifiedlinear models, and a proposal for repairing it.Bayesian Anal. 12(4), 1069‚Äì1103.\nKleijn, B. J., A. W. van der Vaart, et al.¬†(2006). Misspecification in infinite-dimensional bayesianstatistics.The Annals of Statistics 34(2), 837‚Äì877.\nRamamoorthi, R. V., K. Sriram, and R. Martin (2015). On posterior concentration in misspec-ified models.Bayesian Anal. 10(4), 759‚Äì789.\nWalker, S. (2004). New approaches to Bayesian consistency.Ann. Statist. 32(5), 2028‚Äì2043.\nXing, Y. and B. Ranneby (2009). Sufficient conditions for Bayesian consistency. J. Stat. Plan.Inference 139(7), 2479‚Äì2489."
  },
  {
    "objectID": "pages/posts/2020-11-15-theory-of-gibbs-posterior-concentration/theory-of-gibbs-posterior-concentration.html",
    "href": "pages/posts/2020-11-15-theory-of-gibbs-posterior-concentration/theory-of-gibbs-posterior-concentration.html",
    "title": "Theory of Gibbs posterior concentration",
    "section": "",
    "text": "Consider the statistical learning framework where we have data \\(X\\sim Q\\) for some unknown distribution \\(Q\\), a model \\(\\Theta\\) and a loss function \\(\\ell_\\theta(X)\\) measuring a cost associated with fitting the data \\(X\\) using a particular \\(\\theta\\in\\Theta\\). Our goal is to use the data to learn about parameters which minimize the risk \\(R(\\theta) = \\mathbb{E}[\\ell_\\theta(X)]\\). Here are two standard examples.\nDensity estimation. Suppose we observe independent random variables \\(X_1, X_2, \\dots, X_n\\). Here the model \\(\\Theta\\) parametrizes a set \\(\\mathcal{M} = \\{p_\\theta : \\theta \\in \\Theta \\}\\) of probability density functions (with respect to some dominating measure on the sample space), and our loss for \\(X = (X_1, \\dots, X_n)\\) is defined as \\[\n\\ell_\\theta(X) = - \\sum_{i=1}^n \\log p_\\theta(X_i).\n\\] If, for instance, the variables \\(X_i\\) are independent with common distribution with density function \\(p_{\\theta_0}\\) for some \\(\\theta_0 \\in \\mathbb{\\Theta}\\), then it follows from the positivity of the Kullback-Leibler divergence that \\(\\theta_0 \\in \\arg\\min _ \\theta \\mathbb{E}[\\ell _ \\theta(X)]\\). That is, under identifiability conditions, our learning target is the true data-generating distribution.\nIf the model is misspecified, roughly meaning that there is no \\(\\theta_0\\in \\Theta\\) such that \\(p_{\\theta_0}\\) is a density of \\(X_i\\), then our framework sets up the learning problem to be about the parameter \\(\\theta_0\\) which is such that \\(p_{\\theta_0}\\) mininizes the Kullback-Leibler divergence between \\(p_{\\theta_0}\\) and the true marginal distribution of the \\(X_i\\)‚Äôs.\nRegression. Here our observations take the form \\((Y_i, X_i)\\), the model \\(\\Theta\\) parameterizes regression functions \\(f_\\theta\\) and we can consider a sum of squared errors loss \\[\n\\ell_\\theta(X) = \\sum_{i=1}^n(Y_i - f_\\theta(X_i))^2.\n\\]\n\nGibbs posterior distributions\nGibbs Learning approaches this problem from a pseudo Bayesian point of view. While typically a Bayesian approach would require the specification of a full data-generating model, here we replace the likelihood function by the pseudo-likelihood function \\(\\theta \\mapsto e^{-\\ell_\\theta(X)}\\). Given a prior \\(\\pi\\) on \\(\\Theta\\), the Gibbs posterior distribution is then given by \\[\n\\pi(\\theta \\mid X) \\propto e^{-\\ell_\\theta(X)} \\pi(\\theta)\n\\] and satisfies \\[\n\\pi(\\cdot \\mid X) \\in \\text{argmin}_{\\hat \\pi} \\left\\{ \\mathbb{E}_{\\theta \\sim \\hat \\pi}[\\ell_\\theta(X)] + D_{\\text{KL}}(\\hat \\pi \\mid \\pi) \\right\\}\n\\] whenever these expressions are well defined.\nIn the context of integrable pseudo-likelihoods, the above can be re-interpreted as a regular posterior distributions built from density functions \\(f _ \\theta(x) \\propto e^{-\\ell _ \\theta(x)}\\) and with a prior \\(\\tilde \\pi\\) satisfying \\[\n\\frac{d\\tilde \\pi}{d\\pi}(\\theta) \\propto \\int e^{-\\ell_\\theta(x)}\\,dx =: c(\\theta).\n\\] However, the reason we cannot apply standard asymptotic theory to the analysis of Gibbs posterior is that the quantity \\(c(\\theta)\\) will typically be sample-size dependent. That is, if \\(X=X^n=(X_1, X_2, \\dots, X_n)\\) for i.i.d. random variables \\(X_i\\) and if the loss \\(\\ell_\\theta\\) separates as the sum \\[\n\\ell_\\theta(X) = \\sum_{i=1}^nl_{\\theta}(X_i),\n\\] then \\(c(\\theta) = \\left(\\int e^{-l_\\theta(x_1)} \\, dx_1\\right)^n\\). This data-dependent prior, tilting \\(\\pi\\) by the function \\(c(\\theta)^n\\), is what allows Gibbs learning to target general risk-minimizing parameters rather than likelihood Kullback-Leibler minimizers.\nSome of my ongoing research, presented as a poster at the O‚ÄôBayes conference in Warwick last summer, focused on understand the theoretical behaviour of Gibbs posterior distributions. I studied the posterior convergence and finite sample concentration properties of Gibbs posterior distributions under the large sample regime with additive losses \\(\\ell_\\theta^{(n)}(X_1, \\dots, X_n) = \\sum_{i=1}^n\\ell_\\theta(X_i)\\). I‚Äôve attached the poster (joint work with Yu Luo) below and you can find the additional references here.\nNote that this is very preliminary work. We‚Äôre still in the process of exploring interesting directions (and I have very limited time this semester with the beginning of my PhD at Duke).\n\n\n\n\n\nReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2022-08-07-potential-of-privacy-preserving-record-linkage-for-the-statistics-of-hidden-populations/potential-of-privacy-preserving-record-linkage-for-the-statistics-of-hidden-populations.html",
    "href": "pages/posts/2022-08-07-potential-of-privacy-preserving-record-linkage-for-the-statistics-of-hidden-populations/potential-of-privacy-preserving-record-linkage-for-the-statistics-of-hidden-populations.html",
    "title": "Potential of Privacy-Preserving Record Linkage for the Statistics of Hidden Population",
    "section": "",
    "text": "ReuseCC BY 4.0CopyrightOlivier Binette"
  },
  {
    "objectID": "pages/posts/2020-11-15-tubular-neighborhoods/tubular-neighborhoods.html",
    "href": "pages/posts/2020-11-15-tubular-neighborhoods/tubular-neighborhoods.html",
    "title": "Tubular neighborhoods",
    "section": "",
    "text": "Let me introduce some notations. A \\(\\mathcal{C}^l\\) submanifold of dimension \\(m\\) of \\(\\mathbb{R}^k\\) is a subset \\(M\\) that is locally \\(\\mathcal{C}^l\\)-diffeomorphic to open balls of \\(\\mathbb{R}^m\\). Similarily, a \\(\\mathcal{C}^l\\) manifold with boundary is locally diffeomorphic to open balls of the half space \\(\\mathbb{H}^k = \\{(x_1, \\dots, x_m)\\in \\mathbb{R}^m | x_m \\geq 0\\}\\). If \\(f : M \\rightarrow N\\) is a differentiable map between manifolds, we denote by \\(df_x: T_x M \\rightarrow T_{f(x)} N\\) the differential of \\(f\\) at \\(x\\). Each tangent space \\(T_x M\\) has an orthogonal complement \\(N_x M\\) in \\(\\mathbb{R}^k\\); the normal bundle of \\(M\\) is \\(N(M) = \\{(x, v) \\in \\mathbb{R}^{2k} | x \\in M,\\, v \\in N_x M\\}\\). In the following, we assume \\(M\\) is compact.\nGiven \\(\\varepsilon \\gt; 0\\), we let \\(V_\\varepsilon = \\{(x, v) \\in N(M) \\,|\\, |v| \\le \\varepsilon\\}\\) and \\(P_\\varepsilon = \\{y \\in \\mathbb{R}^k | d(y, M) \\le \\varepsilon \\}\\), where \\(d\\) is the euclidean distance. The set \\(V_\\varepsilon\\) is an \\(\\varepsilon\\)-neighborhood of the cross-section \\(M \\times \\{0\\}\\) in \\(N(M)\\), and \\(P_\\varepsilon\\) is a tubular neighborhood of \\(M\\) in \\(\\mathbb{R}^k\\). We will prove the following theorem.\nTubular neighborhood theorem. Let \\(M\\) be a compact submanifold of \\(\\mathbb{R}^k\\), without boundary. For \\(\\varepsilon \\gt; 0\\) sufficiently small, \\(V_\\varepsilon\\) and \\(P_\\varepsilon\\) are manifolds, diffeomorphic under the map \\(F : V_\\varepsilon \\rightarrow P_\\varepsilon : (x, v) \\mapsto x+v\\).\nCorollary 1. If \\(\\varepsilon \\gt; 0\\) is sufficiently small, then for each \\(w \\in P_\\varepsilon\\) there exists an unique closest point to \\(w\\) on \\(M\\).\nNote, however, that this corollary may not hold when \\(M\\) is only a \\(\\mathcal{C}^1\\) manifold. We will require \\(M\\) to be at least \\(\\mathcal{C}^2\\). The proof will make clear why this is necessary, but I also present a counter-example.\nCounterexample (\\(\\mathcal{C}^1\\) manifolds). Let \\(M\\) be the graph of \\(f: [-1,1] \\rightarrow \\mathbb{R} : t \\mapsto t^{4/3}\\) in \\(\\mathbb{R}^2\\). It is indeed a compact \\(\\mathcal{C}^1\\) manifold since \\(f\\) is \\(\\mathcal{C}^1\\). However, the points \\(w_\\varepsilon = (0, \\varepsilon)\\) have, for all \\(\\varepsilon \\gt; 0\\), two closest points on \\(M\\). To see this, first note that if \\(M\\) had an unique closest point to \\(w_\\varepsilon\\), then that point would be \\((0,0)\\), by the parity of \\(f\\). Now, consider the function \\(g_\\varepsilon :[-\\varepsilon, \\varepsilon] \\rightarrow \\mathbb{R}: t \\mapsto \\varepsilon - \\sqrt{\\varepsilon^2 - t^2 }\\), its graph being the lower half of a circle centered at \\(w_\\varepsilon\\) and crossing \\((0,0)\\). We find \\[\\lim_{t \\rightarrow 0} g_\\varepsilon'(t)/f'(t) = \\lim_{t \\rightarrow 0} \\frac{3}{4}\\frac{t^{2/3}}{\\sqrt{\\varepsilon^2 - t^2}} = 0,\\] meaning that the graph of \\(g_\\varepsilon\\) is under \\(M\\) near \\((0,0)\\). This is a contradiction, as me may thus shrink the circle to find two intersection points on \\(M\\) closer to \\(w_\\varepsilon\\) than \\((0,0)\\)."
  },
  {
    "objectID": "pages/posts/2020-11-15-tubular-neighborhoods/tubular-neighborhoods.html#proof-of-the-theorem.",
    "href": "pages/posts/2020-11-15-tubular-neighborhoods/tubular-neighborhoods.html#proof-of-the-theorem.",
    "title": "Tubular neighborhoods",
    "section": "Proof of the theorem.",
    "text": "Proof of the theorem.\nIn the following, \\(M\\) is a compact \\(\\mathcal{C}^2\\) submanifold of \\(\\mathbb{R}^k\\) of dimension \\(m\\).\nLemma 1. The normal bundle \\(N(M)\\) is a \\(\\mathcal{C}^1\\) submanifold of \\(\\mathbb{R}^{2k}\\) and \\(T_{(x,v)} N(M) = T_xM \\times N_x M\\).\nProof. Let \\((x_0, 0) \\in N(M)\\) and consider a neighborhood \\(\\mathcal{U}\\) of \\(x_0\\) in \\(\\mathbb{R}^k\\). It may be chosen so that \\(M\\cap \\mathcal{U} = \\phi^{-1}(0)\\), for some \\(\\phi : \\mathcal{U} \\rightarrow \\mathbb{R}^{k-m}\\) with \\(d\\phi_x\\) surjective. Restricting \\(\\mathcal{U}\\) some more, we can find a \\(\\mathcal{C}^2\\) diffeomorphism \\(\\psi : \\mathbb{R}^m \\rightarrow M\\cap \\mathcal{U}\\). Using \\(\\phi\\) and and \\(\\psi\\), we construct a \\(\\mathcal{C}^1\\) map \\(f : \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^k\\) having \\(0\\) as a regular value and such that \\(N(M\\cap \\mathcal{U}) = f^{-1}(0)\\). It will follow from the preimage theorem that \\(N(M \\cap \\mathcal{U})\\) is a \\(\\mathcal{C}^1\\) submanifold of dimension \\(k\\). Furthermore, \\(N(M \\cap \\mathcal{U})\\) is an open neighborhood of \\((x_0, v)\\) for all \\(v \\in T_{x_0}M\\) and we will have found that \\(N(M)\\) is a \\(\\mathcal{C}^1\\) manifold.\nThe map \\(f\\) is defined as \\(f: \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^k\\), \\(f(x, v) = \\left(\\phi(x), u(x, v)\\right)\\), where \\(u : \\mathbb{R}^{2k} \\rightarrow \\mathbb{R}^m : (x,v) \\mapsto (\\langle v, d\\psi_{\\psi^{-1}(x)}(e_1) \\rangle, \\dots, \\langle v, d\\psi_{\\psi^{-1}(x)}(e_m) \\rangle)\\) and \\((e_i)\\) is a basis of \\(\\mathbb{R}^m\\). Because the vectors \\(d\\psi_{\\psi^{-1}(x)}(e_i)\\) form a basis of \\(T_x M\\), the zero set \\(u^{-1}(0)\\) is precisely \\(N_x M\\) and we find that \\(f^{-1}(0) = M \\cap \\mathcal{U}\\). To differentiate \\(f\\), we use the fact that \\(\\psi\\) is \\(\\mathcal{C}^2\\). In its matrix form,\n\\[\n  df_{(x,v)} = \\left[\\begin{array}{cc}d\\phi_x& 0\\\\ *& \\partial_2 u_{(x,v)}\\end{array}\\right]\n\\]\nwhere both \\(d\\phi_x\\) and \\(\\partial_2 u_(x,v) = u(x, \\cdot)\\) are surjective whenever \\(x \\in M\\). Thus \\(df_{(x,v)}\\) is indeed surjective for all \\((x,v) \\in f^{-1}(0)\\). The assertion \\(T_{(x,v)} N(M) = T_xM \\times N_x M\\) follows from \\(T_{(x,v)} N(M) = ker(df_{(x,v)}\\).¬† QED.\nLemma 2. For all \\(\\varepsilon \\gt; 0\\), \\(V_\\varepsilon \\subset N(M)\\) is a submanifold with boundary.\nProof. Let \\(f : N(M) \\rightarrow \\mathbb{R} : (x,v) \\mapsto ||v||^2\\). For any \\((x, v) \\in f^{-1}(\\varepsilon^2)\\), we have \\(df_{(x,v)}: T_x M \\times N_x M \\rightarrow \\mathbb{R} : (y, u) \\mapsto 2\\langle u, v \\rangle\\) is surjective. By the preimage theorem, we find that \\(f^{-1}((-\\infty, \\varepsilon^2])\\) is a submanifold of \\(N(M)\\) with boundary \\(f^{-1}(\\varepsilon^2)\\). QED.\nLemma 3. The map \\(F: V_\\varepsilon \\rightarrow \\mathbb{R}^k\\) is a local diffeomorphism onto its image \\(N_\\varepsilon\\).\nProof. The differential of \\(F\\) is simply \\(dF_{(x,v)} : T_xM \\times N_xM \\rightarrow \\mathbb{R}^k : (a, b) \\mapsto a+b\\), an isomorphism since \\(\\mathbb{R}^k\\) is the direct sum of \\(T_xM\\) and \\(N_xM\\). By the inverse function theorem, it follows that \\(F\\) is a local diffeomorphism onto its image. Now, it is clear that \\(F(V_\\varepsilon) \\subset N_\\varepsilon\\). If \\(w \\in N_\\varepsilon\\), then by compacity of \\(M\\) we can find a closest point \\(x \\in M\\). It is straightforward to verify that \\(w-x \\in N_xM\\), and thus \\((x, w-x)\\in V_\\varepsilon\\), \\(F(x, w-x) = w\\). QED.\nLemma 4 (See Spivak, 1970). If \\(\\varepsilon \\gt; 0\\) is taken sufficiently small, then \\(F : V_\\varepsilon \\rightarrow N_\\varepsilon : (x, v) \\mapsto x+v\\) is a diffeomorphism.\nProof. It suffices to show that \\(F\\) is bijective, whenever \\(\\varepsilon \\gt; 0\\) is sufficiently small. The local diffeomorphism will then be a global diffeomorphism. Note that \\(F\\) is injective on \\(M\\times \\{0\\}\\). Let \\(A = \\{(a, b) \\in N(M)^2 | a \\not = b,\\, F(a) = F(b)\\}\\) be the set of points showcasing the non-injectivity of \\(F\\). This set is disjoint from the compact set \\((M \\times \\{0\\})^2\\). Therefore, if we can show that \\(A\\) is closed, we will find \\(d(A, (M \\times \\{0\\})^2) \\gt; 0\\) and taking \\(\\varepsilon \\lt; d(A, (M \\times \\{0\\})^2)\\) will suffice. Let \\(\\{(a_n, b_n)\\}\\) be a sequence of points in \\(A\\) converging to some \\((a,b)\\). By continuity of \\(F\\), we must have \\(F(a) = F(b)\\). We cannot have \\(a = b\\), as this would contradict the fact that \\(F\\) is a local diffeomorphism. Therefore \\(a \\not = b\\) and \\((a,b) \\in A\\). QED.\nReferences:\nMilnor, J.W. (1965) Topology from a differentiable point of view. Spivak, M. (1970) A comprehensive introduction to differential geometry."
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html",
    "title": "Intro to Hyperparameter Optimization for Machine Learning",
    "section": "",
    "text": "Machine learning is easy, right? You pick a model, fit it to your data, and out come predictions.\n\nNot quite. Sometimes we talk about the fancy math and algorithms under the hood to make it look serious, but we rarely talk about how difficult it is to transform whatever data can gather into useful, actionable predictions that have business value.\nThere are many challenges. First, there‚Äôs the transformation of a business problem into something that‚Äôs remotely approachable by machine learning and statistics. Second, there‚Äôs the development of a data collection plan or, more often than not, the identification of observational data which is already available. With the collection of this data comes the third step, modeling, which bridges between numbers and useful answers. Modeling may have to account for all kinds of issue with your data, such as class imbalance, missingness, and non-representativeness. You also want to obtain good answers, so throughout this step you loop between model specification, evaluation, and refinement. It is a lengthy process of research and investigation into the performance of your model, insights into the why of what you observe, and various fixes and improvements to your model. Finally, in a fourth stage, you must account for how your model will be used and the management of its lifecycle.\n\nMoral of the story: there is a lot work involved. We need all hands on deck. And even more than that, we need robust automatization tools to support this machine learning workflow.\nThis blog post is about a single set of tools ‚Äì hyperparameter optimization techniques ‚Äì used to help with the model specification, evaluation, and refinement loop. I will focus on the standard machine learning framework of supervised learning. In this context, machine learning algorithms can be seen as black boxes which take in some data, a bunch of tuning hyperparameters specified by the user of the algorithm, and which output predictions. The quality of the predictions can be evaluated through data splitting or cross-validation. That is, we‚Äôre always able to compare predictions to ground truth for the data we have at hand.\nMy goal is to describe key approaches to hyperparameter optimization (see Table 1) in order to provide conceptual understanding that can be helpful practice. I describe black-box methods which treat the machine learning algorithm as, well, a black blox. This includes grid search, randomized search, and sequential model-based optimization such as Bayesian optimization. There are additional methods to be considered, such as Hyperband and Bayesian model selection, which integrate with the learning algorithms themselves. These will be for another blog post.\n\nTable 1: Different types of hyperparameter optimization methods\n\n\nBlack-box methods\nIntegrated methods\n\n\n\n\nGrid Search\nHyperband\n\n\nRandomized Search\nBayesian Model Selection\n\n\nSequential Model-Based Optimization\n\n\n\n\n\nBefore getting into the detail of these methods though, let‚Äôs go over some basic concepts and terminology which I‚Äôll be using."
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#introduction",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#introduction",
    "title": "Intro to Hyperparameter Optimization for Machine Learning",
    "section": "",
    "text": "Machine learning is easy, right? You pick a model, fit it to your data, and out come predictions.\n\nNot quite. Sometimes we talk about the fancy math and algorithms under the hood to make it look serious, but we rarely talk about how difficult it is to transform whatever data can gather into useful, actionable predictions that have business value.\nThere are many challenges. First, there‚Äôs the transformation of a business problem into something that‚Äôs remotely approachable by machine learning and statistics. Second, there‚Äôs the development of a data collection plan or, more often than not, the identification of observational data which is already available. With the collection of this data comes the third step, modeling, which bridges between numbers and useful answers. Modeling may have to account for all kinds of issue with your data, such as class imbalance, missingness, and non-representativeness. You also want to obtain good answers, so throughout this step you loop between model specification, evaluation, and refinement. It is a lengthy process of research and investigation into the performance of your model, insights into the why of what you observe, and various fixes and improvements to your model. Finally, in a fourth stage, you must account for how your model will be used and the management of its lifecycle.\n\nMoral of the story: there is a lot work involved. We need all hands on deck. And even more than that, we need robust automatization tools to support this machine learning workflow.\nThis blog post is about a single set of tools ‚Äì hyperparameter optimization techniques ‚Äì used to help with the model specification, evaluation, and refinement loop. I will focus on the standard machine learning framework of supervised learning. In this context, machine learning algorithms can be seen as black boxes which take in some data, a bunch of tuning hyperparameters specified by the user of the algorithm, and which output predictions. The quality of the predictions can be evaluated through data splitting or cross-validation. That is, we‚Äôre always able to compare predictions to ground truth for the data we have at hand.\nMy goal is to describe key approaches to hyperparameter optimization (see Table 1) in order to provide conceptual understanding that can be helpful practice. I describe black-box methods which treat the machine learning algorithm as, well, a black blox. This includes grid search, randomized search, and sequential model-based optimization such as Bayesian optimization. There are additional methods to be considered, such as Hyperband and Bayesian model selection, which integrate with the learning algorithms themselves. These will be for another blog post.\n\nTable 1: Different types of hyperparameter optimization methods\n\n\nBlack-box methods\nIntegrated methods\n\n\n\n\nGrid Search\nHyperband\n\n\nRandomized Search\nBayesian Model Selection\n\n\nSequential Model-Based Optimization\n\n\n\n\n\nBefore getting into the detail of these methods though, let‚Äôs go over some basic concepts and terminology which I‚Äôll be using."
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#background-and-terminology",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#background-and-terminology",
    "title": "Intro to Hyperparameter Optimization for Machine Learning",
    "section": "1.1 Background and Terminology",
    "text": "1.1 Background and Terminology\nFirst, let‚Äôs talk about models, parameters and performance evaluation. This is going to be the occasion for me to introduce some terminology and notations.\n\nModels, Parameters and Performance Evaluation\nA model is a mathematical representation of something going on in the real world. For instance, suppose you want to predict whether or not a given stock \\(X\\) is going to go up tomorrow. A model for this could be: predict it‚Äôs going to go up with probability \\(\\alpha\\) if it went up today, otherwise predict it‚Äôs not going to go up with probability \\(\\beta\\). There‚Äôs only one variable in this model (whether or not the stock went up today), and there are two parameters, the probabilities \\(\\alpha\\) and \\(\\beta\\). Here the parameters could be learned if we had historical data.\nYou could consider more sophisticated models such as classical time series models or reccurent neural networks. In all cases, you have variables (the input to your model), parameters (what you learn from data), and you end up with predictions.\nYou can compare the performance of any model by comparing the predictions to what actually happened. For instance, you could look at how often your predictions were right. That‚Äôs a performance evaluation metric. Your goal is usually to build a model which will keep on performing well.\nFormally, let \\(R\\) be the (average) future performance of your model. You don‚Äôt know this quantity, but you can estimate it as \\(\\hat R\\) using techniques such as cross-validation and its variants. There might be a bias and a variance to \\(\\hat R\\), but the best we can do in practice is to try to find the model with the best estimated performance (modulo certain adjustments).\nThis brings us to the question: how should you choose a model? The standard in machine learning is to choose a model which maximizes \\(\\hat R\\). It‚Äôs not the only solution, and it‚Äôs not always the best solution (it can be better to do model averaging if \\(\\hat R\\) has some variance), but it‚Äôs what we‚Äôll focus on through this blog post.\nFurthermore, we‚Äôll approach this problem through the lens of hyperparameter selection.\n\n\nHyperparameters\nHyperparameters are things that have you have to specify before you can run a model, such as:\n\nwhat data features to use,\nwhat type of model to use (linear model? random forest? neural network?)\nother decisions that go into the specification of a model:\n\nthe number of layers in your neural network,\nthe learning rate for the gradient descent algorithm,\nthe maximum depth for decision trees, etc.\n\n\nThere is only a practical distinction between parameters and hyperparameters. Hyperparameters are things that are usually set separately from the other model parameters, or that do not nicely fit within a model‚Äôs learning algorithm. Depending on the framework you‚Äôre using, parameters can become hyperparameters and vice versa. For example, by using ensemble methods, you could easily transform the ‚Äúmodel type choice‚Äù hyperparameter to a simple parameter of your ensemble that is learned from data.\nThe key thing is that, in practice, there will typically be some distinction between parameters of your model and a set of hyperparameters that you have to specify.\nThrough experience, you can learn what hyperparameters work well for the kinds of problems that you work on. Other times, you might carefully tune parameters and investigate the impact of your choices on model performance.\nThe manual process of hyperparameter tuning can lead to important insights into the performance and behavior of your model. However, it can also be a menial task that would be better automated through hyperparameter optimization algorithms aiming to maximize \\(\\hat R\\), such as those that I review below.\n\n\nExample\nLet‚Äôs look at an example to make things concrete. This is adapted from scikit-optimize‚Äôs tutorial for tuning scikit-learn estimators.\nWe‚Äôll consider the California housing dataset from the scikit-learn library. Each row in this dataset represents a census block and contains aggregated information regarding houses in that block. Our goal will be to predict median house price at the block level given these other covariates.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\n\ndataset = fetch_california_housing(as_frame=True)\n\nX = dataset.data # Covariates\nn_features = X.shape[1] # Number of features\ny = dataset.target # Median house prices\n\nX\n\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n\n\n\n\n20640 rows √ó 8 columns\n\n\n\nFor the regression, we‚Äôll use scikit-learn‚Äôs gradient boosted trees estimator. This model has a number of internal parameters which don‚Äôt need to know much about, as well as hyperparameters which can be used to tune the model. This includes the max_depth hyperparameter for the maximum depth of decision trees, learning_rate for the learning rate of gradient boosting, max_features for the maximum number of features to use in each decision trees, and a few more. Ranges of reasonable values for these parameters are specified in the space variable below.\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom skopt.space import Real, Integer\n\nmodel = GradientBoostingRegressor(n_estimators=25, random_state=0)\n\nspace  = [Integer(1, 8, name='max_depth'),\n          Real(0.01, 1, \"log-uniform\", name='learning_rate'),\n          Integer(1, n_features, name='max_features'),\n          Integer(1, 50, name='min_samples_leaf')\n]\n\n\nNow, the last thing we need is an estimator \\(\\hat R\\) for the model‚Äôs performance. This is our Rhat() function (i.e.¬†\\(\\hat R\\)) which we‚Äôll try to maximize. Here we use a cross-validated mean absolute error score.\n\n\nCode\nfrom sklearn.model_selection import cross_val_score\n\ndef Rhat(**params):\n  model.set_params(**params)\n  \n  return -np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,\n                                  scoring=\"neg_mean_absolute_error\"))\n\n\nWith this, we can fit the model to the data (using default hyperparameter values to begin with), and evaluate the model‚Äôs performance.\n\n\nCode\nmodel.fit(X, y)\n\nRhat()\n\n\nnp.float64(0.5503720160635011)\n\n\nHere the unit for median house price was in hundreds of thousands of dollars and we can interpret the model performance at this scale. The value \\(\\hat R \\approx 0.55\\) means that, on average, the absolute error of the model is $55,000. We‚Äôll see if we can do better using hyperparameter optimization."
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#black-box-optimization-methods",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#black-box-optimization-methods",
    "title": "Intro to Hyperparameter Optimization for Machine Learning",
    "section": "2 Black-Box Optimization Methods",
    "text": "2 Black-Box Optimization Methods\nBlack-box hyperparameter optimization algorithms consider the underlying machine algorithm as unknown. We only assume that, given a set of hyperparameters \\(\\lambda\\), we can compute the estimated model performance \\(\\hat R(\\lambda)\\). There is usually variance in \\(\\hat R(\\lambda)\\), but this is not something that I will talk about in this post. We will therefore consider \\(\\hat R\\) as a deterministic function to be optimized.\nNote: in practice, you need to account for the variance in \\(\\hat R\\), as otherwise you could get bad surprises. It‚Äôs just not something I‚Äôm covering in this post, since I want to focus on a conceptual understanding of the optimization algorithms.\nWe can use almost any technique to try to optimize \\(\\hat R\\), but there are a number of challenges with hyperparameter optimization:\n\n\\(\\hat R\\) is usually rather costly to evaluate.\nWe usually do not have gradient information regarding \\(\\hat R\\) (otherwise, hyperparameters for which we have gradient information could easily be incorporated as parameters of the underlying ML algorithms).\nThe hyperparameter space is usually complex. It can contain discrete variables and can even be tree-structured, where some hyperparameters are only defined conditionally on other hyperparameters.\nThe hyperparameter space is usually somewhat high-dimensional, with more than just 2-3 dimensions.\n\nThese particularities of the hyperparameter optimization problem has led the machine learning community to favor some of the optimization techniques which I discuss below.\n\n2.1 Grid Search\nThe first technique to consider is grid search, which is a brute force approach to hyperparameter optimization. It is the simplest of all ‚Äì you simply specify values to consider for each hyperparameter, and then evaluate your model performance for each combination of hyperparameter. At the end, you keep the hyperparameter configuration which performed best.\nThere are a few advantages to this approach:\n\nIt gives you precise control over what hyperparameter configurations are evaluated.\nIt is simple to implement and easily parallelizable.\n\nHowever, there are also a number of serious drawbacks:\n\nThe runtime scales exponentially in the number of hyperparameter dimensions.\nThe runtime is tied to the hyperparameter search space which you specify. To reduce runtime, you need to manually redefine this space.\n\nLet‚Äôs see an example of how this works in practice. First, we define a grid of hyperparameter values to evaluate. Given the scoring function \\(\\hat R\\), we can then use scikit-learn‚Äôs GridSearchCV() function to evaluate the model performance at each hyperparameter combination. This is done below:\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Budget of 54 evaluations\ngrid = {\n  'max_depth': [1, 3, 5],\n  'learning_rate': [0.01, 0.1, 1],\n  'max_features': [2, 4, 8],\n  'min_samples_leaf': [1, 10]\n}\n\ndef scoring(estimator, X_test, y_test):\n  y_pred = estimator.predict(X_test)\n  return -np.mean(np.abs(y_test - y_pred))\n\nresults = GridSearchCV(model, grid, cv=3, n_jobs=-1, scoring=scoring).fit(X, y)\n\n\nWe can then recover the best score and best hyperparameters. The best model is slightly better than the default model we looked at earlier, with a $4,000 decrease in average absolute error.\n\n\nCode\n-results.best_score_ # Lowest cross-validated mean absolute error\n\n{key:results.best_params_[key] for key in grid.keys()} # Best parameters\n\n\n{'max_depth': 5,\n 'learning_rate': 0.1,\n 'max_features': 8,\n 'min_samples_leaf': 1}\n\n\nIt is also informative to plot an histogram for the distribution of model scores. We can see that most model configurations performed much worst than the default.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.clf()\np = plt.hist(-results.cv_results_[\"mean_test_score\"], bins=20)\np = plt.title(\"Score distribution for evaluated hyperparameters\", loc=\"left\")\np = plt.xlabel(\"Cross-validated average absolute error\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.2 Random Search\nThe second method we‚Äôll look at is random search. Here, the idea is to sample a number \\(k\\) of hyperparameter configurations at random from a given space, and to evaluate those random configurations.\nThis might seem like a silly idea. Why pick hyperparameter values at random?\nThe answer is that doing so removes all computational penalties from the consideration of useless hyperparameter dimensions. That is, imagine that a number \\(s\\) of your hyperparameters have actually no impact on model performance. With grid search, the consideration of these hyperparameters would incur you a computational penalty which is exponential in \\(s\\). With random search, however, there is no penalty at all for adding these \\(s\\) additional hyperparameter dimensions. The results from random search with or without these additional dimensions are exactly the same in both cases.\nThis is the huge advantage of random search over grid search: you do not get penalized for useless dimensions. Furthermore, in practice, being able to tune the search effort through the number of samples \\(k\\) can be quite convenient.\nLet‚Äôs see how this can be implemented in practice. We‚Äôll define a hyperparameter space which is similar to the grid space we specified earlier, but which is filled in with additional possible values. We can then run scikit-learn‚Äôs RandomizedSearchCV() function to do the randomized search:\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import loguniform\n\n# Around roughly the same values as for the grid search\nparam_distribution = {\n  'max_depth': range(1, 8),\n  'learning_rate': loguniform(0.01, 1),\n  'max_features': range(1, 9),\n  'min_samples_leaf': range(1, 50)\n}\n\n# Budget of 54 evaluations\nresults = RandomizedSearchCV(model, param_distribution, n_iter=54, cv=3, n_jobs=-1, scoring=scoring, random_state=0).fit(X, y)\n\n\nThe results are below. By considering a richer hyperparameter space, and without being penalized by this in the same way we would with a grid search, randomized search allows us to find a better model with the same amount of effort.\n\n\nCode\n-results.best_score_ # Lowest cross-validated mean absolute error\n\n{key:results.best_params_[key] for key in grid.keys()} # Best parameters\n\n\n{'max_depth': 6,\n 'learning_rate': np.float64(0.1453937524243155),\n 'max_features': 7,\n 'min_samples_leaf': 26}\n\n\nAgain, we can look at the distribution of model performance for sampled hyperparameter configurations. It‚Äôs quite similar to grid search, with only a few better-performing models being identified.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.clf()\np = plt.hist(-results.cv_results_[\"mean_test_score\"], bins=20)\np = plt.title(\"Score distribution for evaluated hyperparameters\", loc=\"left\")\np = plt.xlabel(\"Cross-validated average absolute error\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.3 Sequential Model-Based Optimization\nAll of the techniques considered so far made no assumption at all about the function \\(\\hat R\\) to optimize.\nThis is a problem, because we do have prior information about \\(\\hat R\\). We can expect \\(\\hat R\\) to have some level of regularity, meaning that similar hyperparameter configurations should have similar performance. This knowledge allows us to make inference about \\(\\hat R(\\lambda)\\) given the evaluation of \\(\\hat R\\) at other points \\(\\tilde \\lambda \\not = \\lambda\\).\nMore formally, suppose we have evaluated \\(\\hat R\\) at a sequence of hyperparameter configurations \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\), thus observing \\(\\hat R(\\lambda_1), \\hat R(\\lambda_2), \\dots, \\hat R(\\lambda_n)\\). This allows us to make inference about \\(\\hat R\\). In particular, we can try guessing what next \\(\\lambda_{n+1}\\) will maximize \\(\\hat R\\) or improve our knowledge of \\(\\hat R\\). Once we‚Äôve observed \\(\\hat R(\\lambda_{n+1})\\), we repeat the process, trying to guess which \\(\\lambda_{n+2}\\) to pick to improve the procedure. That is the entire idea behind sequential model-based optimization.\nTo make this work in practice, we need the following ingredients:\n\nAn inferential model for \\(\\hat R\\). That could be a Bayesian nonparametric model, like a Gaussian Process, or something else, like a Tree-structure Parzen Estimator.\nA method to guess the next best hyperparameter value to pick. Typically, \\(\\lambda_{n+1}\\) is chosen to maximize the expected improvement criterion. This chooses \\(\\lambda\\) to maximize the expected value of \\(\\max\\{\\hat R(\\lambda) - R^*, 0\\}\\), where \\(R^*\\) is the current observed performance maximum. In other words, we want to maximize the potential for improving the current optimum, without penalizing for the possibility of observing a lower performance. This allows us to optimize \\(\\hat R\\) while still exploring the hyperparameter space. I refer the reader to here for a review of a few other selection criterions.\n\nWhen a Bayesian inferential framework is chosen, then sequential model-based optimization is called Bayesian optimization or Bayesian search. It is beyond of the scope of this blog post to go into the details of gaussian processes, but below I show howthe scikit-optimize library can be used to perform Bayesian optimization based on Gaussian Processes and the expected improvement criterion:\n\n\nCode\nfrom skopt import gp_minimize\nfrom skopt.utils import use_named_args\n\n@use_named_args(space)\ndef objective(**params):\n  model.set_params(**params)\n  \n  return -np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,\n                                  scoring=\"neg_mean_absolute_error\"))\n\nres_gp = gp_minimize(objective, space, n_calls=54, random_state=1)\n\n## 0.46\n\n\nWith Bayesian optimization, we see that much more time is spent sampling performant models.\n\n\nCode\nplt.clf()\np = plt.hist(res_gp.func_vals, bins=20)\np = plt.title(\"Score distribution for evaluated hyperparameters\", loc=\"left\")\np = plt.xlabel(\"Cross-validated average absolute error\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFurthermore, we can see that the algorithm quickly converges towards performant models.\n\n\nCode\nfrom skopt.plots import plot_convergence\n\nplot_convergence(res_gp)"
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#summary",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/a-brief-introduction-to-hyperparameter-optimization.html#summary",
    "title": "Intro to Hyperparameter Optimization for Machine Learning",
    "section": "3 Summary",
    "text": "3 Summary\nThis blog post provided a basic overview of hyperparameter optimization and of what can be gained from these techniques. We reviewed grid search, the simplest brute force approach. We reviewed random search, which improves upon grid search when some hyperparameter dimensions are not influencial. Finally, we reviewed sequential model-based optimization, which much more effectively samples models with good performance."
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html",
    "title": "Untitled",
    "section": "",
    "text": "This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#r-markdown",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#r-markdown",
    "title": "Untitled",
    "section": "",
    "text": "This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#slide-with-bullets",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#slide-with-bullets",
    "title": "Untitled",
    "section": "Slide with Bullets",
    "text": "Slide with Bullets\n\nBullet 1\nBullet 2\nBullet 3"
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#slide-with-r-output",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#slide-with-r-output",
    "title": "Untitled",
    "section": "Slide with R Output",
    "text": "Slide with R Output\n\n\nCode\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#slide-with-plot",
    "href": "pages/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization/talk.html#slide-with-plot",
    "title": "Untitled",
    "section": "Slide with Plot",
    "text": "Slide with Plot"
  },
  {
    "objectID": "index.html#book-a-time",
    "href": "index.html#book-a-time",
    "title": "üëã Welcome!",
    "section": "Book a Time",
    "text": "Book a Time"
  },
  {
    "objectID": "index.html#lets-meet",
    "href": "index.html#lets-meet",
    "title": "üëã Welcome!",
    "section": "Let‚Äôs Meet",
    "text": "Let‚Äôs Meet"
  }
]