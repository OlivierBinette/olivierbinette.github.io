<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Brève introduction à l’optimization d’hyperparamètres en apprentissage machine</title>
    <meta charset="utf-8" />
    <meta name="author" content="Olivier Binette" />
    <script src="presentation_files/header-attrs/header-attrs.js"></script>
    <link href="presentation_files/remark-css/default.css" rel="stylesheet" />
    <link href="presentation_files/remark-css/metropolis.css" rel="stylesheet" />
    <link href="presentation_files/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Brève introduction à l’optimization d’hyperparamètres en apprentissage machine
### Olivier Binette
### Intact Corporation Financière
### 1er février 2022

---


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 1.5em;
}
&lt;/style&gt;



---

# Plan de la présentation

1. Introduction
2. Un peu de terminologie et notions de base
3. Méthodes de type boîte noire pour l'optimization d'hyperparamètres
  - Recherche par grille
  - Recherche aléatoire
  - Optimization séquentielle / optimization bayésienne
4. Discussion + autres types de méthodes

--

**Objectif**:

Donner une compréhension conceptuelle des méthodes et montrer comment ça marche en pratique.


---
class: center, middle

## Introduction

---
class: center, middle

# Introduction

&lt;img src="ml.svg" width="100%"&gt;

---
class: center, middle


# Introduction

&lt;img src="workflow.svg" width="100%"&gt;


---
class: center, middle


# Introduction

Différents types de méthodes:

| Black-box methods | Integrated methods |
| :----- | :-----  |
| Grid Search | Hyperband|
|Randomized Search | Bayesian Model Selection |
| Sequential Model-Based Optimization | |

---
class: center, middle

## Terminologie, notations et exemple

---

# Terminologie et notations

- Modèle
- Paramètres
- Performance `\(R\)` de généralisation du modèle
  - Estimé par `\(\hat R\)` sur des données de validation ou par validation croisée.
  
**Hyperparamètres**

Tout ce qui est fixé à l'avance et pas incorporé à l'algorithme d'apprentissage de base:

- type de modèle à utiliser
- quelles features utiliser
- `learning_rate`, `min_samples_leaf`, etc.

---

# Exemple

## Données


```python
import pandas as pd
import numpy as np
from sklearn.datasets import fetch_california_housing

dataset = fetch_california_housing(as_frame=True)

X = dataset.data # Covariates
n_features = X.shape[1] # Number of features
y = dataset.target # Median house prices

X.head()
```

```
##    MedInc  HouseAge  AveRooms  ...  AveOccup  Latitude  Longitude
## 0  8.3252      41.0  6.984127  ...  2.555556     37.88    -122.23
## 1  8.3014      21.0  6.238137  ...  2.109842     37.86    -122.22
## 2  7.2574      52.0  8.288136  ...  2.802260     37.85    -122.24
## 3  5.6431      52.0  5.817352  ...  2.547945     37.85    -122.25
## 4  3.8462      52.0  6.281853  ...  2.181467     37.85    -122.25
## 
## [5 rows x 8 columns]
```

---

# Exemple

## Modèle et hyperparamètres


```python
from sklearn.ensemble import GradientBoostingRegressor
from skopt.space import Real, Integer

model = GradientBoostingRegressor(n_estimators=25, random_state=0)

space  = [Integer(1, 8, name='max_depth'),
          Real(0.01, 1, "log-uniform", name='learning_rate'),
          Integer(1, n_features, name='max_features'),
          Integer(1, 50, name='min_samples_leaf')
]
```


---

# Exemple

## Évaluation de la performance


```python
from sklearn.model_selection import cross_val_score

def Rhat(**params):
  model.set_params(**params)
  
  return -np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,
                                  scoring="neg_mean_absolute_error"))
```


```python
model.fit(X, y)
```

```
## GradientBoostingRegressor(n_estimators=25, random_state=0)
```

```python
Rhat()
```

```
## 0.5503720160635011
```

---
class: center, middle

## Optimization des hyperparamètres

---

# Optimization des hyperparamètres

## Méthodes black-box

- On fait abstraction du fonctionnement interne des modèles.

## Particularités du problème

1. `\(\hat R\)` est coûteuse à évaluer
2. Pas d'information de gradient à propos de `\(\hat R\)`
3. Complexité de l'espace d'hyperparamètres
4. Dimensionalité de l'espace d'hyperparamètres


---
class: center, middle

## Recherche par grille

---

# Recherche par grille

## Avantages

- Contrôle sur les paramètres évalués
- Simple et facile à paralléliser

## Désavantages

- Complexité computationelle exponentielle dans le nombre d'hyperparamètres considérés. 
- Sous-optimal pour beaucoup de problèmes.

---

# Recherche par grille


```python
from sklearn.model_selection import GridSearchCV

# Budget of 54 evaluations
grid = {
  'max_depth': [1, 3, 5],
  'learning_rate': [0.01, 0.1, 1],
  'max_features': [2, 4, 8],
  'min_samples_leaf': [1, 10]
}

def scoring(estimator, X_test, y_test):
  y_pred = estimator.predict(X_test)
  return -np.mean(np.abs(y_test - y_pred))

results = GridSearchCV(model, grid, cv=3, n_jobs=-1, scoring=scoring).fit(X, y)

-results.best_score_ # Lowest cross-validated mean absolute error
```

```
## 0.5084134111524956
```

---

# Recherche par grille

&lt;img src="presentation_files/figure-html/unnamed-chunk-6-1.png" width="768" /&gt;

---
class: center, middle

## Recherche aléatoire

---

# Recherche aléatoire

&lt;img src="grid-vs-random.png"&gt;

---

# Recherche aléatoire


```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform

# Around roughly the same values as for the grid search
param_distribution = {
  'max_depth': range(1, 8),
  'learning_rate': loguniform(0.01, 1),
  'max_features': range(1, 9),
  'min_samples_leaf': range(1, 50)
}

# Budget of 54 evaluations
results = RandomizedSearchCV(model, param_distribution, n_iter=54, cv=3, n_jobs=-1, scoring=scoring, random_state=0).fit(X, y)

-results.best_score_ # Lowest cross-validated mean absolute error
```

```
## 0.4797334598181764
```

---

# Recherche aléatoire

&lt;img src="presentation_files/figure-html/unnamed-chunk-8-3.png" width="768" /&gt;

---
class: center, middle

## Sequential Model-Based Optimization

---

# Sequential Model-Based Optimization

## Problème:

- Les méthodes précédentes n'utilisent pas d'information a priori sur `\(\hat R\)`.

--

## Hypothèse de régularité:

- Des hyperparamètres similaires ont des performances similaires.

--

Cette hypothèse permet de faire de l'inférence:

Si on a observé `\(\hat R\)` à des points `\(\lambda_1, \lambda_2, \dots, \lambda_n\)`, alors on estimer `\(\hat R\)`. On peut ensuite choisir un nouveau point `\(\lambda_{n+1}\)` qui a des chances de maximiser `\(\hat R\)`.

--

C'est tout l'idée du sequential model-based optimization.


---

# Sequential Model-Based Optimization

Pour le SMBO, on a besoin de:

1. Un modèle inférentiel pour `\(\hat R\)` (e.g. modèle bayésien nonparametrique ou autre).
2. Une méthode pour choisir le prochain hyperparamètre à évaluer, étant donné notre modèle pour `\(\hat R\)` ajusté à l'observation de `\(\hat R(\lambda_1), \hat R(\lambda_2), \dots, \hat R(\lambda_n)\)`.

--

Typiquement, on utilise le critère du **expected improvement** pour choisir `\(\lambda_{n+1}\)`: on veut maximiser l'espérance de `\(\max\{R^* - \hat R(\lambda), 0\}\)`, avec `\(R^*\)` le maximum observé de `\(\hat R\)`.

--

Autrement dit, on veut maximiser le potentiel d'améliorer le modèle, sans pénaliser pour la possibilité d'observer une performance plus faible.

---

# Bayesian optimization


```python
from skopt import gp_minimize
from skopt.utils import use_named_args

@use_named_args(space)
def objective(**params):
  model.set_params(**params)
  
  return -np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,
                                  scoring="neg_mean_absolute_error"))

res_gp = gp_minimize(objective, space, n_calls=54, random_state=1)
```

---

# Bayesian optimization

&lt;img src="presentation_files/figure-html/unnamed-chunk-10-5.png" width="768" /&gt;

---

# Bayesian optimization

&lt;img src="presentation_files/figure-html/unnamed-chunk-11-7.png" width="768" /&gt;

---
class: center, middle

## Conclusion


---

# Conclusion

## Résumé

- recherche par grille
- recherche aléatoire
- recherche bayésienne

## Autres méthodes

- Hyperband
- Approches bayésiennes
- marginalisation
- ensembles


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
