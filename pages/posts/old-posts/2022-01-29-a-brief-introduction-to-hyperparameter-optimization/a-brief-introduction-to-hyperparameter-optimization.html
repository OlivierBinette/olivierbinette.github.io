<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>


  <!--radix_placeholder_meta_tags-->
  <title>A Brief Introduction to Hyperparameter Optimization in Machine Learning</title>

  <meta property="description" itemprop="description" content="I review a few black-box hyperparameter optimization techniques at a high conceptual level: grid search, randomized search and sequential model-based optimization."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2022-01-29"/>
  <meta property="article:created" itemprop="dateCreated" content="2022-01-29"/>
  <meta name="article:author" content="Olivier Binette"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="A Brief Introduction to Hyperparameter Optimization in Machine Learning"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="I review a few black-box hyperparameter optimization techniques at a high conceptual level: grid search, randomized search and sequential model-based optimization."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="A Brief Introduction to Hyperparameter Optimization in Machine Learning"/>
  <meta property="twitter:description" content="I review a few black-box hyperparameter optimization techniques at a high conceptual level: grid search, randomized search and sequential model-based optimization."/>

  <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="A Brief Introduction to Hyperparameter Optimization in Machine Learning"/>
  <meta name="citation_fulltext_html_url" content="https://olivierbinette.github.io/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization"/>
  <meta name="citation_online_date" content="2022/01/29"/>
  <meta name="citation_publication_date" content="2022/01/29"/>
  <meta name="citation_author" content="Olivier Binette"/>
  <meta name="citation_author_institution" content="Duke University"/>
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","preview","author","date","categories","output","draft","repository_url","citation_url","creative-commons"]}},"value":[{"type":"character","attributes":{},"value":["A Brief Introduction to Hyperparameter Optimization in Machine Learning"]},{"type":"character","attributes":{},"value":["I review a few black-box hyperparameter optimization techniques at a high conceptual level: grid search, randomized search and sequential model-based optimization.\n"]},{"type":"character","attributes":{},"value":["a-brief-introduction-to-hyperparameter-optimization-files/figure-html5/unnamed-chunk-15-9.png"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation","orcid_id"]}},"value":[{"type":"character","attributes":{},"value":["Olivier Binette"]},{"type":"character","attributes":{},"value":["https://olivierbinette.github.io"]},{"type":"character","attributes":{},"value":["Duke University"]},{"type":"character","attributes":{},"value":["0000-0001-6009-5206"]}]}]},{"type":"character","attributes":{},"value":["2022-01-29"]},{"type":"character","attributes":{},"value":["Machine Learning","Hyperparameter Optimzation"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","toc_depth"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[3]}]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["https://github.com/olivierbinette/olivierbinette.github.io"]},{"type":"character","attributes":{},"value":["https://olivierbinette.github.io/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization"]},{"type":"character","attributes":{},"value":["CC BY-SA"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["a-brief-introduction-to-hyperparameter-optimization_files/anchor-4.2.2/anchor.min.js","a-brief-introduction-to-hyperparameter-optimization_files/bowser-1.9.3/bowser.min.js","a-brief-introduction-to-hyperparameter-optimization_files/distill-2.2.21/template.v2.js","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-10-3.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-11-1.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-11-3.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-11-5.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-12-1.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-13-5.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-14-5.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-14-7.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-15-7.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-15-9.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-7-1.png","a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-8-1.png","a-brief-introduction-to-hyperparameter-optimization_files/header-attrs-2.11/header-attrs.js","a-brief-introduction-to-hyperparameter-optimization_files/jquery-3.6.0/jquery-3.6.0.js","a-brief-introduction-to-hyperparameter-optimization_files/jquery-3.6.0/jquery-3.6.0.min.js","a-brief-introduction-to-hyperparameter-optimization_files/jquery-3.6.0/jquery-3.6.0.min.map","a-brief-introduction-to-hyperparameter-optimization_files/popper-2.6.0/popper.min.js","a-brief-introduction-to-hyperparameter-optimization_files/tippy-6.2.7/tippy-bundle.umd.min.js","a-brief-introduction-to-hyperparameter-optimization_files/tippy-6.2.7/tippy-light-border.css","a-brief-introduction-to-hyperparameter-optimization_files/tippy-6.2.7/tippy.css","a-brief-introduction-to-hyperparameter-optimization_files/tippy-6.2.7/tippy.umd.min.js","a-brief-introduction-to-hyperparameter-optimization_files/webcomponents-2.0.0/webcomponents.js","grid-vs-random.png","libs/crosstalk/css/crosstalk.min.css","libs/crosstalk/js/crosstalk.js","libs/crosstalk/js/crosstalk.js.map","libs/crosstalk/js/crosstalk.min.js","libs/crosstalk/js/crosstalk.min.js.map","libs/crosstalk/scss/crosstalk.scss","libs/datatables-binding/datatables.js","libs/datatables-css/datatables-crosstalk.css","libs/dt-core/css/jquery.dataTables.extra.css","libs/dt-core/css/jquery.dataTables.min.css","libs/dt-core/js/jquery.dataTables.min.js","libs/header-attrs/header-attrs.js","libs/htmlwidgets/htmlwidgets.js","libs/jquery/jquery-3.6.0.js","libs/jquery/jquery-3.6.0.min.js","libs/jquery/jquery-3.6.0.min.map","libs/leaflet-binding/leaflet.js","libs/leaflet/images/layers-2x.png","libs/leaflet/images/layers.png","libs/leaflet/images/marker-icon-2x.png","libs/leaflet/images/marker-icon.png","libs/leaflet/images/marker-shadow.png","libs/leaflet/leaflet.css","libs/leaflet/leaflet.js","libs/leafletfix/leafletfix.css","libs/proj4/proj4.min.js","libs/Proj4Leaflet/proj4leaflet.js","libs/remark-css/default.css","libs/remark-css/metropolis-fonts.css","libs/remark-css/metropolis.css","libs/rstudio_leaflet/images/1px.png","libs/rstudio_leaflet/rstudio_leaflet.css","ml.png","ml.svg","presentation_files/crosstalk/css/crosstalk.min.css","presentation_files/crosstalk/js/crosstalk.js","presentation_files/crosstalk/js/crosstalk.js.map","presentation_files/crosstalk/js/crosstalk.min.js","presentation_files/crosstalk/js/crosstalk.min.js.map","presentation_files/crosstalk/scss/crosstalk.scss","presentation_files/datatables-binding/datatables.js","presentation_files/datatables-css/datatables-crosstalk.css","presentation_files/dt-core/css/jquery.dataTables.extra.css","presentation_files/dt-core/css/jquery.dataTables.min.css","presentation_files/dt-core/js/jquery.dataTables.min.js","presentation_files/figure-html/cars-1.svg","presentation_files/figure-html/unnamed-chunk-10-5.png","presentation_files/figure-html/unnamed-chunk-11-7.png","presentation_files/figure-html/unnamed-chunk-6-1.png","presentation_files/figure-html/unnamed-chunk-7-1.png","presentation_files/figure-html/unnamed-chunk-8-3.png","presentation_files/figure-html/unnamed-chunk-9-3.png","presentation_files/header-attrs/header-attrs.js","presentation_files/htmlwidgets/htmlwidgets.js","presentation_files/jquery/jquery-3.6.0.js","presentation_files/jquery/jquery-3.6.0.min.js","presentation_files/jquery/jquery-3.6.0.min.map","presentation_files/leaflet-binding/leaflet.js","presentation_files/leaflet/images/layers-2x.png","presentation_files/leaflet/images/layers.png","presentation_files/leaflet/images/marker-icon-2x.png","presentation_files/leaflet/images/marker-icon.png","presentation_files/leaflet/images/marker-shadow.png","presentation_files/leaflet/leaflet.css","presentation_files/leaflet/leaflet.js","presentation_files/leafletfix/leafletfix.css","presentation_files/proj4/proj4.min.js","presentation_files/Proj4Leaflet/proj4leaflet.js","presentation_files/remark-css/default.css","presentation_files/remark-css/metropolis-fonts.css","presentation_files/remark-css/metropolis.css","presentation_files/rstudio_leaflet/images/1px.png","presentation_files/rstudio_leaflet/rstudio_leaflet.css","presentation.html","talk.html","techniques.svg","workflow.svg"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          return "<p>" + $('#ref-' + ref).html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/header-attrs-2.11/header-attrs.js"></script>
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/popper-2.6.0/popper.min.js"></script>
  <link href="a-brief-introduction-to-hyperparameter-optimization_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="a-brief-introduction-to-hyperparameter-optimization_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="a-brief-introduction-to-hyperparameter-optimization_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"A Brief Introduction to Hyperparameter Optimization in Machine Learning","description":"I review a few black-box hyperparameter optimization techniques at a high conceptual level: grid search, randomized search and sequential model-based optimization.","authors":[{"author":"Olivier Binette","authorURL":"https://olivierbinette.github.io","affiliation":"Duke University","affiliationURL":"#","orcidID":"0000-0001-6009-5206"}],"publishedDate":"2022-01-29T00:00:00.000-05:00","citationText":"Binette, 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>A Brief Introduction to Hyperparameter Optimization in Machine Learning</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt=tag">Machine Learning</div>
<div class="dt=tag">Hyperparameter Optimzation</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>I review a few black-box hyperparameter optimization techniques at a high conceptual level: grid search, randomized search and sequential model-based optimization.</p></p>
</div>

<div class="d-byline">
  Olivier Binette <a href="https://olivierbinette.github.io" class="uri">https://olivierbinette.github.io</a> (Duke University)
  
<br/>2022-01-29
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#background-and-terminology">1.1 Background and Terminology</a>
<ul>
<li><a href="#models-parameters-and-performance-evaluation">Models, Parameters and Performance Evaluation</a></li>
<li><a href="#hyperparameters">Hyperparameters</a></li>
<li><a href="#example">Example</a></li>
</ul></li>
<li><a href="#black-box-optimization-methods">2 Black-Box Optimization Methods</a>
<ul>
<li><a href="#grid-search">2.1 Grid Search</a></li>
<li><a href="#random-search">2.2 Random Search</a></li>
<li><a href="#sequential-model-based-optimization">2.3 Sequential Model-Based Optimization</a></li>
</ul></li>
<li><a href="#discussion">3 Discussion</a></li>
</ul>
</nav>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<h2 id="introduction">1 Introduction</h2>
<p>Machine learning is easy, right? You pick a model, fit it to your data, and out come predictions.</p>
<p><img src="ml.svg" /></p>
<p>That’s how machine learning is taught at colleges. That’s how it’s sold to businesses as well. Sometimes we talk about the fancy math and algorithms under the hood to make it look serious, but we rarely talk about how difficult it is to transform whatever data can gather into useful, actionable predictions that have business value.</p>
<p>There are many challenges. First, there’s the transformation of a business problem into something that’s remotely approachable by machine learning and statistics. Second, there’s the development of a data collection plan or, more often than not, the identification of observational data which is already available. With the collection of this data comes the third step, modeling, which bridges between numbers and useful answers. Modeling may have to account for all kinds of issue with your data, such as class imbalance, missingness, and non-representativeness. You also want to obtain <em>good</em> answers, so throughout this step <strong>you loop between model specification, evaluation, and refinement</strong>. It is a lengthy process of research and investigation into the performance of your model, insights into the <em>why</em> of what you observe, and various fixes and improvements to your model. Finally, in a fourth stage, you must account for how your model will be used and the management of its lifecycle.</p>
<p><img src="workflow.svg" /></p>
<p>Moral of the story: there is a lot work involved. We need all hands on deck. And even more than that, <strong>we need robust automatization tools</strong> to support this machine learning workflow.</p>
<p>This blog post is about a single set of tools – <strong>hyperparameter optimization techniques</strong> – used to help with the model specification, evaluation, and refinement loop. I will focus on the standard machine learning framework of supervised learning. In this context, machine learning algorithms can be seen as black boxes which take in some data, a bunch of tuning <em>hyperparameters</em> specified by the user of the algorithm, and which output predictions. The quality of the predictions can be evaluated through data splitting or cross-validation. That is, we’re always able to compare predictions to ground truth for the data we have at hand.</p>
<p>My goal is to describe key approaches to hyperparameter optimization (see Table 1) in order to provide <strong>conceptual understanding that can be helpful practice.</strong> I describe <em>black-box</em> methods which treat the machine learning algorithm as, well, a black blox. This includes <strong>grid search</strong>, <strong>randomized search</strong>, and sequential model-based optimization such as <strong>Bayesian optimization.</strong> There are additional methods to be considered, such as <strong>Hyperband</strong> and <strong>Bayesian model selection</strong>, which integrate with the learning algorithms themselves. These will be for another blog post.</p>
<table>
<caption>Table 1: Different types of hyperparameter optimization methods</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Black-box methods</th>
<th style="text-align: left;">Integrated methods</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Grid Search</td>
<td style="text-align: left;">Hyperband</td>
</tr>
<tr class="even">
<td style="text-align: left;">Randomized Search</td>
<td style="text-align: left;">Bayesian Model Selection</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sequential Model-Based Optimization</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<!--**Disclaimer:** my goal in this post is **not** to say that grid search is bad, or that you should be using algorithm X instead of agorithm Y for hyperparameter optimization. The model specification, evaluation and refinement loop is an important part of the machine learning workflow which leads to useful insights into the behavior and performance of your model. It should not be entirely automated. Hyperparameter optimization techniques should be used to gain more insights into your model and to improve your productivity, not as a drop-in replacement for model building. Use whatever technique works the best for you given what you're trying to achieve.
-->
<p>Before getting into the detail of these methods though, let’s go over some basic concepts and terminology which I’ll be using.</p>
<h2 id="background-and-terminology">1.1 Background and Terminology</h2>
<p>First, let’s talk about models, parameters and performance evaluation. This is going to be the occasion for me to introduce some terminology and notations.</p>
<h3 id="models-parameters-and-performance-evaluation">Models, Parameters and Performance Evaluation</h3>
<p>A <strong>model</strong> is a mathematical representation of something going on in the real world. For instance, suppose you want to predict whether or not a given stock <span class="math inline">\(X\)</span> is going to go up tomorrow. A model for this could be: predict it’s going to go up with probability <span class="math inline">\(\alpha\)</span> if it went up today, otherwise predict it’s not going to go up with probability <span class="math inline">\(\beta\)</span>. There’s only one <strong>variable</strong> in this model (whether or not the stock went up today), and there are two <strong>parameters</strong>, the probabilities <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Here the parameters could be learned if we had historical data.</p>
<p>You could consider more sophisticated models such as classical time series models or reccurent neural networks. In all cases, you have variables (the input to your model), parameters (what you learn from data), and you end up with predictions.</p>
<p>You can compare the performance of any model by comparing the predictions to what actually happened. For instance, you could look at how often your predictions were right. That’s a performance <strong>evaluation metric</strong>. Your goal is usually to build a model which will keep on performing well.</p>
<p>Formally, let <span class="math inline">\(R\)</span> be the (average) future performance of your model. You don’t know this quantity, but you can estimate it as <span class="math inline">\(\hat R\)</span> using techniques such as cross-validation and its variants. There might be a bias and a variance to <span class="math inline">\(\hat R\)</span>, but the best we can do in practice is to try to find the model with the best estimated performance (modulo certain adjustments).</p>
<p>This brings us to the question: <strong>how should you choose a model?</strong> The standard in machine learning is to choose a model which maximizes <span class="math inline">\(\hat R\)</span>. It’s not the only solution, and it’s not always the best solution (it can be better to do model averaging if <span class="math inline">\(\hat R\)</span> has some variance), but it’s what we’ll focus on through this blog post.</p>
<p>Furthermore, we’ll approach this problem through the lens of hyperparameter selection.</p>
<h3 id="hyperparameters">Hyperparameters</h3>
<p>Hyperparameters are things that have you have to specify before you can run a model, such as:</p>
<ul>
<li>what data features to use,</li>
<li>what type of model to use (linear model? random forest? neural network?)</li>
<li>other decisions that go into the specification of a model:
<ul>
<li>the number of layers in your neural network,</li>
<li>the learning rate for the gradient descent algorithm,</li>
<li>the maximum depth for decision trees, etc.</li>
</ul></li>
</ul>
<p>There is only a practical distinction between parameters and hyperparameters. Hyperparameters are things that are usually set separately from the other model parameters, or that do not nicely fit within a model’s learning algorithm. Depending on the framework you’re using, parameters can become hyperparameters and vice versa. For example, by using ensemble methods, you could easily transform the “model type choice” hyperparameter to a simple parameter of your ensemble that is learned from data.</p>
<p>The key thing is that, in practice, there will typically be some distinction between parameters of your model and a set of hyperparameters that you have to specify.</p>
<p>Through experience, you can learn what hyperparameters work well for the kinds of problems that you work on. Other times, you might carefully tune parameters and investigate the impact of your choices on model performance.</p>
<p>The manual process of hyperparameter tuning can lead to important insights into the performance and behavior of your model. However, it can also be a menial task that would be better automated through hyperparameter optimization algorithms aiming to maximize <span class="math inline">\(\hat R\)</span>, such as those that I review below.</p>
<h3 id="example">Example</h3>
<p>Let’s look at an example to make things concrete. This is adapted from <a href="https://scikit-optimize.github.io/stable/auto_examples/hyperparameter-optimization.html">scikit-optimize’s tutorial for tuning scikit-learn estimators</a>.</p>
<p>We’ll consider the <a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">California housing dataset</a> from the scikit-learn library. Each row in this dataset represents a census block and contains aggregated information regarding houses in that block. Our goal will be to predict median house price at the block level given these other covariates.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> fetch_california_housing(as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dataset.data <span class="co"># Covariates</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> X.shape[<span class="dv">1</span>] <span class="co"># Number of features</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dataset.target <span class="co"># Median house prices</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X</span></code></pre></div>
<pre><code>       MedInc  HouseAge  AveRooms  ...  AveOccup  Latitude  Longitude
0      8.3252      41.0  6.984127  ...  2.555556     37.88    -122.23
1      8.3014      21.0  6.238137  ...  2.109842     37.86    -122.22
2      7.2574      52.0  8.288136  ...  2.802260     37.85    -122.24
3      5.6431      52.0  5.817352  ...  2.547945     37.85    -122.25
4      3.8462      52.0  6.281853  ...  2.181467     37.85    -122.25
...       ...       ...       ...  ...       ...       ...        ...
20635  1.5603      25.0  5.045455  ...  2.560606     39.48    -121.09
20636  2.5568      18.0  6.114035  ...  3.122807     39.49    -121.21
20637  1.7000      17.0  5.205543  ...  2.325635     39.43    -121.22
20638  1.8672      18.0  5.329513  ...  2.123209     39.43    -121.32
20639  2.3886      16.0  5.254717  ...  2.616981     39.37    -121.24

[20640 rows x 8 columns]</code></pre>
</div>
<p>For the regression, we’ll use scikit-learn’s gradient boosted trees estimator. This model has a number of internal parameters which don’t need to know much about, as well as hyperparameters which can be used to tune the model. This includes the <code>max_depth</code> hyperparameter for the maximum depth of decision trees, <code>learning_rate</code> for the learning rate of gradient boosting, <code>max_features</code> for the maximum number of features to use in each decision trees, and a few more. Ranges of reasonable values for these parameters are specified in the <code>space</code> variable below.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.space <span class="im">import</span> Real, Integer</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GradientBoostingRegressor(n_estimators<span class="op">=</span><span class="dv">25</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>space  <span class="op">=</span> [Integer(<span class="dv">1</span>, <span class="dv">8</span>, name<span class="op">=</span><span class="st">&#39;max_depth&#39;</span>),</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>          Real(<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="st">&quot;log-uniform&quot;</span>, name<span class="op">=</span><span class="st">&#39;learning_rate&#39;</span>),</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>          Integer(<span class="dv">1</span>, n_features, name<span class="op">=</span><span class="st">&#39;max_features&#39;</span>),</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>          Integer(<span class="dv">1</span>, <span class="dv">50</span>, name<span class="op">=</span><span class="st">&#39;min_samples_leaf&#39;</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div>
</div>
<p>Now, the last thing we need is an estimator <span class="math inline">\(\hat R\)</span> for the model’s performance. This is our <code>Rhat()</code> function (i.e. <span class="math inline">\(\hat R\)</span>) which we’ll try to maximize. Here we use a cross-validated mean absolute error score.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Rhat(<span class="op">**</span>params):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  model.set_params(<span class="op">**</span>params)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="op">-</span>np.mean(cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                                  scoring<span class="op">=</span><span class="st">&quot;neg_mean_absolute_error&quot;</span>))</span></code></pre></div>
</div>
<p>With this, we can fit the model to the data (using default hyperparameter values to begin with), and evaluate the model’s performance.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span></code></pre></div>
<pre><code>GradientBoostingRegressor(n_estimators=25, random_state=0)</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>Rhat()</span></code></pre></div>
<pre><code>0.5503720160635011</code></pre>
</div>
<p>Here the unit for median house price was in hundreds of thousands of dollars and we can interpret the model performance at this scale. The value <span class="math inline">\(\hat R \approx 0.55\)</span> means that, on average, the absolute error of the model is $55,000. We’ll see if we can do better using hyperparameter optimization.</p>
<h2 id="black-box-optimization-methods">2 Black-Box Optimization Methods</h2>
<p>Black-box hyperparameter optimization algorithms consider the underlying machine algorithm as unknown. We only assume that, given a set of hyperparameters <span class="math inline">\(\lambda\)</span>, we can compute the estimated model performance <span class="math inline">\(\hat R(\lambda)\)</span>. There is usually variance in <span class="math inline">\(\hat R(\lambda)\)</span>, but this is not something that I will talk about in this post. We will therefore consider <span class="math inline">\(\hat R\)</span> as a deterministic function to be optimized.</p>
<p>Note: in practice, <strong>you need to account for the variance in <span class="math inline">\(\hat R\)</span></strong>, as otherwise you could get bad surprises. It’s just not something I’m covering in this post, since I want to focus on a conceptual understanding of the optimization algorithms.</p>
<p>We can use almost any technique to try to optimize <span class="math inline">\(\hat R\)</span>, but there are a number of challenges with hyperparameter optimization:</p>
<ol type="1">
<li><span class="math inline">\(\hat R\)</span> is usually rather costly to evaluate.</li>
<li>We usually do not have gradient information regarding <span class="math inline">\(\hat R\)</span> (otherwise, hyperparameters for which we have gradient information could easily be incorporated as parameters of the underlying ML algorithms).</li>
<li>The hyperparameter space is usually complex. It can contain discrete variables and can even be tree-structured, where some hyperparameters are only defined conditionally on other hyperparameters.</li>
<li>The hyperparameter space is usually somewhat high-dimensional, with more than just 2-3 dimensions.</li>
</ol>
<p>These particularities of the hyperparameter optimization problem has led the machine learning community to favor some of the optimization techniques which I discuss below.</p>
<h3 id="grid-search">2.1 Grid Search</h3>
<p>The first technique to consider is <strong>grid search</strong>, which is a brute force approach to hyperparameter optimization. It is the simplest of all – you simply specify values to consider for each hyperparameter, and then evaluate your model performance for each combination of hyperparameter. At the end, you keep the hyperparameter configuration which performed best.</p>
<p>There are a few advantages to this approach:</p>
<ul>
<li>It gives you precise control over what hyperparameter configurations are evaluated.</li>
<li>It is simple to implement and easily parallelizable.</li>
</ul>
<p>However, there are also a number of serious drawbacks:</p>
<ol type="1">
<li>The runtime scales exponentially in the number of hyperparameter dimensions.</li>
<li>The runtime is tied to the hyperparameter search space which you specify. To reduce runtime, you need to manually redefine this space.</li>
</ol>
<p>Let’s see an example of how this works in practice. First, we define a grid of hyperparameter values to evaluate. Given the scoring function <span class="math inline">\(\hat R\)</span>, we can then use scikit-learn’s <code>GridSearchCV()</code> function to evaluate the model performance at each hyperparameter combination. This is done below:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Budget of 54 evaluations</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> {</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;max_depth&#39;</span>: [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;learning_rate&#39;</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>],</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;max_features&#39;</span>: [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>],</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">1</span>, <span class="dv">10</span>]</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scoring(estimator, X_test, y_test):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="op">=</span> estimator.predict(X_test)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="op">-</span>np.mean(np.<span class="bu">abs</span>(y_test <span class="op">-</span> y_pred))</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> GridSearchCV(model, grid, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span>scoring).fit(X, y)</span></code></pre></div>
</div>
<p>We can then recover the best score and best hyperparameters. The best model is slightly better than the default model we looked at earlier, with a $4,000 decrease in average absolute error.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span>results.best_score_ <span class="co"># Lowest cross-validated mean absolute error</span></span></code></pre></div>
<pre><code>0.5084134111524956</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>{key:results.best_params_[key] <span class="cf">for</span> key <span class="kw">in</span> grid.keys()} <span class="co"># Best parameters</span></span></code></pre></div>
<pre><code>{&#39;max_depth&#39;: 5, &#39;learning_rate&#39;: 0.1, &#39;max_features&#39;: 8, &#39;min_samples_leaf&#39;: 1}</code></pre>
</div>
<p>It is also informative to plot an histogram for the distribution of model scores. We can see that most model configurations performed much worst than the default.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.hist(<span class="op">-</span>results.cv_results_[<span class="st">&quot;mean_test_score&quot;</span>], bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.title(<span class="st">&quot;Score distribution for evaluated hyperparameters&quot;</span>, loc<span class="op">=</span><span class="st">&quot;left&quot;</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.xlabel(<span class="st">&quot;Cross-validated average absolute error&quot;</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-8-1.png" width="624" /></p>
</div>
<h3 id="random-search">2.2 Random Search</h3>
<p>The second method we’ll look at is <strong>random search.</strong> Here, the idea is to sample a number <span class="math inline">\(k\)</span> of hyperparameter configurations at random from a given space, and to evaluate those random configurations.</p>
<p>This might seem like a silly idea. Why pick hyperparameter values at random?</p>
<p>The answer is that doing so <strong>removes all computational penalties</strong> from the consideration of useless hyperparameter dimensions. That is, imagine that a number <span class="math inline">\(s\)</span> of your hyperparameters have actually no impact on model performance. With grid search, the consideration of these hyperparameters would incur you a computational penalty which is exponential in <span class="math inline">\(s\)</span>. With random search, however, there is <strong>no penalty at all</strong> for adding these <span class="math inline">\(s\)</span> additional hyperparameter dimensions. The results from random search with or without these additional dimensions are <strong>exactly the same</strong> in both cases.</p>
<p>This is the huge advantage of random search over grid search: you do not get penalized for useless dimensions. Furthermore, in practice, being able to tune the search effort through the number of samples <span class="math inline">\(k\)</span> can be quite convenient.</p>
<p>Let’s see how this can be implemented in practice. We’ll define a hyperparameter space which is similar to the grid space we specified earlier, but which is filled in with additional possible values. We can then run scikit-learn’s <code>RandomizedSearchCV()</code> function to do the randomized search:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> loguniform</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Around roughly the same values as for the grid search</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>param_distribution <span class="op">=</span> {</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;max_depth&#39;</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">8</span>),</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;learning_rate&#39;</span>: loguniform(<span class="fl">0.01</span>, <span class="dv">1</span>),</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;max_features&#39;</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;min_samples_leaf&#39;</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Budget of 54 evaluations</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> RandomizedSearchCV(model, param_distribution, n_iter<span class="op">=</span><span class="dv">54</span>, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span>scoring, random_state<span class="op">=</span><span class="dv">0</span>).fit(X, y)</span></code></pre></div>
</div>
<p>The results are below. By considering a richer hyperparameter space, and without being penalized by this in the same way we would with a grid search, randomized search allows us to find a better model with the same amount of effort.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span>results.best_score_ <span class="co"># Lowest cross-validated mean absolute error</span></span></code></pre></div>
<pre><code>0.4797334598181764</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>{key:results.best_params_[key] <span class="cf">for</span> key <span class="kw">in</span> grid.keys()} <span class="co"># Best parameters</span></span></code></pre></div>
<pre><code>{&#39;max_depth&#39;: 6, &#39;learning_rate&#39;: 0.14539375242431551, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 26}</code></pre>
</div>
<p>Again, we can look at the distribution of model performance for sampled hyperparameter configurations. It’s quite similar to grid search, with only a few better-performing models being identified.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.hist(<span class="op">-</span>results.cv_results_[<span class="st">&quot;mean_test_score&quot;</span>], bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.title(<span class="st">&quot;Score distribution for evaluated hyperparameters&quot;</span>, loc<span class="op">=</span><span class="st">&quot;left&quot;</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.xlabel(<span class="st">&quot;Cross-validated average absolute error&quot;</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-11-3.png" width="624" /></p>
</div>
<h3 id="sequential-model-based-optimization">2.3 Sequential Model-Based Optimization</h3>
<p>All of the techniques considered so far made no assumption at all about the function <span class="math inline">\(\hat R\)</span> to optimize.</p>
<p>This is a problem, because we do have prior information about <span class="math inline">\(\hat R\)</span>. We can expect <span class="math inline">\(\hat R\)</span> to have some level of regularity, meaning that similar hyperparameter configurations should have similar performance. This knowledge allows us to make inference about <span class="math inline">\(\hat R(\lambda)\)</span> given the evaluation of <span class="math inline">\(\hat R\)</span> at other points <span class="math inline">\(\tilde \lambda \not = \lambda\)</span>.</p>
<p>More formally, suppose we have evaluated <span class="math inline">\(\hat R\)</span> at a sequence of hyperparameter configurations <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_n\)</span>, thus observing <span class="math inline">\(\hat R(\lambda_1), \hat R(\lambda_2), \dots, \hat R(\lambda_n)\)</span>. This allows us to make inference about <span class="math inline">\(\hat R\)</span>. In particular, we can try guessing what next <span class="math inline">\(\lambda_{n+1}\)</span> will maximize <span class="math inline">\(\hat R\)</span> or improve our knowledge of <span class="math inline">\(\hat R\)</span>. Once we’ve observed <span class="math inline">\(\hat R(\lambda_{n+1})\)</span>, we repeat the process, trying to guess which <span class="math inline">\(\lambda_{n+2}\)</span> to pick to improve the procedure. That is the entire idea behind <strong>sequential model-based optimization</strong>.</p>
<p>To make this work in practice, we need the following ingredients:</p>
<ol type="1">
<li>An inferential model for <span class="math inline">\(\hat R\)</span>. That could be a Bayesian nonparametric model, like a Gaussian Process, or something else, like a Tree-structure Parzen Estimator.</li>
<li>A method to guess the next best hyperparameter value to pick. Typically, <span class="math inline">\(\lambda_{n+1}\)</span> is chosen to maximize the <strong>expected improvement criterion</strong>. This chooses <span class="math inline">\(\lambda\)</span> to maximize the expected value of <span class="math inline">\(\max\{\hat R(\lambda) - R^*, 0\}\)</span>, where <span class="math inline">\(R^*\)</span> is the current observed performance maximum. In other words, we want to maximize the potential for improving the current optimum, without penalizing for the possibility of observing a lower performance. This allows us to optimize <span class="math inline">\(\hat R\)</span> while still exploring the hyperparameter space. I refer the reader to <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">here</a> for a review of a few other selection criterions.</li>
</ol>
<p>When a Bayesian inferential framework is chosen, then sequential model-based optimization is called <strong>Bayesian optimization</strong> or <strong>Bayesian search</strong>. It is beyond of the scope of this blog post to go into the details of gaussian processes, but below I show howthe scikit-optimize library can be used to perform Bayesian optimization based on Gaussian Processes and the expected improvement criterion:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt <span class="im">import</span> gp_minimize</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.utils <span class="im">import</span> use_named_args</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="at">@use_named_args</span>(space)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(<span class="op">**</span>params):</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>  model.set_params(<span class="op">**</span>params)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="op">-</span>np.mean(cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">3</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>                                  scoring<span class="op">=</span><span class="st">&quot;neg_mean_absolute_error&quot;</span>))</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>res_gp <span class="op">=</span> gp_minimize(objective, space, n_calls<span class="op">=</span><span class="dv">54</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co">## 0.46</span></span></code></pre></div>
</div>
<p>With Bayesian optimization, we see that much more time is spent sampling performant models.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.hist(res_gp.func_vals, bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.title(<span class="st">&quot;Score distribution for evaluated hyperparameters&quot;</span>, loc<span class="op">=</span><span class="st">&quot;left&quot;</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.xlabel(<span class="st">&quot;Cross-validated average absolute error&quot;</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-13-5.png" width="624" /></p>
</div>
<p>Furthermore, we can see that the algorithm quickly converges towards performant models.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.plots <span class="im">import</span> plot_convergence</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>plot_convergence(res_gp)</span></code></pre></div>
<p><img src="a-brief-introduction-to-hyperparameter-optimization_files/figure-html5/unnamed-chunk-14-7.png" width="624" /></p>
</div>
<h2 id="discussion">3 Discussion</h2>
<p>Hopefully, this blog post provided a basic overview of hyperparameter optimization and of what can be gained from these techniques. We reviewed grid search, the simplest brute force approach. We reviewed random search, which improves upon grid search when some hyperparameter dimensions are not influencial. Finally, we reviewed sequential model-based optimization, which much more effectively samples models with good performance.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/olivierbinette/olivierbinette.github.io/issues/new">create an issue</a> on the source repository.</p>
<h3 id="citation">Citation</h3>
<p>For attribution, please cite this work as</p>
<pre class="citation-appendix short">Binette (2022, Jan. 29). A Brief Introduction to Hyperparameter Optimization in Machine Learning. Retrieved from https://olivierbinette.github.io/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization</pre>
<p>BibTeX citation</p>
<pre class="citation-appendix long">@misc{binette2022a,
  author = {Binette, Olivier},
  title = {A Brief Introduction to Hyperparameter Optimization in Machine Learning},
  url = {https://olivierbinette.github.io/posts/2022-01-29-a-brief-introduction-to-hyperparameter-optimization},
  year = {2022}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
